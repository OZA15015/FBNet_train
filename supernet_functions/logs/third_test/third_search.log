09/28 12:13:40 AM | Firstly, start to train weights for epoch 0
Files already downloaded and verified
Files already downloaded and verified
09/28 12:14:06 AM | Train: [  1/180] Step 050/1249 Loss 39.121 Prec@(1,3) (11.6%, 54.8%), ce_loss 4.407, lat_loss 22.192
09/28 12:14:30 AM | Train: [  1/180] Step 100/1249 Loss 32.974 Prec@(1,3) (12.8%, 56.8%), ce_loss 3.715, lat_loss 22.191
09/28 12:14:49 AM | Train: [  1/180] Step 150/1249 Loss 29.528 Prec@(1,3) (14.8%, 59.1%), ce_loss 3.326, lat_loss 22.192
09/28 12:15:09 AM | Train: [  1/180] Step 200/1249 Loss 27.423 Prec@(1,3) (15.7%, 61.7%), ce_loss 3.089, lat_loss 22.192
09/28 12:15:29 AM | Train: [  1/180] Step 250/1249 Loss 25.929 Prec@(1,3) (16.4%, 63.5%), ce_loss 2.921, lat_loss 22.193
09/28 12:15:49 AM | Train: [  1/180] Step 300/1249 Loss 24.837 Prec@(1,3) (16.9%, 65.3%), ce_loss 2.798, lat_loss 22.194
09/28 12:16:09 AM | Train: [  1/180] Step 350/1249 Loss 24.017 Prec@(1,3) (17.4%, 66.7%), ce_loss 2.705, lat_loss 22.194
09/28 12:16:29 AM | Train: [  1/180] Step 400/1249 Loss 23.362 Prec@(1,3) (17.9%, 68.1%), ce_loss 2.632, lat_loss 22.194
09/28 12:16:49 AM | Train: [  1/180] Step 450/1249 Loss 22.813 Prec@(1,3) (18.3%, 69.1%), ce_loss 2.570, lat_loss 22.194
09/28 12:17:08 AM | Train: [  1/180] Step 500/1249 Loss 22.341 Prec@(1,3) (18.8%, 70.1%), ce_loss 2.517, lat_loss 22.194
09/28 12:17:28 AM | Train: [  1/180] Step 550/1249 Loss 21.936 Prec@(1,3) (19.3%, 70.9%), ce_loss 2.471, lat_loss 22.194
09/28 12:17:48 AM | Train: [  1/180] Step 600/1249 Loss 21.574 Prec@(1,3) (19.9%, 71.7%), ce_loss 2.430, lat_loss 22.194
09/28 12:18:08 AM | Train: [  1/180] Step 650/1249 Loss 21.269 Prec@(1,3) (20.3%, 72.4%), ce_loss 2.396, lat_loss 22.194
09/28 12:18:29 AM | Train: [  1/180] Step 700/1249 Loss 21.000 Prec@(1,3) (20.7%, 73.2%), ce_loss 2.366, lat_loss 22.194
09/28 12:18:51 AM | Train: [  1/180] Step 750/1249 Loss 20.777 Prec@(1,3) (21.0%, 73.7%), ce_loss 2.340, lat_loss 22.194
09/28 12:19:15 AM | Train: [  1/180] Step 800/1249 Loss 20.549 Prec@(1,3) (21.3%, 74.3%), ce_loss 2.315, lat_loss 22.194
09/28 12:19:38 AM | Train: [  1/180] Step 850/1249 Loss 20.358 Prec@(1,3) (21.6%, 74.7%), ce_loss 2.293, lat_loss 22.194
09/28 12:20:01 AM | Train: [  1/180] Step 900/1249 Loss 20.167 Prec@(1,3) (21.9%, 75.2%), ce_loss 2.272, lat_loss 22.193
09/28 12:20:24 AM | Train: [  1/180] Step 950/1249 Loss 20.000 Prec@(1,3) (22.2%, 75.5%), ce_loss 2.253, lat_loss 22.193
09/28 12:20:47 AM | Train: [  1/180] Step 1000/1249 Loss 19.849 Prec@(1,3) (22.5%, 75.8%), ce_loss 2.236, lat_loss 22.193
09/28 12:21:11 AM | Train: [  1/180] Step 1050/1249 Loss 19.691 Prec@(1,3) (22.9%, 76.2%), ce_loss 2.218, lat_loss 22.194
09/28 12:21:33 AM | Train: [  1/180] Step 1100/1249 Loss 19.548 Prec@(1,3) (23.2%, 76.5%), ce_loss 2.202, lat_loss 22.194
09/28 12:21:56 AM | Train: [  1/180] Step 1150/1249 Loss 19.416 Prec@(1,3) (23.5%, 76.8%), ce_loss 2.187, lat_loss 22.194
09/28 12:22:18 AM | Train: [  1/180] Step 1200/1249 Loss 19.285 Prec@(1,3) (23.9%, 77.1%), ce_loss 2.172, lat_loss 22.194
09/28 12:22:40 AM | Train: [  1/180] Step 1249/1249 Loss 19.186 Prec@(1,3) (24.1%, 77.3%), ce_loss 2.161, lat_loss 22.194
09/28 12:22:40 AM | _w_step_train: [  1/180] Final Prec@1 24.1100% Time 540.27
09/28 12:22:40 AM | Firstly, start to train weights for epoch 1
09/28 12:23:05 AM | Train: [  2/180] Step 050/1249 Loss 16.295 Prec@(1,3) (33.0%, 84.3%), ce_loss 2.148, lat_loss 22.194
09/28 12:23:25 AM | Train: [  2/180] Step 100/1249 Loss 16.374 Prec@(1,3) (32.4%, 84.2%), ce_loss 2.138, lat_loss 22.194
09/28 12:23:49 AM | Train: [  2/180] Step 150/1249 Loss 16.470 Prec@(1,3) (31.7%, 83.9%), ce_loss 2.128, lat_loss 22.194
09/28 12:24:13 AM | Train: [  2/180] Step 200/1249 Loss 16.386 Prec@(1,3) (31.9%, 83.7%), ce_loss 2.118, lat_loss 22.194
09/28 12:24:38 AM | Train: [  2/180] Step 250/1249 Loss 16.286 Prec@(1,3) (32.2%, 84.0%), ce_loss 2.107, lat_loss 22.194
09/28 12:25:03 AM | Train: [  2/180] Step 300/1249 Loss 16.223 Prec@(1,3) (32.3%, 84.4%), ce_loss 2.096, lat_loss 22.193
09/28 12:25:27 AM | Train: [  2/180] Step 350/1249 Loss 16.197 Prec@(1,3) (32.4%, 84.5%), ce_loss 2.087, lat_loss 22.193
09/28 12:25:52 AM | Train: [  2/180] Step 400/1249 Loss 16.153 Prec@(1,3) (32.6%, 84.6%), ce_loss 2.078, lat_loss 22.193
09/28 12:26:17 AM | Train: [  2/180] Step 450/1249 Loss 16.101 Prec@(1,3) (32.7%, 84.7%), ce_loss 2.069, lat_loss 22.193
09/28 12:26:39 AM | Train: [  2/180] Step 500/1249 Loss 16.075 Prec@(1,3) (32.8%, 84.9%), ce_loss 2.061, lat_loss 22.193
09/28 12:27:04 AM | Train: [  2/180] Step 550/1249 Loss 16.026 Prec@(1,3) (32.9%, 85.0%), ce_loss 2.052, lat_loss 22.193
09/28 12:27:28 AM | Train: [  2/180] Step 600/1249 Loss 15.986 Prec@(1,3) (33.2%, 85.1%), ce_loss 2.044, lat_loss 22.193
09/28 12:27:53 AM | Train: [  2/180] Step 650/1249 Loss 15.959 Prec@(1,3) (33.2%, 85.1%), ce_loss 2.037, lat_loss 22.193
09/28 12:28:15 AM | Train: [  2/180] Step 700/1249 Loss 15.934 Prec@(1,3) (33.4%, 85.3%), ce_loss 2.030, lat_loss 22.193
09/28 12:28:39 AM | Train: [  2/180] Step 750/1249 Loss 15.914 Prec@(1,3) (33.6%, 85.4%), ce_loss 2.023, lat_loss 22.193
09/28 12:29:04 AM | Train: [  2/180] Step 800/1249 Loss 15.930 Prec@(1,3) (33.6%, 85.3%), ce_loss 2.018, lat_loss 22.193
09/28 12:29:27 AM | Train: [  2/180] Step 850/1249 Loss 15.898 Prec@(1,3) (33.7%, 85.4%), ce_loss 2.011, lat_loss 22.193
09/28 12:29:52 AM | Train: [  2/180] Step 900/1249 Loss 15.866 Prec@(1,3) (33.9%, 85.5%), ce_loss 2.005, lat_loss 22.193
09/28 12:30:15 AM | Train: [  2/180] Step 950/1249 Loss 15.844 Prec@(1,3) (34.0%, 85.6%), ce_loss 1.999, lat_loss 22.193
09/28 12:30:40 AM | Train: [  2/180] Step 1000/1249 Loss 15.803 Prec@(1,3) (34.2%, 85.7%), ce_loss 1.992, lat_loss 22.193
09/28 12:31:05 AM | Train: [  2/180] Step 1050/1249 Loss 15.782 Prec@(1,3) (34.3%, 85.7%), ce_loss 1.986, lat_loss 22.193
09/28 12:31:29 AM | Train: [  2/180] Step 1100/1249 Loss 15.756 Prec@(1,3) (34.5%, 85.8%), ce_loss 1.980, lat_loss 22.193
09/28 12:31:53 AM | Train: [  2/180] Step 1150/1249 Loss 15.739 Prec@(1,3) (34.5%, 85.9%), ce_loss 1.975, lat_loss 22.193
09/28 12:32:18 AM | Train: [  2/180] Step 1200/1249 Loss 15.729 Prec@(1,3) (34.6%, 85.9%), ce_loss 1.970, lat_loss 22.193
09/28 12:32:42 AM | Train: [  2/180] Step 1249/1249 Loss 15.699 Prec@(1,3) (34.7%, 86.1%), ce_loss 1.965, lat_loss 22.193
09/28 12:32:42 AM | _w_step_train: [  2/180] Final Prec@1 34.7275% Time 602.03
09/28 12:32:42 AM | Firstly, start to train weights for epoch 2
09/28 12:33:00 AM | Train: [  3/180] Step 050/1249 Loss 15.083 Prec@(1,3) (37.8%, 87.7%), ce_loss 1.960, lat_loss 22.193
09/28 12:33:22 AM | Train: [  3/180] Step 100/1249 Loss 15.030 Prec@(1,3) (38.1%, 88.0%), ce_loss 1.954, lat_loss 22.193
09/28 12:33:47 AM | Train: [  3/180] Step 150/1249 Loss 14.986 Prec@(1,3) (38.9%, 88.0%), ce_loss 1.949, lat_loss 22.193
09/28 12:34:12 AM | Train: [  3/180] Step 200/1249 Loss 14.993 Prec@(1,3) (38.7%, 88.0%), ce_loss 1.944, lat_loss 22.193
09/28 12:34:37 AM | Train: [  3/180] Step 250/1249 Loss 15.010 Prec@(1,3) (38.3%, 88.2%), ce_loss 1.940, lat_loss 22.193
09/28 12:35:01 AM | Train: [  3/180] Step 300/1249 Loss 14.985 Prec@(1,3) (38.5%, 88.3%), ce_loss 1.935, lat_loss 22.193
09/28 12:35:26 AM | Train: [  3/180] Step 350/1249 Loss 14.936 Prec@(1,3) (38.9%, 88.2%), ce_loss 1.930, lat_loss 22.193
09/28 12:35:51 AM | Train: [  3/180] Step 400/1249 Loss 14.869 Prec@(1,3) (39.1%, 88.3%), ce_loss 1.925, lat_loss 22.193
09/28 12:36:15 AM | Train: [  3/180] Step 450/1249 Loss 14.834 Prec@(1,3) (39.2%, 88.3%), ce_loss 1.920, lat_loss 22.193
09/28 12:36:40 AM | Train: [  3/180] Step 500/1249 Loss 14.871 Prec@(1,3) (39.0%, 88.2%), ce_loss 1.917, lat_loss 22.193
09/28 12:37:04 AM | Train: [  3/180] Step 550/1249 Loss 14.895 Prec@(1,3) (38.9%, 88.2%), ce_loss 1.913, lat_loss 22.193
09/28 12:37:29 AM | Train: [  3/180] Step 600/1249 Loss 14.897 Prec@(1,3) (38.7%, 88.3%), ce_loss 1.909, lat_loss 22.193
09/28 12:37:54 AM | Train: [  3/180] Step 650/1249 Loss 14.909 Prec@(1,3) (38.7%, 88.3%), ce_loss 1.906, lat_loss 22.193
09/28 12:38:18 AM | Train: [  3/180] Step 700/1249 Loss 14.862 Prec@(1,3) (38.7%, 88.5%), ce_loss 1.901, lat_loss 22.193
09/28 12:38:42 AM | Train: [  3/180] Step 750/1249 Loss 14.818 Prec@(1,3) (38.9%, 88.6%), ce_loss 1.897, lat_loss 22.193
09/28 12:39:07 AM | Train: [  3/180] Step 800/1249 Loss 14.787 Prec@(1,3) (39.0%, 88.7%), ce_loss 1.892, lat_loss 22.193
09/28 12:39:30 AM | Train: [  3/180] Step 850/1249 Loss 14.782 Prec@(1,3) (39.0%, 88.7%), ce_loss 1.889, lat_loss 22.193
09/28 12:39:52 AM | Train: [  3/180] Step 900/1249 Loss 14.764 Prec@(1,3) (39.1%, 88.7%), ce_loss 1.885, lat_loss 22.193
09/28 12:40:14 AM | Train: [  3/180] Step 950/1249 Loss 14.732 Prec@(1,3) (39.2%, 88.8%), ce_loss 1.881, lat_loss 22.193
09/28 12:40:37 AM | Train: [  3/180] Step 1000/1249 Loss 14.701 Prec@(1,3) (39.3%, 88.9%), ce_loss 1.877, lat_loss 22.193
09/28 12:40:59 AM | Train: [  3/180] Step 1050/1249 Loss 14.688 Prec@(1,3) (39.3%, 88.9%), ce_loss 1.873, lat_loss 22.193
09/28 12:41:21 AM | Train: [  3/180] Step 1100/1249 Loss 14.664 Prec@(1,3) (39.5%, 89.0%), ce_loss 1.869, lat_loss 22.193
09/28 12:41:44 AM | Train: [  3/180] Step 1150/1249 Loss 14.665 Prec@(1,3) (39.6%, 89.0%), ce_loss 1.866, lat_loss 22.193
09/28 12:42:07 AM | Train: [  3/180] Step 1200/1249 Loss 14.648 Prec@(1,3) (39.7%, 89.0%), ce_loss 1.863, lat_loss 22.193
09/28 12:42:32 AM | Train: [  3/180] Step 1249/1249 Loss 14.660 Prec@(1,3) (39.6%, 89.0%), ce_loss 1.860, lat_loss 22.193
09/28 12:42:32 AM | _w_step_train: [  3/180] Final Prec@1 39.5550% Time 589.41
09/28 12:42:32 AM | Firstly, start to train weights for epoch 3
09/28 12:42:58 AM | Train: [  4/180] Step 050/1249 Loss 13.960 Prec@(1,3) (41.9%, 90.1%), ce_loss 1.857, lat_loss 22.193
09/28 12:43:22 AM | Train: [  4/180] Step 100/1249 Loss 14.023 Prec@(1,3) (42.0%, 90.3%), ce_loss 1.853, lat_loss 22.193
09/28 12:43:44 AM | Train: [  4/180] Step 150/1249 Loss 14.009 Prec@(1,3) (42.5%, 90.0%), ce_loss 1.849, lat_loss 22.193
09/28 12:44:06 AM | Train: [  4/180] Step 200/1249 Loss 13.936 Prec@(1,3) (43.0%, 90.1%), ce_loss 1.846, lat_loss 22.193
09/28 12:44:30 AM | Train: [  4/180] Step 250/1249 Loss 13.905 Prec@(1,3) (43.1%, 90.3%), ce_loss 1.842, lat_loss 22.193
09/28 12:44:53 AM | Train: [  4/180] Step 300/1249 Loss 13.967 Prec@(1,3) (43.0%, 90.2%), ce_loss 1.839, lat_loss 22.193
09/28 12:45:15 AM | Train: [  4/180] Step 350/1249 Loss 13.941 Prec@(1,3) (43.1%, 90.3%), ce_loss 1.836, lat_loss 22.193
09/28 12:45:38 AM | Train: [  4/180] Step 400/1249 Loss 13.957 Prec@(1,3) (43.2%, 90.3%), ce_loss 1.833, lat_loss 22.193
09/28 12:46:02 AM | Train: [  4/180] Step 450/1249 Loss 13.976 Prec@(1,3) (43.3%, 90.2%), ce_loss 1.830, lat_loss 22.193
09/28 12:46:26 AM | Train: [  4/180] Step 500/1249 Loss 13.969 Prec@(1,3) (43.2%, 90.4%), ce_loss 1.827, lat_loss 22.193
09/28 12:46:50 AM | Train: [  4/180] Step 550/1249 Loss 13.934 Prec@(1,3) (43.2%, 90.5%), ce_loss 1.823, lat_loss 22.193
09/28 12:47:13 AM | Train: [  4/180] Step 600/1249 Loss 13.923 Prec@(1,3) (43.2%, 90.5%), ce_loss 1.820, lat_loss 22.193
09/28 12:47:37 AM | Train: [  4/180] Step 650/1249 Loss 13.896 Prec@(1,3) (43.3%, 90.5%), ce_loss 1.817, lat_loss 22.193
09/28 12:47:59 AM | Train: [  4/180] Step 700/1249 Loss 13.907 Prec@(1,3) (43.2%, 90.6%), ce_loss 1.814, lat_loss 22.193
09/28 12:48:22 AM | Train: [  4/180] Step 750/1249 Loss 13.906 Prec@(1,3) (43.3%, 90.5%), ce_loss 1.811, lat_loss 22.193
09/28 12:48:43 AM | Train: [  4/180] Step 800/1249 Loss 13.887 Prec@(1,3) (43.3%, 90.6%), ce_loss 1.808, lat_loss 22.193
09/28 12:49:06 AM | Train: [  4/180] Step 850/1249 Loss 13.869 Prec@(1,3) (43.4%, 90.6%), ce_loss 1.805, lat_loss 22.193
09/28 12:49:29 AM | Train: [  4/180] Step 900/1249 Loss 13.843 Prec@(1,3) (43.5%, 90.6%), ce_loss 1.802, lat_loss 22.193
09/28 12:49:54 AM | Train: [  4/180] Step 950/1249 Loss 13.828 Prec@(1,3) (43.6%, 90.7%), ce_loss 1.799, lat_loss 22.193
09/28 12:50:17 AM | Train: [  4/180] Step 1000/1249 Loss 13.818 Prec@(1,3) (43.6%, 90.7%), ce_loss 1.796, lat_loss 22.193
09/28 12:50:41 AM | Train: [  4/180] Step 1050/1249 Loss 13.896 Prec@(1,3) (43.3%, 90.5%), ce_loss 1.796, lat_loss 22.193
09/28 12:51:03 AM | Train: [  4/180] Step 1100/1249 Loss 13.934 Prec@(1,3) (43.2%, 90.5%), ce_loss 1.794, lat_loss 22.193
09/28 12:51:27 AM | Train: [  4/180] Step 1150/1249 Loss 13.937 Prec@(1,3) (43.2%, 90.4%), ce_loss 1.792, lat_loss 22.193
09/28 12:51:52 AM | Train: [  4/180] Step 1200/1249 Loss 13.926 Prec@(1,3) (43.3%, 90.4%), ce_loss 1.790, lat_loss 22.193
09/28 12:52:17 AM | Train: [  4/180] Step 1249/1249 Loss 13.906 Prec@(1,3) (43.3%, 90.5%), ce_loss 1.787, lat_loss 22.193
09/28 12:52:17 AM | _w_step_train: [  4/180] Final Prec@1 43.2825% Time 584.96
09/28 12:52:17 AM | Firstly, start to train weights for epoch 4
09/28 12:52:43 AM | Train: [  5/180] Step 050/1249 Loss 13.200 Prec@(1,3) (46.6%, 93.0%), ce_loss 1.784, lat_loss 22.193
09/28 12:53:07 AM | Train: [  5/180] Step 100/1249 Loss 13.429 Prec@(1,3) (46.2%, 91.8%), ce_loss 1.782, lat_loss 22.193
09/28 12:53:31 AM | Train: [  5/180] Step 150/1249 Loss 13.356 Prec@(1,3) (46.0%, 91.7%), ce_loss 1.779, lat_loss 22.193
09/28 12:53:56 AM | Train: [  5/180] Step 200/1249 Loss 13.445 Prec@(1,3) (45.7%, 91.5%), ce_loss 1.776, lat_loss 22.193
09/28 12:54:20 AM | Train: [  5/180] Step 250/1249 Loss 13.558 Prec@(1,3) (45.3%, 91.2%), ce_loss 1.775, lat_loss 22.193
09/28 12:54:45 AM | Train: [  5/180] Step 300/1249 Loss 13.618 Prec@(1,3) (45.2%, 91.1%), ce_loss 1.773, lat_loss 22.193
09/28 12:55:09 AM | Train: [  5/180] Step 350/1249 Loss 13.652 Prec@(1,3) (45.0%, 91.1%), ce_loss 1.771, lat_loss 22.193
09/28 12:55:34 AM | Train: [  5/180] Step 400/1249 Loss 13.648 Prec@(1,3) (45.0%, 91.0%), ce_loss 1.768, lat_loss 22.193
09/28 12:55:57 AM | Train: [  5/180] Step 450/1249 Loss 13.669 Prec@(1,3) (44.8%, 91.1%), ce_loss 1.767, lat_loss 22.193
09/28 12:56:22 AM | Train: [  5/180] Step 500/1249 Loss 13.613 Prec@(1,3) (44.8%, 91.2%), ce_loss 1.764, lat_loss 22.193
09/28 12:56:46 AM | Train: [  5/180] Step 550/1249 Loss 13.578 Prec@(1,3) (44.9%, 91.2%), ce_loss 1.761, lat_loss 22.193
09/28 12:57:10 AM | Train: [  5/180] Step 600/1249 Loss 13.539 Prec@(1,3) (45.2%, 91.3%), ce_loss 1.759, lat_loss 22.193
09/28 12:57:35 AM | Train: [  5/180] Step 650/1249 Loss 13.536 Prec@(1,3) (45.3%, 91.2%), ce_loss 1.757, lat_loss 22.193
09/28 12:57:55 AM | Train: [  5/180] Step 700/1249 Loss 13.483 Prec@(1,3) (45.5%, 91.3%), ce_loss 1.754, lat_loss 22.193
09/28 12:58:11 AM | Train: [  5/180] Step 750/1249 Loss 13.477 Prec@(1,3) (45.5%, 91.3%), ce_loss 1.752, lat_loss 22.193
09/28 12:58:27 AM | Train: [  5/180] Step 800/1249 Loss 13.456 Prec@(1,3) (45.6%, 91.3%), ce_loss 1.750, lat_loss 22.193
09/28 12:58:50 AM | Train: [  5/180] Step 850/1249 Loss 13.445 Prec@(1,3) (45.6%, 91.3%), ce_loss 1.747, lat_loss 22.193
09/28 12:59:14 AM | Train: [  5/180] Step 900/1249 Loss 13.438 Prec@(1,3) (45.5%, 91.4%), ce_loss 1.745, lat_loss 22.193
09/28 12:59:38 AM | Train: [  5/180] Step 950/1249 Loss 13.397 Prec@(1,3) (45.7%, 91.4%), ce_loss 1.743, lat_loss 22.193
09/28 01:00:01 AM | Train: [  5/180] Step 1000/1249 Loss 13.380 Prec@(1,3) (45.8%, 91.5%), ce_loss 1.740, lat_loss 22.193
09/28 01:00:26 AM | Train: [  5/180] Step 1050/1249 Loss 13.373 Prec@(1,3) (45.8%, 91.5%), ce_loss 1.738, lat_loss 22.193
09/28 01:00:50 AM | Train: [  5/180] Step 1100/1249 Loss 13.354 Prec@(1,3) (45.9%, 91.6%), ce_loss 1.736, lat_loss 22.193
09/28 01:01:14 AM | Train: [  5/180] Step 1150/1249 Loss 13.324 Prec@(1,3) (46.0%, 91.7%), ce_loss 1.733, lat_loss 22.193
09/28 01:01:36 AM | Train: [  5/180] Step 1200/1249 Loss 13.300 Prec@(1,3) (46.1%, 91.7%), ce_loss 1.731, lat_loss 22.193
09/28 01:01:59 AM | Train: [  5/180] Step 1249/1249 Loss 13.266 Prec@(1,3) (46.3%, 91.8%), ce_loss 1.728, lat_loss 22.193
09/28 01:01:59 AM | _w_step_train: [  5/180] Final Prec@1 46.2650% Time 582.49
09/28 01:01:59 AM | Firstly, start to train weights for epoch 5
09/28 01:02:26 AM | Train: [  6/180] Step 050/1249 Loss 13.062 Prec@(1,3) (47.7%, 92.6%), ce_loss 1.726, lat_loss 22.193
09/28 01:02:51 AM | Train: [  6/180] Step 100/1249 Loss 12.994 Prec@(1,3) (47.7%, 93.0%), ce_loss 1.724, lat_loss 22.193
09/28 01:03:15 AM | Train: [  6/180] Step 150/1249 Loss 12.881 Prec@(1,3) (48.5%, 93.0%), ce_loss 1.722, lat_loss 22.193
09/28 01:03:39 AM | Train: [  6/180] Step 200/1249 Loss 12.829 Prec@(1,3) (48.6%, 92.8%), ce_loss 1.720, lat_loss 22.193
09/28 01:04:02 AM | Train: [  6/180] Step 250/1249 Loss 12.841 Prec@(1,3) (48.8%, 92.7%), ce_loss 1.718, lat_loss 22.193
09/28 01:04:26 AM | Train: [  6/180] Step 300/1249 Loss 12.833 Prec@(1,3) (48.8%, 92.5%), ce_loss 1.715, lat_loss 22.193
09/28 01:04:49 AM | Train: [  6/180] Step 350/1249 Loss 12.887 Prec@(1,3) (48.7%, 92.3%), ce_loss 1.714, lat_loss 22.193
09/28 01:05:13 AM | Train: [  6/180] Step 400/1249 Loss 12.876 Prec@(1,3) (48.7%, 92.3%), ce_loss 1.712, lat_loss 22.193
09/28 01:05:38 AM | Train: [  6/180] Step 450/1249 Loss 12.847 Prec@(1,3) (48.7%, 92.3%), ce_loss 1.710, lat_loss 22.193
09/28 01:06:03 AM | Train: [  6/180] Step 500/1249 Loss 12.821 Prec@(1,3) (48.8%, 92.3%), ce_loss 1.707, lat_loss 22.193
09/28 01:06:27 AM | Train: [  6/180] Step 550/1249 Loss 12.800 Prec@(1,3) (48.8%, 92.3%), ce_loss 1.705, lat_loss 22.193
09/28 01:06:51 AM | Train: [  6/180] Step 600/1249 Loss 12.806 Prec@(1,3) (48.7%, 92.3%), ce_loss 1.703, lat_loss 22.193
09/28 01:07:15 AM | Train: [  6/180] Step 650/1249 Loss 12.719 Prec@(1,3) (48.9%, 92.5%), ce_loss 1.701, lat_loss 22.193
09/28 01:07:40 AM | Train: [  6/180] Step 700/1249 Loss 12.674 Prec@(1,3) (49.1%, 92.5%), ce_loss 1.698, lat_loss 22.193
09/28 01:08:03 AM | Train: [  6/180] Step 750/1249 Loss 12.635 Prec@(1,3) (49.3%, 92.6%), ce_loss 1.696, lat_loss 22.193
09/28 01:08:28 AM | Train: [  6/180] Step 800/1249 Loss 12.621 Prec@(1,3) (49.3%, 92.6%), ce_loss 1.694, lat_loss 22.193
09/28 01:08:51 AM | Train: [  6/180] Step 850/1249 Loss 12.620 Prec@(1,3) (49.4%, 92.6%), ce_loss 1.692, lat_loss 22.193
09/28 01:09:16 AM | Train: [  6/180] Step 900/1249 Loss 12.590 Prec@(1,3) (49.5%, 92.7%), ce_loss 1.689, lat_loss 22.193
09/28 01:09:41 AM | Train: [  6/180] Step 950/1249 Loss 12.582 Prec@(1,3) (49.5%, 92.7%), ce_loss 1.687, lat_loss 22.193
09/28 01:10:06 AM | Train: [  6/180] Step 1000/1249 Loss 12.556 Prec@(1,3) (49.6%, 92.8%), ce_loss 1.685, lat_loss 22.193
09/28 01:10:30 AM | Train: [  6/180] Step 1050/1249 Loss 12.542 Prec@(1,3) (49.7%, 92.8%), ce_loss 1.683, lat_loss 22.193
09/28 01:10:55 AM | Train: [  6/180] Step 1100/1249 Loss 12.610 Prec@(1,3) (49.4%, 92.6%), ce_loss 1.682, lat_loss 22.193
09/28 01:11:20 AM | Train: [  6/180] Step 1150/1249 Loss 12.629 Prec@(1,3) (49.3%, 92.6%), ce_loss 1.681, lat_loss 22.193
09/28 01:11:41 AM | Train: [  6/180] Step 1200/1249 Loss 12.642 Prec@(1,3) (49.3%, 92.6%), ce_loss 1.679, lat_loss 22.193
09/28 01:12:04 AM | Train: [  6/180] Step 1249/1249 Loss 12.638 Prec@(1,3) (49.3%, 92.6%), ce_loss 1.678, lat_loss 22.193
09/28 01:12:04 AM | _w_step_train: [  6/180] Final Prec@1 49.2825% Time 605.30
09/28 01:12:04 AM | Firstly, start to train weights for epoch 6
09/28 01:12:30 AM | Train: [  7/180] Step 050/1249 Loss 12.121 Prec@(1,3) (50.1%, 93.1%), ce_loss 1.676, lat_loss 22.193
09/28 01:12:55 AM | Train: [  7/180] Step 100/1249 Loss 12.294 Prec@(1,3) (49.9%, 93.4%), ce_loss 1.674, lat_loss 22.193
09/28 01:13:20 AM | Train: [  7/180] Step 150/1249 Loss 12.221 Prec@(1,3) (50.6%, 93.4%), ce_loss 1.672, lat_loss 22.193
09/28 01:13:44 AM | Train: [  7/180] Step 200/1249 Loss 12.322 Prec@(1,3) (50.7%, 93.1%), ce_loss 1.670, lat_loss 22.193
09/28 01:14:08 AM | Train: [  7/180] Step 250/1249 Loss 12.337 Prec@(1,3) (50.6%, 93.1%), ce_loss 1.668, lat_loss 22.193
09/28 01:14:33 AM | Train: [  7/180] Step 300/1249 Loss 12.331 Prec@(1,3) (50.6%, 92.9%), ce_loss 1.667, lat_loss 22.193
09/28 01:14:58 AM | Train: [  7/180] Step 350/1249 Loss 12.289 Prec@(1,3) (50.6%, 93.0%), ce_loss 1.665, lat_loss 22.193
09/28 01:15:23 AM | Train: [  7/180] Step 400/1249 Loss 12.234 Prec@(1,3) (50.6%, 93.0%), ce_loss 1.662, lat_loss 22.193
09/28 01:15:48 AM | Train: [  7/180] Step 450/1249 Loss 12.152 Prec@(1,3) (50.9%, 93.1%), ce_loss 1.660, lat_loss 22.193
09/28 01:16:13 AM | Train: [  7/180] Step 500/1249 Loss 12.130 Prec@(1,3) (51.0%, 93.2%), ce_loss 1.658, lat_loss 22.193
09/28 01:16:39 AM | Train: [  7/180] Step 550/1249 Loss 12.122 Prec@(1,3) (51.0%, 93.2%), ce_loss 1.656, lat_loss 22.193
09/28 01:17:03 AM | Train: [  7/180] Step 600/1249 Loss 12.066 Prec@(1,3) (51.3%, 93.2%), ce_loss 1.654, lat_loss 22.193
09/28 01:17:26 AM | Train: [  7/180] Step 650/1249 Loss 12.049 Prec@(1,3) (51.5%, 93.2%), ce_loss 1.652, lat_loss 22.193
09/28 01:17:49 AM | Train: [  7/180] Step 700/1249 Loss 12.024 Prec@(1,3) (51.8%, 93.2%), ce_loss 1.650, lat_loss 22.193
09/28 01:18:11 AM | Train: [  7/180] Step 750/1249 Loss 12.004 Prec@(1,3) (51.9%, 93.3%), ce_loss 1.648, lat_loss 22.193
09/28 01:18:33 AM | Train: [  7/180] Step 800/1249 Loss 11.963 Prec@(1,3) (52.1%, 93.3%), ce_loss 1.646, lat_loss 22.193
09/28 01:18:56 AM | Train: [  7/180] Step 850/1249 Loss 11.940 Prec@(1,3) (52.2%, 93.3%), ce_loss 1.644, lat_loss 22.193
09/28 01:19:20 AM | Train: [  7/180] Step 900/1249 Loss 11.915 Prec@(1,3) (52.2%, 93.4%), ce_loss 1.642, lat_loss 22.193
09/28 01:19:45 AM | Train: [  7/180] Step 950/1249 Loss 11.907 Prec@(1,3) (52.4%, 93.4%), ce_loss 1.640, lat_loss 22.193
09/28 01:20:10 AM | Train: [  7/180] Step 1000/1249 Loss 11.897 Prec@(1,3) (52.5%, 93.4%), ce_loss 1.638, lat_loss 22.193
09/28 01:20:35 AM | Train: [  7/180] Step 1050/1249 Loss 11.861 Prec@(1,3) (52.7%, 93.4%), ce_loss 1.636, lat_loss 22.193
09/28 01:21:00 AM | Train: [  7/180] Step 1100/1249 Loss 11.839 Prec@(1,3) (52.8%, 93.4%), ce_loss 1.634, lat_loss 22.193
09/28 01:21:26 AM | Train: [  7/180] Step 1150/1249 Loss 11.815 Prec@(1,3) (52.9%, 93.5%), ce_loss 1.632, lat_loss 22.193
09/28 01:21:50 AM | Train: [  7/180] Step 1200/1249 Loss 11.795 Prec@(1,3) (53.0%, 93.5%), ce_loss 1.629, lat_loss 22.193
09/28 01:22:15 AM | Train: [  7/180] Step 1249/1249 Loss 11.802 Prec@(1,3) (53.0%, 93.5%), ce_loss 1.628, lat_loss 22.193
09/28 01:22:15 AM | _w_step_train: [  7/180] Final Prec@1 53.0325% Time 610.51
09/28 01:22:15 AM | Firstly, start to train weights for epoch 7
09/28 01:22:39 AM | Train: [  8/180] Step 050/1249 Loss 11.809 Prec@(1,3) (53.4%, 93.9%), ce_loss 1.626, lat_loss 22.193
09/28 01:23:01 AM | Train: [  8/180] Step 100/1249 Loss 11.716 Prec@(1,3) (54.5%, 93.3%), ce_loss 1.624, lat_loss 22.193
09/28 01:23:22 AM | Train: [  8/180] Step 150/1249 Loss 11.464 Prec@(1,3) (55.0%, 93.6%), ce_loss 1.622, lat_loss 22.193
09/28 01:23:44 AM | Train: [  8/180] Step 200/1249 Loss 11.278 Prec@(1,3) (55.6%, 94.0%), ce_loss 1.620, lat_loss 22.193
09/28 01:24:05 AM | Train: [  8/180] Step 250/1249 Loss 11.295 Prec@(1,3) (55.3%, 94.1%), ce_loss 1.618, lat_loss 22.193
09/28 01:24:30 AM | Train: [  8/180] Step 300/1249 Loss 11.312 Prec@(1,3) (55.3%, 94.1%), ce_loss 1.616, lat_loss 22.193
09/28 01:24:55 AM | Train: [  8/180] Step 350/1249 Loss 11.285 Prec@(1,3) (55.5%, 94.2%), ce_loss 1.614, lat_loss 22.193
09/28 01:25:19 AM | Train: [  8/180] Step 400/1249 Loss 11.340 Prec@(1,3) (55.4%, 94.0%), ce_loss 1.613, lat_loss 22.193
09/28 01:25:43 AM | Train: [  8/180] Step 450/1249 Loss 11.320 Prec@(1,3) (55.4%, 94.0%), ce_loss 1.611, lat_loss 22.193
09/28 01:26:07 AM | Train: [  8/180] Step 500/1249 Loss 11.274 Prec@(1,3) (55.6%, 94.1%), ce_loss 1.609, lat_loss 22.192
09/28 01:26:30 AM | Train: [  8/180] Step 550/1249 Loss 11.259 Prec@(1,3) (55.6%, 94.1%), ce_loss 1.607, lat_loss 22.192
09/28 01:26:52 AM | Train: [  8/180] Step 600/1249 Loss 11.327 Prec@(1,3) (55.4%, 93.9%), ce_loss 1.605, lat_loss 22.192
09/28 01:27:16 AM | Train: [  8/180] Step 650/1249 Loss 11.368 Prec@(1,3) (55.2%, 93.9%), ce_loss 1.604, lat_loss 22.192
09/28 01:27:39 AM | Train: [  8/180] Step 700/1249 Loss 11.313 Prec@(1,3) (55.5%, 93.9%), ce_loss 1.602, lat_loss 22.192
09/28 01:28:02 AM | Train: [  8/180] Step 750/1249 Loss 11.302 Prec@(1,3) (55.6%, 93.9%), ce_loss 1.600, lat_loss 22.192
09/28 01:28:27 AM | Train: [  8/180] Step 800/1249 Loss 11.285 Prec@(1,3) (55.7%, 93.9%), ce_loss 1.598, lat_loss 22.192
09/28 01:28:50 AM | Train: [  8/180] Step 850/1249 Loss 11.259 Prec@(1,3) (55.8%, 93.9%), ce_loss 1.596, lat_loss 22.192
09/28 01:29:14 AM | Train: [  8/180] Step 900/1249 Loss 11.243 Prec@(1,3) (55.8%, 94.0%), ce_loss 1.594, lat_loss 22.192
09/28 01:29:38 AM | Train: [  8/180] Step 950/1249 Loss 11.258 Prec@(1,3) (55.8%, 94.0%), ce_loss 1.593, lat_loss 22.192
09/28 01:30:01 AM | Train: [  8/180] Step 1000/1249 Loss 11.212 Prec@(1,3) (55.9%, 94.1%), ce_loss 1.590, lat_loss 22.192
09/28 01:30:25 AM | Train: [  8/180] Step 1050/1249 Loss 11.213 Prec@(1,3) (55.9%, 94.0%), ce_loss 1.589, lat_loss 22.192
09/28 01:30:47 AM | Train: [  8/180] Step 1100/1249 Loss 11.218 Prec@(1,3) (55.9%, 94.1%), ce_loss 1.587, lat_loss 22.192
09/28 01:31:11 AM | Train: [  8/180] Step 1150/1249 Loss 11.192 Prec@(1,3) (56.0%, 94.1%), ce_loss 1.585, lat_loss 22.192
09/28 01:31:35 AM | Train: [  8/180] Step 1200/1249 Loss 11.166 Prec@(1,3) (56.1%, 94.1%), ce_loss 1.583, lat_loss 22.192
09/28 01:31:59 AM | Train: [  8/180] Step 1249/1249 Loss 11.139 Prec@(1,3) (56.3%, 94.1%), ce_loss 1.581, lat_loss 22.192
09/28 01:31:59 AM | _w_step_train: [  8/180] Final Prec@1 56.2700% Time 584.30
09/28 01:31:59 AM | Firstly, start to train weights for epoch 8
09/28 01:32:25 AM | Train: [  9/180] Step 050/1249 Loss 10.201 Prec@(1,3) (60.4%, 95.1%), ce_loss 1.579, lat_loss 22.192
09/28 01:32:50 AM | Train: [  9/180] Step 100/1249 Loss 10.550 Prec@(1,3) (58.6%, 94.4%), ce_loss 1.577, lat_loss 22.192
09/28 01:33:14 AM | Train: [  9/180] Step 150/1249 Loss 10.605 Prec@(1,3) (58.3%, 94.4%), ce_loss 1.576, lat_loss 22.192
09/28 01:33:38 AM | Train: [  9/180] Step 200/1249 Loss 10.664 Prec@(1,3) (58.1%, 94.1%), ce_loss 1.574, lat_loss 22.192
09/28 01:34:02 AM | Train: [  9/180] Step 250/1249 Loss 10.589 Prec@(1,3) (58.6%, 94.4%), ce_loss 1.572, lat_loss 22.192
09/28 01:34:26 AM | Train: [  9/180] Step 300/1249 Loss 10.599 Prec@(1,3) (58.4%, 94.4%), ce_loss 1.570, lat_loss 22.192
09/28 01:34:51 AM | Train: [  9/180] Step 350/1249 Loss 10.588 Prec@(1,3) (58.3%, 94.5%), ce_loss 1.568, lat_loss 22.192
09/28 01:35:15 AM | Train: [  9/180] Step 400/1249 Loss 10.571 Prec@(1,3) (58.4%, 94.5%), ce_loss 1.566, lat_loss 22.192
09/28 01:35:31 AM | Train: [  9/180] Step 450/1249 Loss 10.566 Prec@(1,3) (58.3%, 94.7%), ce_loss 1.564, lat_loss 22.192
09/28 01:35:47 AM | Train: [  9/180] Step 500/1249 Loss 10.524 Prec@(1,3) (58.4%, 94.7%), ce_loss 1.562, lat_loss 22.192
09/28 01:36:03 AM | Train: [  9/180] Step 550/1249 Loss 10.524 Prec@(1,3) (58.5%, 94.7%), ce_loss 1.561, lat_loss 22.192
09/28 01:36:20 AM | Train: [  9/180] Step 600/1249 Loss 10.518 Prec@(1,3) (58.5%, 94.8%), ce_loss 1.559, lat_loss 22.192
09/28 01:36:36 AM | Train: [  9/180] Step 650/1249 Loss 10.472 Prec@(1,3) (58.8%, 94.8%), ce_loss 1.557, lat_loss 22.192
09/28 01:36:52 AM | Train: [  9/180] Step 700/1249 Loss 10.469 Prec@(1,3) (58.7%, 94.9%), ce_loss 1.555, lat_loss 22.192
09/28 01:37:08 AM | Train: [  9/180] Step 750/1249 Loss 10.466 Prec@(1,3) (58.7%, 94.9%), ce_loss 1.553, lat_loss 22.192
09/28 01:37:24 AM | Train: [  9/180] Step 800/1249 Loss 10.494 Prec@(1,3) (58.7%, 94.8%), ce_loss 1.552, lat_loss 22.192
09/28 01:37:40 AM | Train: [  9/180] Step 850/1249 Loss 10.508 Prec@(1,3) (58.6%, 94.8%), ce_loss 1.550, lat_loss 22.192
09/28 01:37:56 AM | Train: [  9/180] Step 900/1249 Loss 10.511 Prec@(1,3) (58.6%, 94.9%), ce_loss 1.548, lat_loss 22.192
09/28 01:38:12 AM | Train: [  9/180] Step 950/1249 Loss 10.465 Prec@(1,3) (58.8%, 94.9%), ce_loss 1.546, lat_loss 22.192
09/28 01:38:27 AM | Train: [  9/180] Step 1000/1249 Loss 10.513 Prec@(1,3) (58.6%, 94.8%), ce_loss 1.545, lat_loss 22.192
09/28 01:38:43 AM | Train: [  9/180] Step 1050/1249 Loss 10.504 Prec@(1,3) (58.7%, 94.9%), ce_loss 1.543, lat_loss 22.192
09/28 01:38:59 AM | Train: [  9/180] Step 1100/1249 Loss 10.513 Prec@(1,3) (58.6%, 94.9%), ce_loss 1.542, lat_loss 22.192
09/28 01:39:21 AM | Train: [  9/180] Step 1150/1249 Loss 10.517 Prec@(1,3) (58.6%, 94.9%), ce_loss 1.540, lat_loss 22.192
09/28 01:39:45 AM | Train: [  9/180] Step 1200/1249 Loss 10.516 Prec@(1,3) (58.7%, 94.9%), ce_loss 1.539, lat_loss 22.192
09/28 01:40:10 AM | Train: [  9/180] Step 1249/1249 Loss 10.507 Prec@(1,3) (58.7%, 94.9%), ce_loss 1.537, lat_loss 22.192
09/28 01:40:10 AM | _w_step_train: [  9/180] Final Prec@1 58.6900% Time 490.46
09/28 01:40:10 AM | Firstly, start to train weights for epoch 9
09/28 01:40:34 AM | Train: [ 10/180] Step 050/1249 Loss 10.191 Prec@(1,3) (59.5%, 94.5%), ce_loss 1.535, lat_loss 22.192
09/28 01:40:57 AM | Train: [ 10/180] Step 100/1249 Loss 10.256 Prec@(1,3) (59.2%, 94.9%), ce_loss 1.534, lat_loss 22.192
09/28 01:41:19 AM | Train: [ 10/180] Step 150/1249 Loss 10.197 Prec@(1,3) (60.1%, 95.1%), ce_loss 1.532, lat_loss 22.192
09/28 01:41:42 AM | Train: [ 10/180] Step 200/1249 Loss 10.082 Prec@(1,3) (60.6%, 95.4%), ce_loss 1.530, lat_loss 22.192
09/28 01:42:03 AM | Train: [ 10/180] Step 250/1249 Loss 10.156 Prec@(1,3) (60.3%, 95.3%), ce_loss 1.529, lat_loss 22.192
09/28 01:42:25 AM | Train: [ 10/180] Step 300/1249 Loss 10.171 Prec@(1,3) (60.3%, 95.2%), ce_loss 1.527, lat_loss 22.192
09/28 01:42:50 AM | Train: [ 10/180] Step 350/1249 Loss 10.193 Prec@(1,3) (60.1%, 95.2%), ce_loss 1.525, lat_loss 22.192
09/28 01:43:15 AM | Train: [ 10/180] Step 400/1249 Loss 10.207 Prec@(1,3) (60.0%, 95.3%), ce_loss 1.524, lat_loss 22.192
09/28 01:43:40 AM | Train: [ 10/180] Step 450/1249 Loss 10.169 Prec@(1,3) (60.1%, 95.3%), ce_loss 1.522, lat_loss 22.192
09/28 01:44:05 AM | Train: [ 10/180] Step 500/1249 Loss 10.163 Prec@(1,3) (60.1%, 95.3%), ce_loss 1.520, lat_loss 22.192
09/28 01:44:31 AM | Train: [ 10/180] Step 550/1249 Loss 10.108 Prec@(1,3) (60.3%, 95.3%), ce_loss 1.518, lat_loss 22.192
09/28 01:44:56 AM | Train: [ 10/180] Step 600/1249 Loss 10.072 Prec@(1,3) (60.5%, 95.3%), ce_loss 1.517, lat_loss 22.192
09/28 01:45:22 AM | Train: [ 10/180] Step 650/1249 Loss 10.045 Prec@(1,3) (60.7%, 95.3%), ce_loss 1.515, lat_loss 22.192
09/28 01:45:47 AM | Train: [ 10/180] Step 700/1249 Loss 10.047 Prec@(1,3) (60.6%, 95.3%), ce_loss 1.513, lat_loss 22.192
09/28 01:46:12 AM | Train: [ 10/180] Step 750/1249 Loss 10.058 Prec@(1,3) (60.5%, 95.3%), ce_loss 1.512, lat_loss 22.192
09/28 01:46:37 AM | Train: [ 10/180] Step 800/1249 Loss 10.046 Prec@(1,3) (60.6%, 95.3%), ce_loss 1.510, lat_loss 22.192
09/28 01:47:02 AM | Train: [ 10/180] Step 850/1249 Loss 10.024 Prec@(1,3) (60.8%, 95.3%), ce_loss 1.508, lat_loss 22.192
09/28 01:47:28 AM | Train: [ 10/180] Step 900/1249 Loss 10.048 Prec@(1,3) (60.7%, 95.3%), ce_loss 1.507, lat_loss 22.192
09/28 01:47:53 AM | Train: [ 10/180] Step 950/1249 Loss 10.028 Prec@(1,3) (60.7%, 95.4%), ce_loss 1.505, lat_loss 22.192
09/28 01:48:18 AM | Train: [ 10/180] Step 1000/1249 Loss 10.040 Prec@(1,3) (60.7%, 95.4%), ce_loss 1.504, lat_loss 22.192
09/28 01:48:43 AM | Train: [ 10/180] Step 1050/1249 Loss 10.105 Prec@(1,3) (60.4%, 95.4%), ce_loss 1.503, lat_loss 22.192
09/28 01:49:08 AM | Train: [ 10/180] Step 1100/1249 Loss 10.122 Prec@(1,3) (60.3%, 95.4%), ce_loss 1.502, lat_loss 22.192
09/28 01:49:31 AM | Train: [ 10/180] Step 1150/1249 Loss 10.119 Prec@(1,3) (60.3%, 95.4%), ce_loss 1.500, lat_loss 22.192
09/28 01:49:55 AM | Train: [ 10/180] Step 1200/1249 Loss 10.086 Prec@(1,3) (60.3%, 95.4%), ce_loss 1.498, lat_loss 22.192
09/28 01:50:19 AM | Train: [ 10/180] Step 1249/1249 Loss 10.066 Prec@(1,3) (60.4%, 95.4%), ce_loss 1.497, lat_loss 22.192
09/28 01:50:19 AM | _w_step_train: [ 10/180] Final Prec@1 60.4350% Time 609.24
09/28 01:50:19 AM | Start to train weights for epoch 10
09/28 01:50:45 AM | Train: [ 11/180] Step 050/1249 Loss 9.721 Prec@(1,3) (62.1%, 96.0%), ce_loss 1.495, lat_loss 22.192
09/28 01:51:08 AM | Train: [ 11/180] Step 100/1249 Loss 9.688 Prec@(1,3) (62.3%, 96.0%), ce_loss 1.494, lat_loss 22.192
09/28 01:51:31 AM | Train: [ 11/180] Step 150/1249 Loss 9.719 Prec@(1,3) (61.9%, 95.8%), ce_loss 1.492, lat_loss 22.192
09/28 01:51:52 AM | Train: [ 11/180] Step 200/1249 Loss 9.749 Prec@(1,3) (61.8%, 95.8%), ce_loss 1.490, lat_loss 22.192
09/28 01:52:14 AM | Train: [ 11/180] Step 250/1249 Loss 9.768 Prec@(1,3) (61.8%, 95.9%), ce_loss 1.489, lat_loss 22.192
09/28 01:52:35 AM | Train: [ 11/180] Step 300/1249 Loss 9.748 Prec@(1,3) (62.1%, 95.7%), ce_loss 1.487, lat_loss 22.192
09/28 01:52:57 AM | Train: [ 11/180] Step 350/1249 Loss 9.737 Prec@(1,3) (62.1%, 95.6%), ce_loss 1.486, lat_loss 22.192
09/28 01:53:19 AM | Train: [ 11/180] Step 400/1249 Loss 9.663 Prec@(1,3) (62.5%, 95.7%), ce_loss 1.484, lat_loss 22.192
09/28 01:53:40 AM | Train: [ 11/180] Step 450/1249 Loss 9.648 Prec@(1,3) (62.4%, 95.8%), ce_loss 1.483, lat_loss 22.192
09/28 01:54:05 AM | Train: [ 11/180] Step 500/1249 Loss 9.662 Prec@(1,3) (62.4%, 95.7%), ce_loss 1.481, lat_loss 22.192
09/28 01:54:30 AM | Train: [ 11/180] Step 550/1249 Loss 9.713 Prec@(1,3) (62.2%, 95.6%), ce_loss 1.480, lat_loss 22.192
09/28 01:54:54 AM | Train: [ 11/180] Step 600/1249 Loss 9.747 Prec@(1,3) (62.0%, 95.6%), ce_loss 1.478, lat_loss 22.192
09/28 01:55:16 AM | Train: [ 11/180] Step 650/1249 Loss 9.752 Prec@(1,3) (61.9%, 95.6%), ce_loss 1.477, lat_loss 22.192
09/28 01:55:41 AM | Train: [ 11/180] Step 700/1249 Loss 9.733 Prec@(1,3) (62.1%, 95.6%), ce_loss 1.476, lat_loss 22.192
09/28 01:56:06 AM | Train: [ 11/180] Step 750/1249 Loss 9.739 Prec@(1,3) (62.0%, 95.6%), ce_loss 1.474, lat_loss 22.192
09/28 01:56:30 AM | Train: [ 11/180] Step 800/1249 Loss 9.705 Prec@(1,3) (62.1%, 95.6%), ce_loss 1.472, lat_loss 22.192
09/28 01:56:55 AM | Train: [ 11/180] Step 850/1249 Loss 9.707 Prec@(1,3) (62.2%, 95.6%), ce_loss 1.471, lat_loss 22.192
09/28 01:57:20 AM | Train: [ 11/180] Step 900/1249 Loss 9.714 Prec@(1,3) (62.2%, 95.6%), ce_loss 1.470, lat_loss 22.192
09/28 01:57:44 AM | Train: [ 11/180] Step 950/1249 Loss 9.717 Prec@(1,3) (62.3%, 95.6%), ce_loss 1.468, lat_loss 22.192
09/28 01:58:08 AM | Train: [ 11/180] Step 1000/1249 Loss 9.727 Prec@(1,3) (62.1%, 95.6%), ce_loss 1.467, lat_loss 22.192
09/28 01:58:33 AM | Train: [ 11/180] Step 1050/1249 Loss 9.709 Prec@(1,3) (62.2%, 95.6%), ce_loss 1.466, lat_loss 22.192
09/28 01:58:58 AM | Train: [ 11/180] Step 1100/1249 Loss 9.698 Prec@(1,3) (62.3%, 95.6%), ce_loss 1.464, lat_loss 22.192
09/28 01:59:23 AM | Train: [ 11/180] Step 1150/1249 Loss 9.670 Prec@(1,3) (62.4%, 95.6%), ce_loss 1.462, lat_loss 22.192
09/28 01:59:47 AM | Train: [ 11/180] Step 1200/1249 Loss 9.665 Prec@(1,3) (62.4%, 95.6%), ce_loss 1.461, lat_loss 22.192
09/28 02:00:12 AM | Train: [ 11/180] Step 1249/1249 Loss 9.650 Prec@(1,3) (62.4%, 95.6%), ce_loss 1.460, lat_loss 22.192
09/28 02:00:12 AM | _w_step_train: [ 11/180] Final Prec@1 62.4150% Time 592.69
09/28 02:00:12 AM | Start to train theta for epoch 10
09/28 02:00:32 AM | Train: [ 11/180] Step 050/312 Loss 9.508 Prec@(1,3) (63.0%, 95.6%), ce_loss 1.458, lat_loss 22.192
09/28 02:00:51 AM | Train: [ 11/180] Step 100/312 Loss 9.373 Prec@(1,3) (63.6%, 95.6%), ce_loss 1.457, lat_loss 22.192
09/28 02:01:09 AM | Train: [ 11/180] Step 150/312 Loss 9.520 Prec@(1,3) (62.4%, 95.6%), ce_loss 1.455, lat_loss 22.192
09/28 02:01:28 AM | Train: [ 11/180] Step 200/312 Loss 9.562 Prec@(1,3) (62.0%, 95.6%), ce_loss 1.454, lat_loss 22.192
09/28 02:01:47 AM | Train: [ 11/180] Step 250/312 Loss 9.561 Prec@(1,3) (62.0%, 95.7%), ce_loss 1.453, lat_loss 22.192
09/28 02:02:06 AM | Train: [ 11/180] Step 300/312 Loss 9.495 Prec@(1,3) (62.3%, 95.8%), ce_loss 1.451, lat_loss 22.192
09/28 02:02:12 AM | Train: [ 11/180] Step 312/312 Loss 9.472 Prec@(1,3) (62.4%, 95.8%), ce_loss 1.451, lat_loss 22.192
09/28 02:02:12 AM | _theta_step_train: [ 11/180] Final Prec@1 62.4100% Time 120.03
09/28 02:02:17 AM | Valid: [ 11/180] Step 050/312 Loss 9.448 Prec@(1,3) (63.1%, 96.3%), ce_loss 1.449, lat_loss 22.192
09/28 02:02:22 AM | Valid: [ 11/180] Step 100/312 Loss 9.513 Prec@(1,3) (62.7%, 95.7%), ce_loss 1.448, lat_loss 22.192
09/28 02:02:26 AM | Valid: [ 11/180] Step 150/312 Loss 9.509 Prec@(1,3) (63.0%, 95.5%), ce_loss 1.447, lat_loss 22.192
09/28 02:02:31 AM | Valid: [ 11/180] Step 200/312 Loss 9.621 Prec@(1,3) (62.4%, 95.7%), ce_loss 1.446, lat_loss 22.192
09/28 02:02:35 AM | Valid: [ 11/180] Step 250/312 Loss 9.661 Prec@(1,3) (62.0%, 95.6%), ce_loss 1.444, lat_loss 22.192
09/28 02:02:40 AM | Valid: [ 11/180] Step 300/312 Loss 9.651 Prec@(1,3) (62.1%, 95.5%), ce_loss 1.443, lat_loss 22.192
09/28 02:02:41 AM | Valid: [ 11/180] Step 312/312 Loss 9.644 Prec@(1,3) (62.1%, 95.5%), ce_loss 1.443, lat_loss 22.192
09/28 02:02:41 AM | val: [ 11/180] Final Prec@1 62.1400% Time 29.32
09/28 02:02:41 AM | Best top1 acc by now. Save model
09/28 02:02:41 AM | Start to train weights for epoch 11
09/28 02:03:08 AM | Train: [ 12/180] Step 050/1249 Loss 9.433 Prec@(1,3) (64.0%, 95.8%), ce_loss 1.442, lat_loss 22.192
09/28 02:03:32 AM | Train: [ 12/180] Step 100/1249 Loss 9.298 Prec@(1,3) (63.8%, 96.0%), ce_loss 1.440, lat_loss 22.192
09/28 02:03:55 AM | Train: [ 12/180] Step 150/1249 Loss 9.340 Prec@(1,3) (64.0%, 95.7%), ce_loss 1.439, lat_loss 22.192
09/28 02:04:20 AM | Train: [ 12/180] Step 200/1249 Loss 9.285 Prec@(1,3) (64.2%, 95.9%), ce_loss 1.437, lat_loss 22.192
09/28 02:04:45 AM | Train: [ 12/180] Step 250/1249 Loss 9.427 Prec@(1,3) (63.9%, 95.7%), ce_loss 1.436, lat_loss 22.192
09/28 02:05:10 AM | Train: [ 12/180] Step 300/1249 Loss 9.401 Prec@(1,3) (63.9%, 95.7%), ce_loss 1.435, lat_loss 22.192
09/28 02:05:35 AM | Train: [ 12/180] Step 350/1249 Loss 9.377 Prec@(1,3) (63.8%, 95.7%), ce_loss 1.434, lat_loss 22.192
09/28 02:05:57 AM | Train: [ 12/180] Step 400/1249 Loss 9.363 Prec@(1,3) (63.9%, 95.7%), ce_loss 1.432, lat_loss 22.192
09/28 02:06:11 AM | Train: [ 12/180] Step 450/1249 Loss 9.375 Prec@(1,3) (63.8%, 95.7%), ce_loss 1.431, lat_loss 22.192
09/28 02:06:26 AM | Train: [ 12/180] Step 500/1249 Loss 9.378 Prec@(1,3) (63.8%, 95.7%), ce_loss 1.430, lat_loss 22.192
09/28 02:06:41 AM | Train: [ 12/180] Step 550/1249 Loss 9.363 Prec@(1,3) (63.9%, 95.7%), ce_loss 1.429, lat_loss 22.192
09/28 02:06:55 AM | Train: [ 12/180] Step 600/1249 Loss 9.333 Prec@(1,3) (64.0%, 95.8%), ce_loss 1.427, lat_loss 22.192
09/28 02:07:10 AM | Train: [ 12/180] Step 650/1249 Loss 9.349 Prec@(1,3) (63.9%, 95.7%), ce_loss 1.426, lat_loss 22.192
09/28 02:07:25 AM | Train: [ 12/180] Step 700/1249 Loss 9.308 Prec@(1,3) (64.0%, 95.9%), ce_loss 1.425, lat_loss 22.192
09/28 02:07:40 AM | Train: [ 12/180] Step 750/1249 Loss 9.306 Prec@(1,3) (64.0%, 95.9%), ce_loss 1.423, lat_loss 22.192
09/28 02:07:55 AM | Train: [ 12/180] Step 800/1249 Loss 9.281 Prec@(1,3) (64.0%, 95.9%), ce_loss 1.422, lat_loss 22.192
09/28 02:08:10 AM | Train: [ 12/180] Step 850/1249 Loss 9.271 Prec@(1,3) (64.0%, 96.0%), ce_loss 1.421, lat_loss 22.192
09/28 02:08:25 AM | Train: [ 12/180] Step 900/1249 Loss 9.245 Prec@(1,3) (64.1%, 96.0%), ce_loss 1.419, lat_loss 22.192
09/28 02:08:40 AM | Train: [ 12/180] Step 950/1249 Loss 9.207 Prec@(1,3) (64.2%, 96.1%), ce_loss 1.418, lat_loss 22.192
09/28 02:08:55 AM | Train: [ 12/180] Step 1000/1249 Loss 9.192 Prec@(1,3) (64.3%, 96.1%), ce_loss 1.416, lat_loss 22.192
09/28 02:09:10 AM | Train: [ 12/180] Step 1050/1249 Loss 9.205 Prec@(1,3) (64.2%, 96.0%), ce_loss 1.415, lat_loss 22.192
09/28 02:09:25 AM | Train: [ 12/180] Step 1100/1249 Loss 9.205 Prec@(1,3) (64.3%, 96.0%), ce_loss 1.414, lat_loss 22.192
09/28 02:09:40 AM | Train: [ 12/180] Step 1150/1249 Loss 9.194 Prec@(1,3) (64.3%, 96.0%), ce_loss 1.413, lat_loss 22.192
09/28 02:09:56 AM | Train: [ 12/180] Step 1200/1249 Loss 9.192 Prec@(1,3) (64.3%, 96.0%), ce_loss 1.412, lat_loss 22.192
09/28 02:10:11 AM | Train: [ 12/180] Step 1249/1249 Loss 9.180 Prec@(1,3) (64.4%, 96.0%), ce_loss 1.410, lat_loss 22.192
09/28 02:10:11 AM | _w_step_train: [ 12/180] Final Prec@1 64.3525% Time 449.63
09/28 02:10:11 AM | Start to train theta for epoch 11
09/28 02:10:33 AM | Train: [ 12/180] Step 050/312 Loss 9.737 Prec@(1,3) (62.6%, 95.5%), ce_loss 1.409, lat_loss 22.192
09/28 02:10:54 AM | Train: [ 12/180] Step 100/312 Loss 9.059 Prec@(1,3) (65.0%, 96.3%), ce_loss 1.408, lat_loss 22.191
09/28 02:11:14 AM | Train: [ 12/180] Step 150/312 Loss 9.133 Prec@(1,3) (64.7%, 96.3%), ce_loss 1.407, lat_loss 22.191
09/28 02:11:34 AM | Train: [ 12/180] Step 200/312 Loss 9.129 Prec@(1,3) (64.6%, 96.4%), ce_loss 1.405, lat_loss 22.191
09/28 02:11:55 AM | Train: [ 12/180] Step 250/312 Loss 9.031 Prec@(1,3) (64.7%, 96.5%), ce_loss 1.404, lat_loss 22.191
09/28 02:12:16 AM | Train: [ 12/180] Step 300/312 Loss 9.143 Prec@(1,3) (64.2%, 96.4%), ce_loss 1.403, lat_loss 22.191
09/28 02:12:21 AM | Train: [ 12/180] Step 312/312 Loss 9.111 Prec@(1,3) (64.4%, 96.5%), ce_loss 1.403, lat_loss 22.191
09/28 02:12:21 AM | _theta_step_train: [ 12/180] Final Prec@1 64.3900% Time 129.81
09/28 02:12:26 AM | Valid: [ 12/180] Step 050/312 Loss 8.552 Prec@(1,3) (66.4%, 97.0%), ce_loss 1.401, lat_loss 22.191
09/28 02:12:31 AM | Valid: [ 12/180] Step 100/312 Loss 8.397 Prec@(1,3) (66.6%, 96.8%), ce_loss 1.400, lat_loss 22.191
09/28 02:12:35 AM | Valid: [ 12/180] Step 150/312 Loss 8.482 Prec@(1,3) (67.1%, 96.5%), ce_loss 1.399, lat_loss 22.191
09/28 02:12:40 AM | Valid: [ 12/180] Step 200/312 Loss 8.614 Prec@(1,3) (66.7%, 96.5%), ce_loss 1.397, lat_loss 22.190
09/28 02:12:44 AM | Valid: [ 12/180] Step 250/312 Loss 8.647 Prec@(1,3) (66.4%, 96.6%), ce_loss 1.396, lat_loss 22.190
09/28 02:12:49 AM | Valid: [ 12/180] Step 300/312 Loss 8.565 Prec@(1,3) (66.7%, 96.7%), ce_loss 1.395, lat_loss 22.190
09/28 02:12:50 AM | Valid: [ 12/180] Step 312/312 Loss 8.570 Prec@(1,3) (66.6%, 96.7%), ce_loss 1.394, lat_loss 22.190
09/28 02:12:50 AM | val: [ 12/180] Final Prec@1 66.6300% Time 29.43
09/28 02:12:50 AM | Best top1 acc by now. Save model
09/28 02:12:50 AM | Start to train weights for epoch 12
09/28 02:13:16 AM | Train: [ 13/180] Step 050/1249 Loss 9.107 Prec@(1,3) (63.8%, 96.1%), ce_loss 1.393, lat_loss 22.190
09/28 02:13:37 AM | Train: [ 13/180] Step 100/1249 Loss 9.141 Prec@(1,3) (64.4%, 96.4%), ce_loss 1.392, lat_loss 22.190
09/28 02:14:01 AM | Train: [ 13/180] Step 150/1249 Loss 9.310 Prec@(1,3) (63.7%, 96.3%), ce_loss 1.391, lat_loss 22.190
09/28 02:14:25 AM | Train: [ 13/180] Step 200/1249 Loss 9.173 Prec@(1,3) (64.2%, 96.3%), ce_loss 1.390, lat_loss 22.190
09/28 02:14:49 AM | Train: [ 13/180] Step 250/1249 Loss 9.068 Prec@(1,3) (64.8%, 96.5%), ce_loss 1.389, lat_loss 22.189
09/28 02:15:13 AM | Train: [ 13/180] Step 300/1249 Loss 9.068 Prec@(1,3) (64.8%, 96.5%), ce_loss 1.388, lat_loss 22.189
09/28 02:15:35 AM | Train: [ 13/180] Step 350/1249 Loss 9.061 Prec@(1,3) (64.8%, 96.4%), ce_loss 1.387, lat_loss 22.189
09/28 02:15:58 AM | Train: [ 13/180] Step 400/1249 Loss 9.060 Prec@(1,3) (64.6%, 96.5%), ce_loss 1.385, lat_loss 22.189
09/28 02:16:21 AM | Train: [ 13/180] Step 450/1249 Loss 9.037 Prec@(1,3) (64.7%, 96.5%), ce_loss 1.384, lat_loss 22.189
09/28 02:16:44 AM | Train: [ 13/180] Step 500/1249 Loss 9.007 Prec@(1,3) (64.9%, 96.5%), ce_loss 1.383, lat_loss 22.189
09/28 02:17:07 AM | Train: [ 13/180] Step 550/1249 Loss 9.009 Prec@(1,3) (64.7%, 96.5%), ce_loss 1.382, lat_loss 22.189
09/28 02:17:30 AM | Train: [ 13/180] Step 600/1249 Loss 8.988 Prec@(1,3) (64.8%, 96.5%), ce_loss 1.381, lat_loss 22.189
09/28 02:17:54 AM | Train: [ 13/180] Step 650/1249 Loss 8.934 Prec@(1,3) (65.0%, 96.5%), ce_loss 1.379, lat_loss 22.188
09/28 02:18:16 AM | Train: [ 13/180] Step 700/1249 Loss 8.928 Prec@(1,3) (65.0%, 96.5%), ce_loss 1.378, lat_loss 22.188
09/28 02:18:37 AM | Train: [ 13/180] Step 750/1249 Loss 8.940 Prec@(1,3) (64.9%, 96.5%), ce_loss 1.377, lat_loss 22.188
09/28 02:18:59 AM | Train: [ 13/180] Step 800/1249 Loss 8.986 Prec@(1,3) (64.8%, 96.5%), ce_loss 1.377, lat_loss 22.188
09/28 02:19:23 AM | Train: [ 13/180] Step 850/1249 Loss 9.052 Prec@(1,3) (64.6%, 96.4%), ce_loss 1.376, lat_loss 22.188
09/28 02:19:47 AM | Train: [ 13/180] Step 900/1249 Loss 9.067 Prec@(1,3) (64.5%, 96.4%), ce_loss 1.375, lat_loss 22.188
09/28 02:20:11 AM | Train: [ 13/180] Step 950/1249 Loss 9.074 Prec@(1,3) (64.5%, 96.4%), ce_loss 1.374, lat_loss 22.188
09/28 02:20:36 AM | Train: [ 13/180] Step 1000/1249 Loss 9.070 Prec@(1,3) (64.6%, 96.4%), ce_loss 1.373, lat_loss 22.188
09/28 02:20:59 AM | Train: [ 13/180] Step 1050/1249 Loss 9.107 Prec@(1,3) (64.5%, 96.4%), ce_loss 1.372, lat_loss 22.187
09/28 02:21:24 AM | Train: [ 13/180] Step 1100/1249 Loss 9.128 Prec@(1,3) (64.4%, 96.4%), ce_loss 1.371, lat_loss 22.187
09/28 02:21:47 AM | Train: [ 13/180] Step 1150/1249 Loss 9.143 Prec@(1,3) (64.5%, 96.4%), ce_loss 1.370, lat_loss 22.187
09/28 02:22:12 AM | Train: [ 13/180] Step 1200/1249 Loss 9.131 Prec@(1,3) (64.6%, 96.4%), ce_loss 1.369, lat_loss 22.187
09/28 02:22:36 AM | Train: [ 13/180] Step 1249/1249 Loss 9.093 Prec@(1,3) (64.7%, 96.4%), ce_loss 1.368, lat_loss 22.187
09/28 02:22:36 AM | _w_step_train: [ 13/180] Final Prec@1 64.6950% Time 585.89
09/28 02:22:36 AM | Start to train theta for epoch 12
09/28 02:22:57 AM | Train: [ 13/180] Step 050/312 Loss 9.142 Prec@(1,3) (64.2%, 96.1%), ce_loss 1.367, lat_loss 22.187
09/28 02:23:18 AM | Train: [ 13/180] Step 100/312 Loss 9.011 Prec@(1,3) (64.8%, 96.3%), ce_loss 1.366, lat_loss 22.187
09/28 02:23:38 AM | Train: [ 13/180] Step 150/312 Loss 8.947 Prec@(1,3) (64.9%, 96.5%), ce_loss 1.365, lat_loss 22.187
09/28 02:23:59 AM | Train: [ 13/180] Step 200/312 Loss 8.781 Prec@(1,3) (65.3%, 96.7%), ce_loss 1.364, lat_loss 22.187
09/28 02:24:19 AM | Train: [ 13/180] Step 250/312 Loss 8.804 Prec@(1,3) (65.0%, 96.8%), ce_loss 1.363, lat_loss 22.186
09/28 02:24:40 AM | Train: [ 13/180] Step 300/312 Loss 8.820 Prec@(1,3) (65.2%, 96.7%), ce_loss 1.362, lat_loss 22.186
09/28 02:24:45 AM | Train: [ 13/180] Step 312/312 Loss 8.830 Prec@(1,3) (65.2%, 96.7%), ce_loss 1.362, lat_loss 22.186
09/28 02:24:45 AM | _theta_step_train: [ 13/180] Final Prec@1 65.1700% Time 129.02
09/28 02:24:51 AM | Valid: [ 13/180] Step 050/312 Loss 8.749 Prec@(1,3) (64.0%, 96.6%), ce_loss 1.361, lat_loss 22.186
09/28 02:24:55 AM | Valid: [ 13/180] Step 100/312 Loss 8.880 Prec@(1,3) (64.8%, 96.3%), ce_loss 1.360, lat_loss 22.186
09/28 02:25:00 AM | Valid: [ 13/180] Step 150/312 Loss 8.791 Prec@(1,3) (64.8%, 96.4%), ce_loss 1.358, lat_loss 22.186
09/28 02:25:04 AM | Valid: [ 13/180] Step 200/312 Loss 8.795 Prec@(1,3) (64.6%, 96.5%), ce_loss 1.357, lat_loss 22.186
09/28 02:25:09 AM | Valid: [ 13/180] Step 250/312 Loss 8.831 Prec@(1,3) (64.6%, 96.6%), ce_loss 1.357, lat_loss 22.186
09/28 02:25:13 AM | Valid: [ 13/180] Step 300/312 Loss 8.790 Prec@(1,3) (64.7%, 96.6%), ce_loss 1.355, lat_loss 22.186
09/28 02:25:15 AM | Valid: [ 13/180] Step 312/312 Loss 8.781 Prec@(1,3) (64.6%, 96.6%), ce_loss 1.355, lat_loss 22.186
09/28 02:25:15 AM | val: [ 13/180] Final Prec@1 64.5900% Time 29.20
09/28 02:25:15 AM | Start to train weights for epoch 13
09/28 02:25:40 AM | Train: [ 14/180] Step 050/1249 Loss 8.826 Prec@(1,3) (66.7%, 96.9%), ce_loss 1.354, lat_loss 22.185
09/28 02:26:03 AM | Train: [ 14/180] Step 100/1249 Loss 8.633 Prec@(1,3) (67.2%, 96.9%), ce_loss 1.353, lat_loss 22.185
09/28 02:26:26 AM | Train: [ 14/180] Step 150/1249 Loss 8.583 Prec@(1,3) (67.3%, 96.8%), ce_loss 1.352, lat_loss 22.185
09/28 02:26:48 AM | Train: [ 14/180] Step 200/1249 Loss 8.744 Prec@(1,3) (67.0%, 96.5%), ce_loss 1.351, lat_loss 22.185
09/28 02:27:12 AM | Train: [ 14/180] Step 250/1249 Loss 8.857 Prec@(1,3) (66.2%, 96.5%), ce_loss 1.350, lat_loss 22.185
09/28 02:27:34 AM | Train: [ 14/180] Step 300/1249 Loss 8.895 Prec@(1,3) (66.0%, 96.5%), ce_loss 1.349, lat_loss 22.185
09/28 02:27:57 AM | Train: [ 14/180] Step 350/1249 Loss 8.868 Prec@(1,3) (66.1%, 96.5%), ce_loss 1.348, lat_loss 22.185
09/28 02:28:20 AM | Train: [ 14/180] Step 400/1249 Loss 8.796 Prec@(1,3) (66.2%, 96.5%), ce_loss 1.347, lat_loss 22.185
09/28 02:28:43 AM | Train: [ 14/180] Step 450/1249 Loss 8.800 Prec@(1,3) (66.1%, 96.5%), ce_loss 1.346, lat_loss 22.184
09/28 02:29:06 AM | Train: [ 14/180] Step 500/1249 Loss 8.781 Prec@(1,3) (66.2%, 96.5%), ce_loss 1.345, lat_loss 22.184
09/28 02:29:29 AM | Train: [ 14/180] Step 550/1249 Loss 8.761 Prec@(1,3) (66.1%, 96.5%), ce_loss 1.344, lat_loss 22.184
09/28 02:29:52 AM | Train: [ 14/180] Step 600/1249 Loss 8.762 Prec@(1,3) (65.9%, 96.6%), ce_loss 1.343, lat_loss 22.184
09/28 02:30:14 AM | Train: [ 14/180] Step 650/1249 Loss 8.766 Prec@(1,3) (66.0%, 96.6%), ce_loss 1.343, lat_loss 22.184
09/28 02:30:37 AM | Train: [ 14/180] Step 700/1249 Loss 8.757 Prec@(1,3) (66.0%, 96.6%), ce_loss 1.342, lat_loss 22.184
09/28 02:30:59 AM | Train: [ 14/180] Step 750/1249 Loss 8.718 Prec@(1,3) (66.1%, 96.6%), ce_loss 1.340, lat_loss 22.184
09/28 02:31:22 AM | Train: [ 14/180] Step 800/1249 Loss 8.707 Prec@(1,3) (66.1%, 96.6%), ce_loss 1.339, lat_loss 22.184
09/28 02:31:46 AM | Train: [ 14/180] Step 850/1249 Loss 8.678 Prec@(1,3) (66.3%, 96.6%), ce_loss 1.338, lat_loss 22.183
09/28 02:32:09 AM | Train: [ 14/180] Step 900/1249 Loss 8.669 Prec@(1,3) (66.4%, 96.6%), ce_loss 1.337, lat_loss 22.183
09/28 02:32:31 AM | Train: [ 14/180] Step 950/1249 Loss 8.673 Prec@(1,3) (66.4%, 96.6%), ce_loss 1.336, lat_loss 22.183
09/28 02:32:52 AM | Train: [ 14/180] Step 1000/1249 Loss 8.658 Prec@(1,3) (66.5%, 96.6%), ce_loss 1.335, lat_loss 22.183
09/28 02:33:13 AM | Train: [ 14/180] Step 1050/1249 Loss 8.651 Prec@(1,3) (66.5%, 96.6%), ce_loss 1.334, lat_loss 22.183
09/28 02:33:34 AM | Train: [ 14/180] Step 1100/1249 Loss 8.659 Prec@(1,3) (66.5%, 96.6%), ce_loss 1.334, lat_loss 22.183
09/28 02:33:55 AM | Train: [ 14/180] Step 1150/1249 Loss 8.660 Prec@(1,3) (66.5%, 96.6%), ce_loss 1.333, lat_loss 22.183
09/28 02:34:18 AM | Train: [ 14/180] Step 1200/1249 Loss 8.647 Prec@(1,3) (66.6%, 96.6%), ce_loss 1.332, lat_loss 22.183
09/28 02:34:43 AM | Train: [ 14/180] Step 1249/1249 Loss 8.662 Prec@(1,3) (66.5%, 96.6%), ce_loss 1.331, lat_loss 22.183
09/28 02:34:43 AM | _w_step_train: [ 14/180] Final Prec@1 66.4925% Time 568.12
09/28 02:34:43 AM | Start to train theta for epoch 13
09/28 02:34:55 AM | Train: [ 14/180] Step 050/312 Loss 8.361 Prec@(1,3) (67.7%, 96.8%), ce_loss 1.330, lat_loss 22.182
09/28 02:35:06 AM | Train: [ 14/180] Step 100/312 Loss 8.428 Prec@(1,3) (67.7%, 96.5%), ce_loss 1.329, lat_loss 22.182
09/28 02:35:17 AM | Train: [ 14/180] Step 150/312 Loss 8.360 Prec@(1,3) (67.7%, 96.6%), ce_loss 1.328, lat_loss 22.182
09/28 02:35:28 AM | Train: [ 14/180] Step 200/312 Loss 8.417 Prec@(1,3) (67.6%, 96.7%), ce_loss 1.327, lat_loss 22.182
09/28 02:35:40 AM | Train: [ 14/180] Step 250/312 Loss 8.375 Prec@(1,3) (67.8%, 96.8%), ce_loss 1.326, lat_loss 22.182
09/28 02:35:51 AM | Train: [ 14/180] Step 300/312 Loss 8.330 Prec@(1,3) (67.8%, 96.9%), ce_loss 1.325, lat_loss 22.182
09/28 02:35:53 AM | Train: [ 14/180] Step 312/312 Loss 8.355 Prec@(1,3) (67.5%, 96.9%), ce_loss 1.325, lat_loss 22.182
09/28 02:35:54 AM | _theta_step_train: [ 14/180] Final Prec@1 67.5500% Time 70.73
09/28 02:35:58 AM | Valid: [ 14/180] Step 050/312 Loss 8.331 Prec@(1,3) (68.8%, 97.3%), ce_loss 1.324, lat_loss 22.182
09/28 02:36:02 AM | Valid: [ 14/180] Step 100/312 Loss 8.453 Prec@(1,3) (68.4%, 97.0%), ce_loss 1.323, lat_loss 22.182
09/28 02:36:07 AM | Valid: [ 14/180] Step 150/312 Loss 8.492 Prec@(1,3) (68.9%, 97.0%), ce_loss 1.322, lat_loss 22.182
09/28 02:36:11 AM | Valid: [ 14/180] Step 200/312 Loss 8.540 Prec@(1,3) (68.5%, 97.1%), ce_loss 1.321, lat_loss 22.181
09/28 02:36:15 AM | Valid: [ 14/180] Step 250/312 Loss 8.470 Prec@(1,3) (68.3%, 97.1%), ce_loss 1.320, lat_loss 22.181
09/28 02:36:20 AM | Valid: [ 14/180] Step 300/312 Loss 8.469 Prec@(1,3) (68.4%, 97.1%), ce_loss 1.319, lat_loss 22.181
09/28 02:36:21 AM | Valid: [ 14/180] Step 312/312 Loss 8.469 Prec@(1,3) (68.3%, 97.2%), ce_loss 1.319, lat_loss 22.181
09/28 02:36:21 AM | val: [ 14/180] Final Prec@1 68.3000% Time 27.47
09/28 02:36:21 AM | Best top1 acc by now. Save model
09/28 02:36:21 AM | Start to train weights for epoch 14
09/28 02:36:47 AM | Train: [ 15/180] Step 050/1249 Loss 8.719 Prec@(1,3) (66.9%, 97.1%), ce_loss 1.318, lat_loss 22.181
09/28 02:37:11 AM | Train: [ 15/180] Step 100/1249 Loss 8.508 Prec@(1,3) (67.5%, 97.3%), ce_loss 1.317, lat_loss 22.181
09/28 02:37:35 AM | Train: [ 15/180] Step 150/1249 Loss 8.542 Prec@(1,3) (66.9%, 97.0%), ce_loss 1.316, lat_loss 22.181
09/28 02:37:59 AM | Train: [ 15/180] Step 200/1249 Loss 8.580 Prec@(1,3) (66.8%, 96.8%), ce_loss 1.315, lat_loss 22.181
09/28 02:38:21 AM | Train: [ 15/180] Step 250/1249 Loss 8.522 Prec@(1,3) (66.8%, 96.9%), ce_loss 1.315, lat_loss 22.181
09/28 02:38:45 AM | Train: [ 15/180] Step 300/1249 Loss 8.543 Prec@(1,3) (66.9%, 96.9%), ce_loss 1.314, lat_loss 22.181
09/28 02:39:08 AM | Train: [ 15/180] Step 350/1249 Loss 8.584 Prec@(1,3) (66.7%, 96.8%), ce_loss 1.313, lat_loss 22.180
09/28 02:39:32 AM | Train: [ 15/180] Step 400/1249 Loss 8.557 Prec@(1,3) (67.0%, 96.9%), ce_loss 1.312, lat_loss 22.180
09/28 02:39:56 AM | Train: [ 15/180] Step 450/1249 Loss 8.586 Prec@(1,3) (66.9%, 96.9%), ce_loss 1.311, lat_loss 22.180
09/28 02:40:21 AM | Train: [ 15/180] Step 500/1249 Loss 8.567 Prec@(1,3) (67.0%, 96.9%), ce_loss 1.310, lat_loss 22.180
09/28 02:40:45 AM | Train: [ 15/180] Step 550/1249 Loss 8.554 Prec@(1,3) (67.0%, 96.8%), ce_loss 1.309, lat_loss 22.180
09/28 02:41:10 AM | Train: [ 15/180] Step 600/1249 Loss 8.584 Prec@(1,3) (66.9%, 96.8%), ce_loss 1.309, lat_loss 22.180
09/28 02:41:35 AM | Train: [ 15/180] Step 650/1249 Loss 8.614 Prec@(1,3) (66.8%, 96.7%), ce_loss 1.308, lat_loss 22.180
09/28 02:42:00 AM | Train: [ 15/180] Step 700/1249 Loss 8.617 Prec@(1,3) (66.9%, 96.7%), ce_loss 1.307, lat_loss 22.180
09/28 02:42:25 AM | Train: [ 15/180] Step 750/1249 Loss 8.617 Prec@(1,3) (66.9%, 96.7%), ce_loss 1.306, lat_loss 22.180
09/28 02:42:50 AM | Train: [ 15/180] Step 800/1249 Loss 8.618 Prec@(1,3) (66.9%, 96.7%), ce_loss 1.306, lat_loss 22.179
09/28 02:43:15 AM | Train: [ 15/180] Step 850/1249 Loss 8.625 Prec@(1,3) (66.9%, 96.7%), ce_loss 1.305, lat_loss 22.179
09/28 02:43:40 AM | Train: [ 15/180] Step 900/1249 Loss 8.602 Prec@(1,3) (66.9%, 96.7%), ce_loss 1.304, lat_loss 22.179
09/28 02:44:04 AM | Train: [ 15/180] Step 950/1249 Loss 8.599 Prec@(1,3) (66.9%, 96.7%), ce_loss 1.303, lat_loss 22.179
09/28 02:44:30 AM | Train: [ 15/180] Step 1000/1249 Loss 8.617 Prec@(1,3) (66.9%, 96.7%), ce_loss 1.302, lat_loss 22.179
09/28 02:44:55 AM | Train: [ 15/180] Step 1050/1249 Loss 8.620 Prec@(1,3) (66.8%, 96.7%), ce_loss 1.302, lat_loss 22.179
09/28 02:45:20 AM | Train: [ 15/180] Step 1100/1249 Loss 8.625 Prec@(1,3) (66.8%, 96.7%), ce_loss 1.301, lat_loss 22.179
09/28 02:45:44 AM | Train: [ 15/180] Step 1150/1249 Loss 8.616 Prec@(1,3) (66.8%, 96.7%), ce_loss 1.300, lat_loss 22.179
09/28 02:46:07 AM | Train: [ 15/180] Step 1200/1249 Loss 8.607 Prec@(1,3) (66.8%, 96.7%), ce_loss 1.299, lat_loss 22.179
09/28 02:46:30 AM | Train: [ 15/180] Step 1249/1249 Loss 8.604 Prec@(1,3) (66.9%, 96.7%), ce_loss 1.299, lat_loss 22.179
09/28 02:46:30 AM | _w_step_train: [ 15/180] Final Prec@1 66.8775% Time 608.54
09/28 02:46:30 AM | Start to train theta for epoch 14
09/28 02:46:52 AM | Train: [ 15/180] Step 050/312 Loss 8.289 Prec@(1,3) (67.3%, 96.3%), ce_loss 1.298, lat_loss 22.179
09/28 02:47:12 AM | Train: [ 15/180] Step 100/312 Loss 8.451 Prec@(1,3) (66.8%, 96.4%), ce_loss 1.297, lat_loss 22.178
09/28 02:47:34 AM | Train: [ 15/180] Step 150/312 Loss 8.540 Prec@(1,3) (66.9%, 96.2%), ce_loss 1.296, lat_loss 22.178
09/28 02:47:54 AM | Train: [ 15/180] Step 200/312 Loss 8.537 Prec@(1,3) (67.0%, 96.3%), ce_loss 1.295, lat_loss 22.178
09/28 02:48:15 AM | Train: [ 15/180] Step 250/312 Loss 8.442 Prec@(1,3) (67.5%, 96.3%), ce_loss 1.295, lat_loss 22.178
09/28 02:48:33 AM | Train: [ 15/180] Step 300/312 Loss 8.372 Prec@(1,3) (68.0%, 96.4%), ce_loss 1.294, lat_loss 22.178
09/28 02:48:37 AM | Train: [ 15/180] Step 312/312 Loss 8.348 Prec@(1,3) (68.0%, 96.5%), ce_loss 1.293, lat_loss 22.178
09/28 02:48:38 AM | _theta_step_train: [ 15/180] Final Prec@1 67.9900% Time 127.72
09/28 02:48:43 AM | Valid: [ 15/180] Step 050/312 Loss 7.969 Prec@(1,3) (68.0%, 97.7%), ce_loss 1.292, lat_loss 22.178
09/28 02:48:48 AM | Valid: [ 15/180] Step 100/312 Loss 7.957 Prec@(1,3) (68.8%, 97.4%), ce_loss 1.292, lat_loss 22.178
09/28 02:48:52 AM | Valid: [ 15/180] Step 150/312 Loss 8.076 Prec@(1,3) (68.5%, 97.1%), ce_loss 1.291, lat_loss 22.178
09/28 02:48:57 AM | Valid: [ 15/180] Step 200/312 Loss 8.093 Prec@(1,3) (68.1%, 97.2%), ce_loss 1.290, lat_loss 22.178
09/28 02:49:01 AM | Valid: [ 15/180] Step 250/312 Loss 8.079 Prec@(1,3) (68.2%, 97.2%), ce_loss 1.289, lat_loss 22.177
09/28 02:49:06 AM | Valid: [ 15/180] Step 300/312 Loss 8.063 Prec@(1,3) (68.3%, 97.2%), ce_loss 1.288, lat_loss 22.177
09/28 02:49:07 AM | Valid: [ 15/180] Step 312/312 Loss 8.058 Prec@(1,3) (68.3%, 97.2%), ce_loss 1.288, lat_loss 22.177
09/28 02:49:07 AM | val: [ 15/180] Final Prec@1 68.3400% Time 29.42
09/28 02:49:07 AM | Best top1 acc by now. Save model
09/28 02:49:07 AM | Start to train weights for epoch 15
09/28 02:49:24 AM | Train: [ 16/180] Step 050/1249 Loss 7.921 Prec@(1,3) (67.9%, 97.1%), ce_loss 1.287, lat_loss 22.177
09/28 02:49:39 AM | Train: [ 16/180] Step 100/1249 Loss 8.074 Prec@(1,3) (68.0%, 97.2%), ce_loss 1.286, lat_loss 22.177
09/28 02:49:53 AM | Train: [ 16/180] Step 150/1249 Loss 8.002 Prec@(1,3) (68.5%, 97.3%), ce_loss 1.285, lat_loss 22.177
09/28 02:50:07 AM | Train: [ 16/180] Step 200/1249 Loss 7.968 Prec@(1,3) (68.5%, 97.2%), ce_loss 1.284, lat_loss 22.177
09/28 02:50:22 AM | Train: [ 16/180] Step 250/1249 Loss 8.103 Prec@(1,3) (68.3%, 97.1%), ce_loss 1.284, lat_loss 22.177
09/28 02:50:37 AM | Train: [ 16/180] Step 300/1249 Loss 8.087 Prec@(1,3) (68.4%, 97.1%), ce_loss 1.283, lat_loss 22.177
09/28 02:50:51 AM | Train: [ 16/180] Step 350/1249 Loss 8.048 Prec@(1,3) (68.8%, 97.1%), ce_loss 1.282, lat_loss 22.177
09/28 02:51:05 AM | Train: [ 16/180] Step 400/1249 Loss 8.084 Prec@(1,3) (68.4%, 97.0%), ce_loss 1.281, lat_loss 22.176
09/28 02:51:20 AM | Train: [ 16/180] Step 450/1249 Loss 8.143 Prec@(1,3) (68.2%, 97.0%), ce_loss 1.280, lat_loss 22.176
09/28 02:51:34 AM | Train: [ 16/180] Step 500/1249 Loss 8.170 Prec@(1,3) (68.1%, 97.0%), ce_loss 1.280, lat_loss 22.176
09/28 02:51:49 AM | Train: [ 16/180] Step 550/1249 Loss 8.129 Prec@(1,3) (68.2%, 97.0%), ce_loss 1.279, lat_loss 22.176
09/28 02:52:03 AM | Train: [ 16/180] Step 600/1249 Loss 8.151 Prec@(1,3) (68.1%, 97.0%), ce_loss 1.278, lat_loss 22.176
09/28 02:52:18 AM | Train: [ 16/180] Step 650/1249 Loss 8.179 Prec@(1,3) (68.1%, 96.9%), ce_loss 1.277, lat_loss 22.176
09/28 02:52:33 AM | Train: [ 16/180] Step 700/1249 Loss 8.198 Prec@(1,3) (68.1%, 96.9%), ce_loss 1.277, lat_loss 22.176
09/28 02:52:47 AM | Train: [ 16/180] Step 750/1249 Loss 8.188 Prec@(1,3) (68.0%, 96.9%), ce_loss 1.276, lat_loss 22.176
09/28 02:53:03 AM | Train: [ 16/180] Step 800/1249 Loss 8.170 Prec@(1,3) (68.2%, 97.0%), ce_loss 1.275, lat_loss 22.175
09/28 02:53:25 AM | Train: [ 16/180] Step 850/1249 Loss 8.174 Prec@(1,3) (68.3%, 96.9%), ce_loss 1.274, lat_loss 22.175
09/28 02:53:46 AM | Train: [ 16/180] Step 900/1249 Loss 8.161 Prec@(1,3) (68.3%, 97.0%), ce_loss 1.273, lat_loss 22.175
09/28 02:54:09 AM | Train: [ 16/180] Step 950/1249 Loss 8.141 Prec@(1,3) (68.4%, 97.0%), ce_loss 1.273, lat_loss 22.175
09/28 02:54:32 AM | Train: [ 16/180] Step 1000/1249 Loss 8.132 Prec@(1,3) (68.5%, 97.0%), ce_loss 1.272, lat_loss 22.175
09/28 02:54:56 AM | Train: [ 16/180] Step 1050/1249 Loss 8.138 Prec@(1,3) (68.5%, 96.9%), ce_loss 1.271, lat_loss 22.175
09/28 02:55:19 AM | Train: [ 16/180] Step 1100/1249 Loss 8.119 Prec@(1,3) (68.7%, 97.0%), ce_loss 1.270, lat_loss 22.175
09/28 02:55:42 AM | Train: [ 16/180] Step 1150/1249 Loss 8.112 Prec@(1,3) (68.7%, 97.0%), ce_loss 1.269, lat_loss 22.175
09/28 02:56:05 AM | Train: [ 16/180] Step 1200/1249 Loss 8.100 Prec@(1,3) (68.8%, 97.0%), ce_loss 1.269, lat_loss 22.175
09/28 02:56:30 AM | Train: [ 16/180] Step 1249/1249 Loss 8.102 Prec@(1,3) (68.8%, 97.0%), ce_loss 1.268, lat_loss 22.175
09/28 02:56:30 AM | _w_step_train: [ 16/180] Final Prec@1 68.8225% Time 442.71
09/28 02:56:30 AM | Start to train theta for epoch 15
09/28 02:56:52 AM | Train: [ 16/180] Step 050/312 Loss 7.772 Prec@(1,3) (71.0%, 96.4%), ce_loss 1.267, lat_loss 22.174
09/28 02:57:12 AM | Train: [ 16/180] Step 100/312 Loss 7.727 Prec@(1,3) (70.8%, 96.9%), ce_loss 1.266, lat_loss 22.174
09/28 02:57:32 AM | Train: [ 16/180] Step 150/312 Loss 7.823 Prec@(1,3) (70.2%, 96.9%), ce_loss 1.265, lat_loss 22.174
09/28 02:57:53 AM | Train: [ 16/180] Step 200/312 Loss 7.851 Prec@(1,3) (70.1%, 97.0%), ce_loss 1.265, lat_loss 22.174
09/28 02:58:12 AM | Train: [ 16/180] Step 250/312 Loss 7.791 Prec@(1,3) (70.4%, 97.1%), ce_loss 1.264, lat_loss 22.174
09/28 02:58:32 AM | Train: [ 16/180] Step 300/312 Loss 7.832 Prec@(1,3) (70.1%, 97.1%), ce_loss 1.263, lat_loss 22.174
09/28 02:58:37 AM | Train: [ 16/180] Step 312/312 Loss 7.851 Prec@(1,3) (70.1%, 97.1%), ce_loss 1.263, lat_loss 22.174
09/28 02:58:37 AM | _theta_step_train: [ 16/180] Final Prec@1 70.1000% Time 127.28
09/28 02:58:42 AM | Valid: [ 16/180] Step 050/312 Loss 8.118 Prec@(1,3) (70.3%, 97.5%), ce_loss 1.262, lat_loss 22.174
09/28 02:58:47 AM | Valid: [ 16/180] Step 100/312 Loss 8.284 Prec@(1,3) (69.6%, 97.2%), ce_loss 1.261, lat_loss 22.173
09/28 02:58:52 AM | Valid: [ 16/180] Step 150/312 Loss 8.089 Prec@(1,3) (69.6%, 97.2%), ce_loss 1.260, lat_loss 22.173
09/28 02:58:56 AM | Valid: [ 16/180] Step 200/312 Loss 7.967 Prec@(1,3) (69.9%, 97.3%), ce_loss 1.260, lat_loss 22.173
09/28 02:59:01 AM | Valid: [ 16/180] Step 250/312 Loss 8.076 Prec@(1,3) (69.4%, 97.1%), ce_loss 1.259, lat_loss 22.173
09/28 02:59:05 AM | Valid: [ 16/180] Step 300/312 Loss 8.036 Prec@(1,3) (69.4%, 97.1%), ce_loss 1.258, lat_loss 22.173
09/28 02:59:06 AM | Valid: [ 16/180] Step 312/312 Loss 8.027 Prec@(1,3) (69.6%, 97.2%), ce_loss 1.258, lat_loss 22.173
09/28 02:59:06 AM | val: [ 16/180] Final Prec@1 69.5600% Time 29.26
09/28 02:59:07 AM | Best top1 acc by now. Save model
09/28 02:59:07 AM | Start to train weights for epoch 16
09/28 02:59:31 AM | Train: [ 17/180] Step 050/1249 Loss 7.783 Prec@(1,3) (71.1%, 97.8%), ce_loss 1.257, lat_loss 22.173
09/28 02:59:55 AM | Train: [ 17/180] Step 100/1249 Loss 7.919 Prec@(1,3) (70.3%, 97.5%), ce_loss 1.257, lat_loss 22.172
09/28 03:00:20 AM | Train: [ 17/180] Step 150/1249 Loss 7.882 Prec@(1,3) (69.8%, 97.5%), ce_loss 1.256, lat_loss 22.172
09/28 03:00:45 AM | Train: [ 17/180] Step 200/1249 Loss 7.879 Prec@(1,3) (70.0%, 97.6%), ce_loss 1.255, lat_loss 22.172
09/28 03:01:10 AM | Train: [ 17/180] Step 250/1249 Loss 7.907 Prec@(1,3) (70.0%, 97.5%), ce_loss 1.254, lat_loss 22.172
09/28 03:01:35 AM | Train: [ 17/180] Step 300/1249 Loss 7.857 Prec@(1,3) (70.3%, 97.5%), ce_loss 1.253, lat_loss 22.172
09/28 03:02:00 AM | Train: [ 17/180] Step 350/1249 Loss 7.838 Prec@(1,3) (70.6%, 97.5%), ce_loss 1.253, lat_loss 22.172
09/28 03:02:25 AM | Train: [ 17/180] Step 400/1249 Loss 7.874 Prec@(1,3) (70.5%, 97.4%), ce_loss 1.252, lat_loss 22.172
09/28 03:02:51 AM | Train: [ 17/180] Step 450/1249 Loss 7.862 Prec@(1,3) (70.4%, 97.4%), ce_loss 1.251, lat_loss 22.171
09/28 03:03:16 AM | Train: [ 17/180] Step 500/1249 Loss 7.834 Prec@(1,3) (70.4%, 97.4%), ce_loss 1.250, lat_loss 22.171
09/28 03:03:41 AM | Train: [ 17/180] Step 550/1249 Loss 7.828 Prec@(1,3) (70.3%, 97.4%), ce_loss 1.250, lat_loss 22.171
09/28 03:04:06 AM | Train: [ 17/180] Step 600/1249 Loss 7.827 Prec@(1,3) (70.3%, 97.4%), ce_loss 1.249, lat_loss 22.171
09/28 03:04:31 AM | Train: [ 17/180] Step 650/1249 Loss 7.823 Prec@(1,3) (70.2%, 97.4%), ce_loss 1.248, lat_loss 22.171
09/28 03:04:55 AM | Train: [ 17/180] Step 700/1249 Loss 7.817 Prec@(1,3) (70.2%, 97.4%), ce_loss 1.247, lat_loss 22.171
09/28 03:05:20 AM | Train: [ 17/180] Step 750/1249 Loss 7.822 Prec@(1,3) (70.2%, 97.4%), ce_loss 1.247, lat_loss 22.170
09/28 03:05:45 AM | Train: [ 17/180] Step 800/1249 Loss 7.811 Prec@(1,3) (70.2%, 97.4%), ce_loss 1.246, lat_loss 22.170
09/28 03:06:10 AM | Train: [ 17/180] Step 850/1249 Loss 7.818 Prec@(1,3) (70.1%, 97.4%), ce_loss 1.245, lat_loss 22.170
09/28 03:06:35 AM | Train: [ 17/180] Step 900/1249 Loss 7.812 Prec@(1,3) (70.1%, 97.4%), ce_loss 1.244, lat_loss 22.170
09/28 03:07:00 AM | Train: [ 17/180] Step 950/1249 Loss 7.768 Prec@(1,3) (70.2%, 97.4%), ce_loss 1.243, lat_loss 22.170
09/28 03:07:26 AM | Train: [ 17/180] Step 1000/1249 Loss 7.769 Prec@(1,3) (70.3%, 97.3%), ce_loss 1.243, lat_loss 22.170
09/28 03:07:51 AM | Train: [ 17/180] Step 1050/1249 Loss 7.769 Prec@(1,3) (70.3%, 97.3%), ce_loss 1.242, lat_loss 22.170
09/28 03:08:16 AM | Train: [ 17/180] Step 1100/1249 Loss 7.742 Prec@(1,3) (70.3%, 97.3%), ce_loss 1.241, lat_loss 22.169
09/28 03:08:41 AM | Train: [ 17/180] Step 1150/1249 Loss 7.742 Prec@(1,3) (70.4%, 97.3%), ce_loss 1.240, lat_loss 22.169
09/28 03:09:06 AM | Train: [ 17/180] Step 1200/1249 Loss 7.747 Prec@(1,3) (70.4%, 97.3%), ce_loss 1.240, lat_loss 22.169
09/28 03:09:30 AM | Train: [ 17/180] Step 1249/1249 Loss 7.739 Prec@(1,3) (70.4%, 97.3%), ce_loss 1.239, lat_loss 22.169
09/28 03:09:30 AM | _w_step_train: [ 17/180] Final Prec@1 70.4275% Time 623.17
09/28 03:09:30 AM | Start to train theta for epoch 16
09/28 03:09:50 AM | Train: [ 17/180] Step 050/312 Loss 7.912 Prec@(1,3) (69.9%, 96.8%), ce_loss 1.238, lat_loss 22.169
09/28 03:10:11 AM | Train: [ 17/180] Step 100/312 Loss 7.731 Prec@(1,3) (69.9%, 97.2%), ce_loss 1.237, lat_loss 22.169
09/28 03:10:32 AM | Train: [ 17/180] Step 150/312 Loss 7.562 Prec@(1,3) (70.5%, 97.4%), ce_loss 1.237, lat_loss 22.169
09/28 03:10:51 AM | Train: [ 17/180] Step 200/312 Loss 7.495 Prec@(1,3) (70.7%, 97.6%), ce_loss 1.236, lat_loss 22.168
09/28 03:11:10 AM | Train: [ 17/180] Step 250/312 Loss 7.492 Prec@(1,3) (70.9%, 97.5%), ce_loss 1.235, lat_loss 22.168
09/28 03:11:29 AM | Train: [ 17/180] Step 300/312 Loss 7.573 Prec@(1,3) (70.9%, 97.4%), ce_loss 1.234, lat_loss 22.168
09/28 03:11:34 AM | Train: [ 17/180] Step 312/312 Loss 7.568 Prec@(1,3) (70.9%, 97.4%), ce_loss 1.234, lat_loss 22.168
09/28 03:11:34 AM | _theta_step_train: [ 17/180] Final Prec@1 70.9100% Time 123.38
09/28 03:11:39 AM | Valid: [ 17/180] Step 050/312 Loss 8.049 Prec@(1,3) (71.0%, 97.7%), ce_loss 1.234, lat_loss 22.168
09/28 03:11:44 AM | Valid: [ 17/180] Step 100/312 Loss 7.994 Prec@(1,3) (70.8%, 97.4%), ce_loss 1.233, lat_loss 22.168
09/28 03:11:48 AM | Valid: [ 17/180] Step 150/312 Loss 8.146 Prec@(1,3) (70.4%, 97.1%), ce_loss 1.232, lat_loss 22.168
09/28 03:11:53 AM | Valid: [ 17/180] Step 200/312 Loss 8.283 Prec@(1,3) (70.3%, 97.1%), ce_loss 1.232, lat_loss 22.168
09/28 03:11:57 AM | Valid: [ 17/180] Step 250/312 Loss 8.187 Prec@(1,3) (70.0%, 97.1%), ce_loss 1.231, lat_loss 22.167
09/28 03:12:02 AM | Valid: [ 17/180] Step 300/312 Loss 8.153 Prec@(1,3) (70.2%, 97.2%), ce_loss 1.231, lat_loss 22.167
09/28 03:12:03 AM | Valid: [ 17/180] Step 312/312 Loss 8.137 Prec@(1,3) (70.3%, 97.2%), ce_loss 1.230, lat_loss 22.167
09/28 03:12:03 AM | val: [ 17/180] Final Prec@1 70.2900% Time 29.56
09/28 03:12:03 AM | Best top1 acc by now. Save model
09/28 03:12:03 AM | Start to train weights for epoch 17
09/28 03:12:29 AM | Train: [ 18/180] Step 050/1249 Loss 7.499 Prec@(1,3) (71.3%, 98.0%), ce_loss 1.230, lat_loss 22.167
09/28 03:12:52 AM | Train: [ 18/180] Step 100/1249 Loss 7.776 Prec@(1,3) (70.2%, 97.5%), ce_loss 1.229, lat_loss 22.167
09/28 03:13:16 AM | Train: [ 18/180] Step 150/1249 Loss 8.015 Prec@(1,3) (69.6%, 97.2%), ce_loss 1.228, lat_loss 22.167
09/28 03:13:41 AM | Train: [ 18/180] Step 200/1249 Loss 8.102 Prec@(1,3) (69.7%, 97.1%), ce_loss 1.228, lat_loss 22.167
09/28 03:14:06 AM | Train: [ 18/180] Step 250/1249 Loss 7.970 Prec@(1,3) (70.2%, 97.1%), ce_loss 1.227, lat_loss 22.167
09/28 03:14:30 AM | Train: [ 18/180] Step 300/1249 Loss 7.962 Prec@(1,3) (70.2%, 97.2%), ce_loss 1.227, lat_loss 22.166
09/28 03:14:55 AM | Train: [ 18/180] Step 350/1249 Loss 7.955 Prec@(1,3) (70.2%, 97.2%), ce_loss 1.226, lat_loss 22.166
09/28 03:15:20 AM | Train: [ 18/180] Step 400/1249 Loss 7.917 Prec@(1,3) (70.4%, 97.2%), ce_loss 1.225, lat_loss 22.166
09/28 03:15:45 AM | Train: [ 18/180] Step 450/1249 Loss 7.873 Prec@(1,3) (70.4%, 97.3%), ce_loss 1.224, lat_loss 22.166
09/28 03:16:10 AM | Train: [ 18/180] Step 500/1249 Loss 7.869 Prec@(1,3) (70.4%, 97.3%), ce_loss 1.224, lat_loss 22.166
09/28 03:16:34 AM | Train: [ 18/180] Step 550/1249 Loss 7.860 Prec@(1,3) (70.5%, 97.3%), ce_loss 1.223, lat_loss 22.166
09/28 03:16:57 AM | Train: [ 18/180] Step 600/1249 Loss 7.871 Prec@(1,3) (70.3%, 97.2%), ce_loss 1.223, lat_loss 22.166
09/28 03:17:22 AM | Train: [ 18/180] Step 650/1249 Loss 7.838 Prec@(1,3) (70.4%, 97.3%), ce_loss 1.222, lat_loss 22.165
09/28 03:17:45 AM | Train: [ 18/180] Step 700/1249 Loss 7.865 Prec@(1,3) (70.3%, 97.2%), ce_loss 1.221, lat_loss 22.165
09/28 03:18:10 AM | Train: [ 18/180] Step 750/1249 Loss 7.838 Prec@(1,3) (70.3%, 97.2%), ce_loss 1.221, lat_loss 22.165
09/28 03:18:33 AM | Train: [ 18/180] Step 800/1249 Loss 7.811 Prec@(1,3) (70.5%, 97.2%), ce_loss 1.220, lat_loss 22.165
09/28 03:18:57 AM | Train: [ 18/180] Step 850/1249 Loss 7.777 Prec@(1,3) (70.6%, 97.2%), ce_loss 1.219, lat_loss 22.165
09/28 03:19:20 AM | Train: [ 18/180] Step 900/1249 Loss 7.755 Prec@(1,3) (70.7%, 97.2%), ce_loss 1.218, lat_loss 22.165
09/28 03:19:44 AM | Train: [ 18/180] Step 950/1249 Loss 7.756 Prec@(1,3) (70.6%, 97.3%), ce_loss 1.218, lat_loss 22.165
09/28 03:20:08 AM | Train: [ 18/180] Step 1000/1249 Loss 7.756 Prec@(1,3) (70.6%, 97.3%), ce_loss 1.217, lat_loss 22.165
09/28 03:20:32 AM | Train: [ 18/180] Step 1050/1249 Loss 7.763 Prec@(1,3) (70.6%, 97.3%), ce_loss 1.216, lat_loss 22.164
09/28 03:20:55 AM | Train: [ 18/180] Step 1100/1249 Loss 7.752 Prec@(1,3) (70.6%, 97.3%), ce_loss 1.216, lat_loss 22.164
09/28 03:21:18 AM | Train: [ 18/180] Step 1150/1249 Loss 7.750 Prec@(1,3) (70.6%, 97.3%), ce_loss 1.215, lat_loss 22.164
09/28 03:21:43 AM | Train: [ 18/180] Step 1200/1249 Loss 7.735 Prec@(1,3) (70.6%, 97.3%), ce_loss 1.214, lat_loss 22.164
09/28 03:22:08 AM | Train: [ 18/180] Step 1249/1249 Loss 7.724 Prec@(1,3) (70.8%, 97.3%), ce_loss 1.214, lat_loss 22.164
09/28 03:22:08 AM | _w_step_train: [ 18/180] Final Prec@1 70.7575% Time 604.49
09/28 03:22:08 AM | Start to train theta for epoch 17
09/28 03:22:30 AM | Train: [ 18/180] Step 050/312 Loss 7.659 Prec@(1,3) (71.4%, 97.2%), ce_loss 1.213, lat_loss 22.164
09/28 03:22:51 AM | Train: [ 18/180] Step 100/312 Loss 7.449 Prec@(1,3) (71.9%, 97.2%), ce_loss 1.212, lat_loss 22.164
09/28 03:23:11 AM | Train: [ 18/180] Step 150/312 Loss 7.636 Prec@(1,3) (71.4%, 97.2%), ce_loss 1.212, lat_loss 22.163
09/28 03:23:31 AM | Train: [ 18/180] Step 200/312 Loss 7.724 Prec@(1,3) (71.1%, 97.3%), ce_loss 1.211, lat_loss 22.163
09/28 03:23:52 AM | Train: [ 18/180] Step 250/312 Loss 7.756 Prec@(1,3) (71.1%, 97.3%), ce_loss 1.211, lat_loss 22.163
09/28 03:24:12 AM | Train: [ 18/180] Step 300/312 Loss 7.780 Prec@(1,3) (71.1%, 97.3%), ce_loss 1.210, lat_loss 22.163
09/28 03:24:17 AM | Train: [ 18/180] Step 312/312 Loss 7.786 Prec@(1,3) (71.0%, 97.2%), ce_loss 1.210, lat_loss 22.163
09/28 03:24:17 AM | _theta_step_train: [ 18/180] Final Prec@1 71.0300% Time 129.11
09/28 03:24:23 AM | Valid: [ 18/180] Step 050/312 Loss 7.984 Prec@(1,3) (70.7%, 97.4%), ce_loss 1.209, lat_loss 22.163
09/28 03:24:27 AM | Valid: [ 18/180] Step 100/312 Loss 8.037 Prec@(1,3) (70.5%, 97.4%), ce_loss 1.209, lat_loss 22.163
09/28 03:24:32 AM | Valid: [ 18/180] Step 150/312 Loss 8.026 Prec@(1,3) (70.2%, 97.0%), ce_loss 1.208, lat_loss 22.162
09/28 03:24:36 AM | Valid: [ 18/180] Step 200/312 Loss 7.882 Prec@(1,3) (70.5%, 97.2%), ce_loss 1.208, lat_loss 22.162
09/28 03:24:41 AM | Valid: [ 18/180] Step 250/312 Loss 8.232 Prec@(1,3) (70.5%, 97.1%), ce_loss 1.207, lat_loss 22.162
09/28 03:24:45 AM | Valid: [ 18/180] Step 300/312 Loss 8.117 Prec@(1,3) (70.8%, 97.1%), ce_loss 1.207, lat_loss 22.162
09/28 03:24:46 AM | Valid: [ 18/180] Step 312/312 Loss 8.099 Prec@(1,3) (70.9%, 97.1%), ce_loss 1.207, lat_loss 22.162
09/28 03:24:46 AM | val: [ 18/180] Final Prec@1 70.8500% Time 28.98
09/28 03:24:46 AM | Best top1 acc by now. Save model
09/28 03:24:47 AM | Start to train weights for epoch 18
09/28 03:25:12 AM | Train: [ 19/180] Step 050/1249 Loss 7.086 Prec@(1,3) (72.4%, 97.5%), ce_loss 1.206, lat_loss 22.162
09/28 03:25:37 AM | Train: [ 19/180] Step 100/1249 Loss 7.414 Prec@(1,3) (71.2%, 97.4%), ce_loss 1.205, lat_loss 22.162
09/28 03:26:01 AM | Train: [ 19/180] Step 150/1249 Loss 7.351 Prec@(1,3) (71.7%, 97.5%), ce_loss 1.205, lat_loss 22.161
09/28 03:26:26 AM | Train: [ 19/180] Step 200/1249 Loss 7.533 Prec@(1,3) (71.7%, 97.4%), ce_loss 1.204, lat_loss 22.161
09/28 03:26:51 AM | Train: [ 19/180] Step 250/1249 Loss 7.549 Prec@(1,3) (71.6%, 97.3%), ce_loss 1.203, lat_loss 22.161
09/28 03:27:15 AM | Train: [ 19/180] Step 300/1249 Loss 7.584 Prec@(1,3) (71.6%, 97.2%), ce_loss 1.203, lat_loss 22.161
09/28 03:27:38 AM | Train: [ 19/180] Step 350/1249 Loss 7.625 Prec@(1,3) (71.3%, 97.2%), ce_loss 1.202, lat_loss 22.161
09/28 03:28:01 AM | Train: [ 19/180] Step 400/1249 Loss 7.597 Prec@(1,3) (71.4%, 97.3%), ce_loss 1.202, lat_loss 22.161
09/28 03:28:24 AM | Train: [ 19/180] Step 450/1249 Loss 7.580 Prec@(1,3) (71.4%, 97.4%), ce_loss 1.201, lat_loss 22.160
09/28 03:28:49 AM | Train: [ 19/180] Step 500/1249 Loss 7.686 Prec@(1,3) (71.1%, 97.4%), ce_loss 1.201, lat_loss 22.160
09/28 03:29:13 AM | Train: [ 19/180] Step 550/1249 Loss 7.716 Prec@(1,3) (71.0%, 97.3%), ce_loss 1.200, lat_loss 22.160
09/28 03:29:38 AM | Train: [ 19/180] Step 600/1249 Loss 7.694 Prec@(1,3) (70.9%, 97.3%), ce_loss 1.199, lat_loss 22.160
09/28 03:30:03 AM | Train: [ 19/180] Step 650/1249 Loss 7.661 Prec@(1,3) (71.1%, 97.3%), ce_loss 1.199, lat_loss 22.160
09/28 03:30:28 AM | Train: [ 19/180] Step 700/1249 Loss 7.656 Prec@(1,3) (71.0%, 97.4%), ce_loss 1.198, lat_loss 22.160
09/28 03:30:53 AM | Train: [ 19/180] Step 750/1249 Loss 7.632 Prec@(1,3) (71.1%, 97.4%), ce_loss 1.198, lat_loss 22.159
09/28 03:31:16 AM | Train: [ 19/180] Step 800/1249 Loss 7.627 Prec@(1,3) (71.1%, 97.4%), ce_loss 1.197, lat_loss 22.159
09/28 03:31:40 AM | Train: [ 19/180] Step 850/1249 Loss 7.604 Prec@(1,3) (71.2%, 97.4%), ce_loss 1.196, lat_loss 22.159
09/28 03:32:04 AM | Train: [ 19/180] Step 900/1249 Loss 7.608 Prec@(1,3) (71.2%, 97.4%), ce_loss 1.196, lat_loss 22.159
09/28 03:32:27 AM | Train: [ 19/180] Step 950/1249 Loss 7.579 Prec@(1,3) (71.2%, 97.4%), ce_loss 1.195, lat_loss 22.159
09/28 03:32:50 AM | Train: [ 19/180] Step 1000/1249 Loss 7.544 Prec@(1,3) (71.3%, 97.4%), ce_loss 1.194, lat_loss 22.159
09/28 03:33:15 AM | Train: [ 19/180] Step 1050/1249 Loss 7.533 Prec@(1,3) (71.4%, 97.4%), ce_loss 1.194, lat_loss 22.158
09/28 03:33:41 AM | Train: [ 19/180] Step 1100/1249 Loss 7.512 Prec@(1,3) (71.5%, 97.4%), ce_loss 1.193, lat_loss 22.158
09/28 03:34:05 AM | Train: [ 19/180] Step 1150/1249 Loss 7.514 Prec@(1,3) (71.5%, 97.4%), ce_loss 1.192, lat_loss 22.158
09/28 03:34:30 AM | Train: [ 19/180] Step 1200/1249 Loss 7.498 Prec@(1,3) (71.5%, 97.4%), ce_loss 1.192, lat_loss 22.158
09/28 03:34:54 AM | Train: [ 19/180] Step 1249/1249 Loss 7.499 Prec@(1,3) (71.5%, 97.4%), ce_loss 1.191, lat_loss 22.158
09/28 03:34:54 AM | _w_step_train: [ 19/180] Final Prec@1 71.5125% Time 607.64
09/28 03:34:54 AM | Start to train theta for epoch 18
09/28 03:35:16 AM | Train: [ 19/180] Step 050/312 Loss 7.094 Prec@(1,3) (72.7%, 98.2%), ce_loss 1.190, lat_loss 22.158
09/28 03:35:36 AM | Train: [ 19/180] Step 100/312 Loss 7.134 Prec@(1,3) (71.9%, 98.1%), ce_loss 1.190, lat_loss 22.157
09/28 03:35:57 AM | Train: [ 19/180] Step 150/312 Loss 7.200 Prec@(1,3) (71.9%, 97.8%), ce_loss 1.189, lat_loss 22.157
09/28 03:36:18 AM | Train: [ 19/180] Step 200/312 Loss 7.187 Prec@(1,3) (72.2%, 97.7%), ce_loss 1.189, lat_loss 22.157
09/28 03:36:38 AM | Train: [ 19/180] Step 250/312 Loss 7.238 Prec@(1,3) (72.0%, 97.6%), ce_loss 1.188, lat_loss 22.157
09/28 03:36:59 AM | Train: [ 19/180] Step 300/312 Loss 7.237 Prec@(1,3) (72.1%, 97.5%), ce_loss 1.187, lat_loss 22.157
09/28 03:37:04 AM | Train: [ 19/180] Step 312/312 Loss 7.253 Prec@(1,3) (72.0%, 97.5%), ce_loss 1.187, lat_loss 22.157
09/28 03:37:05 AM | _theta_step_train: [ 19/180] Final Prec@1 72.0100% Time 130.36
09/28 03:37:10 AM | Valid: [ 19/180] Step 050/312 Loss 7.042 Prec@(1,3) (71.9%, 98.8%), ce_loss 1.186, lat_loss 22.157
09/28 03:37:15 AM | Valid: [ 19/180] Step 100/312 Loss 7.233 Prec@(1,3) (71.6%, 98.4%), ce_loss 1.186, lat_loss 22.156
09/28 03:37:19 AM | Valid: [ 19/180] Step 150/312 Loss 7.248 Prec@(1,3) (72.1%, 98.0%), ce_loss 1.185, lat_loss 22.156
09/28 03:37:24 AM | Valid: [ 19/180] Step 200/312 Loss 7.562 Prec@(1,3) (71.5%, 97.8%), ce_loss 1.185, lat_loss 22.156
09/28 03:37:28 AM | Valid: [ 19/180] Step 250/312 Loss 7.548 Prec@(1,3) (71.3%, 97.9%), ce_loss 1.184, lat_loss 22.156
09/28 03:37:33 AM | Valid: [ 19/180] Step 300/312 Loss 7.425 Prec@(1,3) (71.9%, 97.9%), ce_loss 1.184, lat_loss 22.156
09/28 03:37:34 AM | Valid: [ 19/180] Step 312/312 Loss 7.402 Prec@(1,3) (72.0%, 97.9%), ce_loss 1.183, lat_loss 22.156
09/28 03:37:34 AM | val: [ 19/180] Final Prec@1 71.9800% Time 29.44
09/28 03:37:34 AM | Best top1 acc by now. Save model
09/28 03:37:34 AM | Start to train weights for epoch 19
09/28 03:38:00 AM | Train: [ 20/180] Step 050/1249 Loss 6.999 Prec@(1,3) (72.2%, 98.3%), ce_loss 1.183, lat_loss 22.155
09/28 03:38:23 AM | Train: [ 20/180] Step 100/1249 Loss 6.848 Prec@(1,3) (73.0%, 98.6%), ce_loss 1.182, lat_loss 22.155
09/28 03:38:45 AM | Train: [ 20/180] Step 150/1249 Loss 6.872 Prec@(1,3) (73.3%, 98.3%), ce_loss 1.181, lat_loss 22.155
09/28 03:39:09 AM | Train: [ 20/180] Step 200/1249 Loss 7.027 Prec@(1,3) (73.3%, 98.0%), ce_loss 1.181, lat_loss 22.155
09/28 03:39:33 AM | Train: [ 20/180] Step 250/1249 Loss 7.038 Prec@(1,3) (73.3%, 97.9%), ce_loss 1.180, lat_loss 22.155
09/28 03:39:57 AM | Train: [ 20/180] Step 300/1249 Loss 7.148 Prec@(1,3) (72.8%, 97.8%), ce_loss 1.180, lat_loss 22.155
09/28 03:40:21 AM | Train: [ 20/180] Step 350/1249 Loss 7.142 Prec@(1,3) (72.7%, 97.7%), ce_loss 1.179, lat_loss 22.154
09/28 03:40:45 AM | Train: [ 20/180] Step 400/1249 Loss 7.150 Prec@(1,3) (72.5%, 97.7%), ce_loss 1.178, lat_loss 22.154
09/28 03:41:08 AM | Train: [ 20/180] Step 450/1249 Loss 7.156 Prec@(1,3) (72.6%, 97.7%), ce_loss 1.178, lat_loss 22.154
09/28 03:41:29 AM | Train: [ 20/180] Step 500/1249 Loss 7.188 Prec@(1,3) (72.6%, 97.6%), ce_loss 1.177, lat_loss 22.154
09/28 03:41:51 AM | Train: [ 20/180] Step 550/1249 Loss 7.179 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.177, lat_loss 22.154
09/28 03:42:14 AM | Train: [ 20/180] Step 600/1249 Loss 7.132 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.176, lat_loss 22.154
09/28 03:42:36 AM | Train: [ 20/180] Step 650/1249 Loss 7.174 Prec@(1,3) (72.8%, 97.6%), ce_loss 1.175, lat_loss 22.153
09/28 03:43:00 AM | Train: [ 20/180] Step 700/1249 Loss 7.173 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.175, lat_loss 22.153
09/28 03:43:23 AM | Train: [ 20/180] Step 750/1249 Loss 7.186 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.174, lat_loss 22.153
09/28 03:43:45 AM | Train: [ 20/180] Step 800/1249 Loss 7.197 Prec@(1,3) (72.6%, 97.6%), ce_loss 1.174, lat_loss 22.153
09/28 03:44:07 AM | Train: [ 20/180] Step 850/1249 Loss 7.196 Prec@(1,3) (72.6%, 97.6%), ce_loss 1.173, lat_loss 22.153
09/28 03:44:29 AM | Train: [ 20/180] Step 900/1249 Loss 7.197 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.173, lat_loss 22.152
09/28 03:44:51 AM | Train: [ 20/180] Step 950/1249 Loss 7.226 Prec@(1,3) (72.5%, 97.6%), ce_loss 1.172, lat_loss 22.152
09/28 03:45:14 AM | Train: [ 20/180] Step 1000/1249 Loss 7.234 Prec@(1,3) (72.4%, 97.6%), ce_loss 1.171, lat_loss 22.152
09/28 03:45:38 AM | Train: [ 20/180] Step 1050/1249 Loss 7.218 Prec@(1,3) (72.5%, 97.6%), ce_loss 1.171, lat_loss 22.152
09/28 03:46:02 AM | Train: [ 20/180] Step 1100/1249 Loss 7.212 Prec@(1,3) (72.6%, 97.6%), ce_loss 1.170, lat_loss 22.152
09/28 03:46:27 AM | Train: [ 20/180] Step 1150/1249 Loss 7.193 Prec@(1,3) (72.6%, 97.7%), ce_loss 1.170, lat_loss 22.152
09/28 03:46:45 AM | Train: [ 20/180] Step 1200/1249 Loss 7.212 Prec@(1,3) (72.6%, 97.7%), ce_loss 1.169, lat_loss 22.151
09/28 03:47:01 AM | Train: [ 20/180] Step 1249/1249 Loss 7.225 Prec@(1,3) (72.6%, 97.7%), ce_loss 1.169, lat_loss 22.151
09/28 03:47:01 AM | _w_step_train: [ 20/180] Final Prec@1 72.5600% Time 566.87
09/28 03:47:01 AM | Start to train theta for epoch 19
09/28 03:47:15 AM | Train: [ 20/180] Step 050/312 Loss 6.933 Prec@(1,3) (73.0%, 98.0%), ce_loss 1.168, lat_loss 22.151
09/28 03:47:27 AM | Train: [ 20/180] Step 100/312 Loss 7.149 Prec@(1,3) (72.0%, 97.7%), ce_loss 1.167, lat_loss 22.151
09/28 03:47:39 AM | Train: [ 20/180] Step 150/312 Loss 7.263 Prec@(1,3) (71.9%, 97.7%), ce_loss 1.167, lat_loss 22.151
09/28 03:47:52 AM | Train: [ 20/180] Step 200/312 Loss 7.162 Prec@(1,3) (72.3%, 97.7%), ce_loss 1.166, lat_loss 22.151
09/28 03:48:04 AM | Train: [ 20/180] Step 250/312 Loss 7.208 Prec@(1,3) (72.4%, 97.6%), ce_loss 1.166, lat_loss 22.151
09/28 03:48:16 AM | Train: [ 20/180] Step 300/312 Loss 7.199 Prec@(1,3) (72.5%, 97.6%), ce_loss 1.165, lat_loss 22.151
09/28 03:48:19 AM | Train: [ 20/180] Step 312/312 Loss 7.150 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.165, lat_loss 22.150
09/28 03:48:19 AM | _theta_step_train: [ 20/180] Final Prec@1 72.6500% Time 77.94
09/28 03:48:25 AM | Valid: [ 20/180] Step 050/312 Loss 7.183 Prec@(1,3) (72.2%, 97.8%), ce_loss 1.164, lat_loss 22.150
09/28 03:48:29 AM | Valid: [ 20/180] Step 100/312 Loss 7.540 Prec@(1,3) (71.6%, 97.4%), ce_loss 1.164, lat_loss 22.150
09/28 03:48:34 AM | Valid: [ 20/180] Step 150/312 Loss 7.342 Prec@(1,3) (72.2%, 97.5%), ce_loss 1.163, lat_loss 22.150
09/28 03:48:38 AM | Valid: [ 20/180] Step 200/312 Loss 7.407 Prec@(1,3) (71.8%, 97.6%), ce_loss 1.163, lat_loss 22.150
09/28 03:48:43 AM | Valid: [ 20/180] Step 250/312 Loss 7.391 Prec@(1,3) (71.9%, 97.7%), ce_loss 1.162, lat_loss 22.150
09/28 03:48:48 AM | Valid: [ 20/180] Step 300/312 Loss 7.356 Prec@(1,3) (71.9%, 97.7%), ce_loss 1.162, lat_loss 22.150
09/28 03:48:49 AM | Valid: [ 20/180] Step 312/312 Loss 7.371 Prec@(1,3) (71.8%, 97.7%), ce_loss 1.162, lat_loss 22.150
09/28 03:48:49 AM | val: [ 20/180] Final Prec@1 71.8400% Time 29.70
09/28 03:48:49 AM | Start to train weights for epoch 20
09/28 03:49:13 AM | Train: [ 21/180] Step 050/1249 Loss 6.836 Prec@(1,3) (73.5%, 97.6%), ce_loss 1.161, lat_loss 22.150
09/28 03:49:35 AM | Train: [ 21/180] Step 100/1249 Loss 7.400 Prec@(1,3) (72.0%, 97.3%), ce_loss 1.161, lat_loss 22.150
09/28 03:49:57 AM | Train: [ 21/180] Step 150/1249 Loss 7.248 Prec@(1,3) (72.3%, 97.6%), ce_loss 1.160, lat_loss 22.149
09/28 03:50:19 AM | Train: [ 21/180] Step 200/1249 Loss 7.333 Prec@(1,3) (72.3%, 97.5%), ce_loss 1.160, lat_loss 22.149
09/28 03:50:41 AM | Train: [ 21/180] Step 250/1249 Loss 7.359 Prec@(1,3) (72.3%, 97.5%), ce_loss 1.159, lat_loss 22.149
09/28 03:51:02 AM | Train: [ 21/180] Step 300/1249 Loss 7.341 Prec@(1,3) (72.4%, 97.5%), ce_loss 1.159, lat_loss 22.149
09/28 03:51:24 AM | Train: [ 21/180] Step 350/1249 Loss 7.245 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.158, lat_loss 22.149
09/28 03:51:44 AM | Train: [ 21/180] Step 400/1249 Loss 7.217 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.157, lat_loss 22.149
09/28 03:52:06 AM | Train: [ 21/180] Step 450/1249 Loss 7.252 Prec@(1,3) (72.8%, 97.5%), ce_loss 1.157, lat_loss 22.149
09/28 03:52:29 AM | Train: [ 21/180] Step 500/1249 Loss 7.230 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.156, lat_loss 22.149
09/28 03:52:51 AM | Train: [ 21/180] Step 550/1249 Loss 7.222 Prec@(1,3) (73.0%, 97.6%), ce_loss 1.156, lat_loss 22.149
09/28 03:53:14 AM | Train: [ 21/180] Step 600/1249 Loss 7.234 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.155, lat_loss 22.148
09/28 03:53:35 AM | Train: [ 21/180] Step 650/1249 Loss 7.230 Prec@(1,3) (73.0%, 97.6%), ce_loss 1.155, lat_loss 22.148
09/28 03:54:00 AM | Train: [ 21/180] Step 700/1249 Loss 7.229 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.154, lat_loss 22.148
09/28 03:54:25 AM | Train: [ 21/180] Step 750/1249 Loss 7.256 Prec@(1,3) (72.8%, 97.6%), ce_loss 1.154, lat_loss 22.148
09/28 03:54:50 AM | Train: [ 21/180] Step 800/1249 Loss 7.210 Prec@(1,3) (73.0%, 97.6%), ce_loss 1.153, lat_loss 22.148
09/28 03:55:15 AM | Train: [ 21/180] Step 850/1249 Loss 7.219 Prec@(1,3) (73.0%, 97.6%), ce_loss 1.153, lat_loss 22.148
09/28 03:55:40 AM | Train: [ 21/180] Step 900/1249 Loss 7.182 Prec@(1,3) (73.2%, 97.6%), ce_loss 1.152, lat_loss 22.148
09/28 03:56:05 AM | Train: [ 21/180] Step 950/1249 Loss 7.201 Prec@(1,3) (73.2%, 97.6%), ce_loss 1.151, lat_loss 22.148
09/28 03:56:27 AM | Train: [ 21/180] Step 1000/1249 Loss 7.200 Prec@(1,3) (73.1%, 97.6%), ce_loss 1.151, lat_loss 22.147
09/28 03:56:48 AM | Train: [ 21/180] Step 1050/1249 Loss 7.194 Prec@(1,3) (73.1%, 97.6%), ce_loss 1.150, lat_loss 22.147
09/28 03:57:09 AM | Train: [ 21/180] Step 1100/1249 Loss 7.211 Prec@(1,3) (73.1%, 97.6%), ce_loss 1.150, lat_loss 22.147
09/28 03:57:31 AM | Train: [ 21/180] Step 1150/1249 Loss 7.231 Prec@(1,3) (73.0%, 97.6%), ce_loss 1.149, lat_loss 22.147
09/28 03:57:51 AM | Train: [ 21/180] Step 1200/1249 Loss 7.239 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.149, lat_loss 22.147
09/28 03:58:12 AM | Train: [ 21/180] Step 1249/1249 Loss 7.215 Prec@(1,3) (73.0%, 97.6%), ce_loss 1.148, lat_loss 22.147
09/28 03:58:12 AM | _w_step_train: [ 21/180] Final Prec@1 73.0025% Time 562.95
09/28 03:58:12 AM | Start to train theta for epoch 20
09/28 03:58:33 AM | Train: [ 21/180] Step 050/312 Loss 6.965 Prec@(1,3) (73.6%, 97.6%), ce_loss 1.148, lat_loss 22.147
09/28 03:58:53 AM | Train: [ 21/180] Step 100/312 Loss 7.063 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.147, lat_loss 22.147
09/28 03:59:13 AM | Train: [ 21/180] Step 150/312 Loss 6.934 Prec@(1,3) (73.8%, 97.5%), ce_loss 1.147, lat_loss 22.147
09/28 03:59:34 AM | Train: [ 21/180] Step 200/312 Loss 6.979 Prec@(1,3) (73.4%, 97.5%), ce_loss 1.146, lat_loss 22.146
09/28 03:59:54 AM | Train: [ 21/180] Step 250/312 Loss 7.002 Prec@(1,3) (73.5%, 97.5%), ce_loss 1.146, lat_loss 22.146
09/28 04:00:14 AM | Train: [ 21/180] Step 300/312 Loss 7.077 Prec@(1,3) (73.3%, 97.5%), ce_loss 1.145, lat_loss 22.146
09/28 04:00:19 AM | Train: [ 21/180] Step 312/312 Loss 7.098 Prec@(1,3) (73.2%, 97.5%), ce_loss 1.145, lat_loss 22.146
09/28 04:00:19 AM | _theta_step_train: [ 21/180] Final Prec@1 73.1800% Time 126.82
09/28 04:00:24 AM | Valid: [ 21/180] Step 050/312 Loss 7.007 Prec@(1,3) (73.0%, 98.0%), ce_loss 1.145, lat_loss 22.146
09/28 04:00:28 AM | Valid: [ 21/180] Step 100/312 Loss 7.260 Prec@(1,3) (72.6%, 97.6%), ce_loss 1.144, lat_loss 22.146
09/28 04:00:32 AM | Valid: [ 21/180] Step 150/312 Loss 7.357 Prec@(1,3) (72.4%, 97.2%), ce_loss 1.144, lat_loss 22.146
09/28 04:00:36 AM | Valid: [ 21/180] Step 200/312 Loss 7.377 Prec@(1,3) (72.1%, 97.4%), ce_loss 1.143, lat_loss 22.145
09/28 04:00:40 AM | Valid: [ 21/180] Step 250/312 Loss 7.343 Prec@(1,3) (72.2%, 97.5%), ce_loss 1.143, lat_loss 22.145
09/28 04:00:45 AM | Valid: [ 21/180] Step 300/312 Loss 7.284 Prec@(1,3) (72.3%, 97.5%), ce_loss 1.142, lat_loss 22.145
09/28 04:00:46 AM | Valid: [ 21/180] Step 312/312 Loss 7.289 Prec@(1,3) (72.3%, 97.5%), ce_loss 1.142, lat_loss 22.145
09/28 04:00:46 AM | val: [ 21/180] Final Prec@1 72.2600% Time 27.20
09/28 04:00:46 AM | Best top1 acc by now. Save model
09/28 04:00:46 AM | Start to train weights for epoch 21
09/28 04:01:11 AM | Train: [ 22/180] Step 050/1249 Loss 7.275 Prec@(1,3) (72.2%, 97.0%), ce_loss 1.142, lat_loss 22.145
09/28 04:01:34 AM | Train: [ 22/180] Step 100/1249 Loss 7.122 Prec@(1,3) (73.0%, 97.4%), ce_loss 1.141, lat_loss 22.145
09/28 04:01:57 AM | Train: [ 22/180] Step 150/1249 Loss 6.967 Prec@(1,3) (73.4%, 97.5%), ce_loss 1.141, lat_loss 22.145
09/28 04:02:19 AM | Train: [ 22/180] Step 200/1249 Loss 7.029 Prec@(1,3) (73.7%, 97.5%), ce_loss 1.140, lat_loss 22.144
09/28 04:02:41 AM | Train: [ 22/180] Step 250/1249 Loss 7.127 Prec@(1,3) (73.4%, 97.5%), ce_loss 1.140, lat_loss 22.144
09/28 04:03:03 AM | Train: [ 22/180] Step 300/1249 Loss 7.153 Prec@(1,3) (73.3%, 97.4%), ce_loss 1.139, lat_loss 22.144
09/28 04:03:26 AM | Train: [ 22/180] Step 350/1249 Loss 7.201 Prec@(1,3) (72.9%, 97.4%), ce_loss 1.139, lat_loss 22.144
09/28 04:03:50 AM | Train: [ 22/180] Step 400/1249 Loss 7.168 Prec@(1,3) (72.9%, 97.5%), ce_loss 1.138, lat_loss 22.144
09/28 04:04:14 AM | Train: [ 22/180] Step 450/1249 Loss 7.169 Prec@(1,3) (72.9%, 97.5%), ce_loss 1.138, lat_loss 22.144
09/28 04:04:37 AM | Train: [ 22/180] Step 500/1249 Loss 7.148 Prec@(1,3) (73.0%, 97.5%), ce_loss 1.137, lat_loss 22.143
09/28 04:05:00 AM | Train: [ 22/180] Step 550/1249 Loss 7.154 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.137, lat_loss 22.143
09/28 04:05:24 AM | Train: [ 22/180] Step 600/1249 Loss 7.201 Prec@(1,3) (72.8%, 97.5%), ce_loss 1.136, lat_loss 22.143
09/28 04:05:48 AM | Train: [ 22/180] Step 650/1249 Loss 7.250 Prec@(1,3) (72.6%, 97.5%), ce_loss 1.136, lat_loss 22.143
09/28 04:06:10 AM | Train: [ 22/180] Step 700/1249 Loss 7.244 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.135, lat_loss 22.143
09/28 04:06:32 AM | Train: [ 22/180] Step 750/1249 Loss 7.263 Prec@(1,3) (72.6%, 97.5%), ce_loss 1.135, lat_loss 22.143
09/28 04:06:54 AM | Train: [ 22/180] Step 800/1249 Loss 7.262 Prec@(1,3) (72.7%, 97.5%), ce_loss 1.135, lat_loss 22.142
09/28 04:07:15 AM | Train: [ 22/180] Step 850/1249 Loss 7.251 Prec@(1,3) (72.8%, 97.5%), ce_loss 1.134, lat_loss 22.142
09/28 04:07:38 AM | Train: [ 22/180] Step 900/1249 Loss 7.234 Prec@(1,3) (72.8%, 97.5%), ce_loss 1.134, lat_loss 22.142
09/28 04:07:59 AM | Train: [ 22/180] Step 950/1249 Loss 7.217 Prec@(1,3) (72.8%, 97.6%), ce_loss 1.133, lat_loss 22.142
09/28 04:08:24 AM | Train: [ 22/180] Step 1000/1249 Loss 7.217 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.133, lat_loss 22.142
09/28 04:08:48 AM | Train: [ 22/180] Step 1050/1249 Loss 7.217 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.132, lat_loss 22.142
09/28 04:09:13 AM | Train: [ 22/180] Step 1100/1249 Loss 7.212 Prec@(1,3) (72.7%, 97.6%), ce_loss 1.132, lat_loss 22.141
09/28 04:09:38 AM | Train: [ 22/180] Step 1150/1249 Loss 7.209 Prec@(1,3) (72.8%, 97.6%), ce_loss 1.131, lat_loss 22.141
09/28 04:10:03 AM | Train: [ 22/180] Step 1200/1249 Loss 7.182 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.131, lat_loss 22.141
09/28 04:10:27 AM | Train: [ 22/180] Step 1249/1249 Loss 7.174 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.130, lat_loss 22.141
09/28 04:10:27 AM | _w_step_train: [ 22/180] Final Prec@1 72.9075% Time 580.84
09/28 04:10:27 AM | Start to train theta for epoch 21
09/28 04:10:49 AM | Train: [ 22/180] Step 050/312 Loss 7.239 Prec@(1,3) (72.2%, 97.6%), ce_loss 1.130, lat_loss 22.141
09/28 04:11:09 AM | Train: [ 22/180] Step 100/312 Loss 7.478 Prec@(1,3) (72.1%, 97.5%), ce_loss 1.129, lat_loss 22.141
09/28 04:11:30 AM | Train: [ 22/180] Step 150/312 Loss 7.278 Prec@(1,3) (73.0%, 97.5%), ce_loss 1.129, lat_loss 22.140
09/28 04:11:51 AM | Train: [ 22/180] Step 200/312 Loss 7.284 Prec@(1,3) (72.7%, 97.5%), ce_loss 1.128, lat_loss 22.140
09/28 04:12:10 AM | Train: [ 22/180] Step 250/312 Loss 7.306 Prec@(1,3) (72.8%, 97.6%), ce_loss 1.128, lat_loss 22.140
09/28 04:12:29 AM | Train: [ 22/180] Step 300/312 Loss 7.209 Prec@(1,3) (73.1%, 97.6%), ce_loss 1.127, lat_loss 22.140
09/28 04:12:34 AM | Train: [ 22/180] Step 312/312 Loss 7.174 Prec@(1,3) (73.2%, 97.6%), ce_loss 1.127, lat_loss 22.140
09/28 04:12:34 AM | _theta_step_train: [ 22/180] Final Prec@1 73.2100% Time 126.79
09/28 04:12:39 AM | Valid: [ 22/180] Step 050/312 Loss 6.981 Prec@(1,3) (73.3%, 98.0%), ce_loss 1.127, lat_loss 22.140
09/28 04:12:43 AM | Valid: [ 22/180] Step 100/312 Loss 7.270 Prec@(1,3) (72.2%, 97.6%), ce_loss 1.126, lat_loss 22.140
09/28 04:12:48 AM | Valid: [ 22/180] Step 150/312 Loss 7.193 Prec@(1,3) (72.9%, 97.6%), ce_loss 1.126, lat_loss 22.139
09/28 04:12:53 AM | Valid: [ 22/180] Step 200/312 Loss 7.275 Prec@(1,3) (72.5%, 97.6%), ce_loss 1.126, lat_loss 22.139
09/28 04:12:57 AM | Valid: [ 22/180] Step 250/312 Loss 7.298 Prec@(1,3) (72.5%, 97.6%), ce_loss 1.125, lat_loss 22.139
09/28 04:13:02 AM | Valid: [ 22/180] Step 300/312 Loss 7.139 Prec@(1,3) (72.8%, 97.8%), ce_loss 1.125, lat_loss 22.139
09/28 04:13:03 AM | Valid: [ 22/180] Step 312/312 Loss 7.150 Prec@(1,3) (72.7%, 97.8%), ce_loss 1.125, lat_loss 22.139
09/28 04:13:03 AM | val: [ 22/180] Final Prec@1 72.7200% Time 29.43
09/28 04:13:03 AM | Best top1 acc by now. Save model
09/28 04:13:03 AM | Start to train weights for epoch 22
09/28 04:13:28 AM | Train: [ 23/180] Step 050/1249 Loss 7.013 Prec@(1,3) (73.6%, 97.5%), ce_loss 1.124, lat_loss 22.139
09/28 04:13:52 AM | Train: [ 23/180] Step 100/1249 Loss 7.016 Prec@(1,3) (74.1%, 97.3%), ce_loss 1.124, lat_loss 22.139
09/28 04:14:16 AM | Train: [ 23/180] Step 150/1249 Loss 7.046 Prec@(1,3) (73.7%, 97.5%), ce_loss 1.123, lat_loss 22.138
09/28 04:14:40 AM | Train: [ 23/180] Step 200/1249 Loss 6.913 Prec@(1,3) (74.0%, 97.7%), ce_loss 1.123, lat_loss 22.138
09/28 04:15:04 AM | Train: [ 23/180] Step 250/1249 Loss 6.891 Prec@(1,3) (74.1%, 97.7%), ce_loss 1.122, lat_loss 22.138
09/28 04:15:28 AM | Train: [ 23/180] Step 300/1249 Loss 6.890 Prec@(1,3) (74.1%, 97.7%), ce_loss 1.122, lat_loss 22.138
09/28 04:15:53 AM | Train: [ 23/180] Step 350/1249 Loss 6.873 Prec@(1,3) (74.3%, 97.7%), ce_loss 1.121, lat_loss 22.138
09/28 04:16:18 AM | Train: [ 23/180] Step 400/1249 Loss 6.848 Prec@(1,3) (74.4%, 97.7%), ce_loss 1.121, lat_loss 22.137
09/28 04:16:43 AM | Train: [ 23/180] Step 450/1249 Loss 6.807 Prec@(1,3) (74.5%, 97.8%), ce_loss 1.120, lat_loss 22.137
09/28 04:17:07 AM | Train: [ 23/180] Step 500/1249 Loss 6.827 Prec@(1,3) (74.4%, 97.7%), ce_loss 1.120, lat_loss 22.137
09/28 04:17:32 AM | Train: [ 23/180] Step 550/1249 Loss 6.839 Prec@(1,3) (74.4%, 97.7%), ce_loss 1.119, lat_loss 22.137
09/28 04:17:56 AM | Train: [ 23/180] Step 600/1249 Loss 6.858 Prec@(1,3) (74.3%, 97.7%), ce_loss 1.119, lat_loss 22.137
09/28 04:18:21 AM | Train: [ 23/180] Step 650/1249 Loss 6.869 Prec@(1,3) (74.3%, 97.7%), ce_loss 1.118, lat_loss 22.137
09/28 04:18:45 AM | Train: [ 23/180] Step 700/1249 Loss 6.903 Prec@(1,3) (74.1%, 97.7%), ce_loss 1.118, lat_loss 22.136
09/28 04:19:08 AM | Train: [ 23/180] Step 750/1249 Loss 6.881 Prec@(1,3) (74.1%, 97.7%), ce_loss 1.117, lat_loss 22.136
09/28 04:19:30 AM | Train: [ 23/180] Step 800/1249 Loss 7.088 Prec@(1,3) (73.4%, 97.5%), ce_loss 1.117, lat_loss 22.136
09/28 04:19:54 AM | Train: [ 23/180] Step 850/1249 Loss 7.333 Prec@(1,3) (72.6%, 97.3%), ce_loss 1.118, lat_loss 22.136
09/28 04:20:18 AM | Train: [ 23/180] Step 900/1249 Loss 7.454 Prec@(1,3) (72.1%, 97.2%), ce_loss 1.118, lat_loss 22.136
09/28 04:20:40 AM | Train: [ 23/180] Step 950/1249 Loss 7.531 Prec@(1,3) (71.7%, 97.1%), ce_loss 1.117, lat_loss 22.136
09/28 04:21:05 AM | Train: [ 23/180] Step 1000/1249 Loss 7.588 Prec@(1,3) (71.4%, 97.1%), ce_loss 1.117, lat_loss 22.135
09/28 04:21:29 AM | Train: [ 23/180] Step 1050/1249 Loss 7.622 Prec@(1,3) (71.3%, 97.1%), ce_loss 1.117, lat_loss 22.135
09/28 04:21:53 AM | Train: [ 23/180] Step 1100/1249 Loss 7.633 Prec@(1,3) (71.2%, 97.1%), ce_loss 1.117, lat_loss 22.135
09/28 04:22:17 AM | Train: [ 23/180] Step 1150/1249 Loss 7.664 Prec@(1,3) (71.1%, 97.1%), ce_loss 1.116, lat_loss 22.135
09/28 04:22:42 AM | Train: [ 23/180] Step 1200/1249 Loss 7.669 Prec@(1,3) (71.1%, 97.1%), ce_loss 1.116, lat_loss 22.135
09/28 04:23:06 AM | Train: [ 23/180] Step 1249/1249 Loss 7.682 Prec@(1,3) (71.0%, 97.1%), ce_loss 1.116, lat_loss 22.135
09/28 04:23:06 AM | _w_step_train: [ 23/180] Final Prec@1 71.0450% Time 603.16
09/28 04:23:06 AM | Start to train theta for epoch 22
09/28 04:23:20 AM | Train: [ 23/180] Step 050/312 Loss 8.995 Prec@(1,3) (67.5%, 95.7%), ce_loss 1.116, lat_loss 22.134
09/28 04:23:33 AM | Train: [ 23/180] Step 100/312 Loss 8.894 Prec@(1,3) (67.2%, 96.1%), ce_loss 1.116, lat_loss 22.134
09/28 04:23:54 AM | Train: [ 23/180] Step 150/312 Loss 8.821 Prec@(1,3) (66.8%, 96.3%), ce_loss 1.115, lat_loss 22.134
09/28 04:24:15 AM | Train: [ 23/180] Step 200/312 Loss 8.606 Prec@(1,3) (67.1%, 96.5%), ce_loss 1.115, lat_loss 22.134
09/28 04:24:28 AM | Train: [ 23/180] Step 250/312 Loss 8.511 Prec@(1,3) (67.4%, 96.6%), ce_loss 1.115, lat_loss 22.134
09/28 04:24:40 AM | Train: [ 23/180] Step 300/312 Loss 8.423 Prec@(1,3) (67.9%, 96.6%), ce_loss 1.115, lat_loss 22.133
09/28 04:24:42 AM | Train: [ 23/180] Step 312/312 Loss 8.387 Prec@(1,3) (68.0%, 96.6%), ce_loss 1.114, lat_loss 22.133
09/28 04:24:42 AM | _theta_step_train: [ 23/180] Final Prec@1 67.9700% Time 95.82
09/28 04:24:48 AM | Valid: [ 23/180] Step 050/312 Loss 8.603 Prec@(1,3) (66.4%, 96.8%), ce_loss 1.114, lat_loss 22.133
09/28 04:24:52 AM | Valid: [ 23/180] Step 100/312 Loss 8.919 Prec@(1,3) (66.0%, 96.2%), ce_loss 1.114, lat_loss 22.133
09/28 04:24:57 AM | Valid: [ 23/180] Step 150/312 Loss 8.877 Prec@(1,3) (66.5%, 96.2%), ce_loss 1.114, lat_loss 22.132
09/28 04:25:01 AM | Valid: [ 23/180] Step 200/312 Loss 8.864 Prec@(1,3) (66.4%, 96.2%), ce_loss 1.114, lat_loss 22.132
09/28 04:25:06 AM | Valid: [ 23/180] Step 250/312 Loss 8.946 Prec@(1,3) (66.0%, 96.2%), ce_loss 1.114, lat_loss 22.132
09/28 04:25:10 AM | Valid: [ 23/180] Step 300/312 Loss 8.951 Prec@(1,3) (65.9%, 96.2%), ce_loss 1.114, lat_loss 22.132
09/28 04:25:11 AM | Valid: [ 23/180] Step 312/312 Loss 8.955 Prec@(1,3) (65.8%, 96.2%), ce_loss 1.114, lat_loss 22.132
09/28 04:25:11 AM | val: [ 23/180] Final Prec@1 65.8000% Time 29.09
09/28 04:25:11 AM | Start to train weights for epoch 23
09/28 04:25:36 AM | Train: [ 24/180] Step 050/1249 Loss 8.269 Prec@(1,3) (68.9%, 96.8%), ce_loss 1.113, lat_loss 22.131
09/28 04:25:59 AM | Train: [ 24/180] Step 100/1249 Loss 8.291 Prec@(1,3) (67.9%, 96.8%), ce_loss 1.113, lat_loss 22.131
09/28 04:26:22 AM | Train: [ 24/180] Step 150/1249 Loss 8.038 Prec@(1,3) (68.9%, 96.8%), ce_loss 1.113, lat_loss 22.131
09/28 04:26:45 AM | Train: [ 24/180] Step 200/1249 Loss 8.120 Prec@(1,3) (68.9%, 96.8%), ce_loss 1.113, lat_loss 22.131
09/28 04:27:07 AM | Train: [ 24/180] Step 250/1249 Loss 8.046 Prec@(1,3) (69.4%, 96.9%), ce_loss 1.112, lat_loss 22.130
09/28 04:27:30 AM | Train: [ 24/180] Step 300/1249 Loss 7.980 Prec@(1,3) (69.5%, 97.1%), ce_loss 1.112, lat_loss 22.130
09/28 04:27:50 AM | Train: [ 24/180] Step 350/1249 Loss 7.919 Prec@(1,3) (69.8%, 97.2%), ce_loss 1.112, lat_loss 22.130
09/28 04:28:12 AM | Train: [ 24/180] Step 400/1249 Loss 7.829 Prec@(1,3) (70.1%, 97.1%), ce_loss 1.111, lat_loss 22.130
09/28 04:28:32 AM | Train: [ 24/180] Step 450/1249 Loss 7.840 Prec@(1,3) (70.0%, 97.2%), ce_loss 1.111, lat_loss 22.129
09/28 04:28:54 AM | Train: [ 24/180] Step 500/1249 Loss 7.764 Prec@(1,3) (70.3%, 97.2%), ce_loss 1.111, lat_loss 22.129
09/28 04:29:15 AM | Train: [ 24/180] Step 550/1249 Loss 7.691 Prec@(1,3) (70.6%, 97.3%), ce_loss 1.110, lat_loss 22.129
09/28 04:29:37 AM | Train: [ 24/180] Step 600/1249 Loss 7.678 Prec@(1,3) (70.6%, 97.3%), ce_loss 1.110, lat_loss 22.129
09/28 04:29:59 AM | Train: [ 24/180] Step 650/1249 Loss 7.684 Prec@(1,3) (70.7%, 97.3%), ce_loss 1.110, lat_loss 22.128
09/28 04:30:20 AM | Train: [ 24/180] Step 700/1249 Loss 7.682 Prec@(1,3) (70.7%, 97.3%), ce_loss 1.109, lat_loss 22.128
09/28 04:30:43 AM | Train: [ 24/180] Step 750/1249 Loss 7.694 Prec@(1,3) (70.8%, 97.3%), ce_loss 1.109, lat_loss 22.128
09/28 04:31:07 AM | Train: [ 24/180] Step 800/1249 Loss 7.648 Prec@(1,3) (71.0%, 97.4%), ce_loss 1.109, lat_loss 22.128
09/28 04:31:32 AM | Train: [ 24/180] Step 850/1249 Loss 7.608 Prec@(1,3) (71.1%, 97.4%), ce_loss 1.108, lat_loss 22.127
09/28 04:31:57 AM | Train: [ 24/180] Step 900/1249 Loss 7.577 Prec@(1,3) (71.3%, 97.4%), ce_loss 1.108, lat_loss 22.127
09/28 04:32:22 AM | Train: [ 24/180] Step 950/1249 Loss 7.564 Prec@(1,3) (71.3%, 97.5%), ce_loss 1.107, lat_loss 22.127
09/28 04:32:48 AM | Train: [ 24/180] Step 1000/1249 Loss 7.574 Prec@(1,3) (71.2%, 97.4%), ce_loss 1.107, lat_loss 22.127
09/28 04:33:12 AM | Train: [ 24/180] Step 1050/1249 Loss 7.541 Prec@(1,3) (71.3%, 97.5%), ce_loss 1.107, lat_loss 22.126
09/28 04:33:36 AM | Train: [ 24/180] Step 1100/1249 Loss 7.529 Prec@(1,3) (71.3%, 97.4%), ce_loss 1.106, lat_loss 22.126
09/28 04:34:00 AM | Train: [ 24/180] Step 1150/1249 Loss 7.484 Prec@(1,3) (71.4%, 97.5%), ce_loss 1.106, lat_loss 22.126
09/28 04:34:22 AM | Train: [ 24/180] Step 1200/1249 Loss 7.468 Prec@(1,3) (71.5%, 97.5%), ce_loss 1.105, lat_loss 22.126
09/28 04:34:46 AM | Train: [ 24/180] Step 1249/1249 Loss 7.451 Prec@(1,3) (71.5%, 97.5%), ce_loss 1.105, lat_loss 22.125
09/28 04:34:47 AM | _w_step_train: [ 24/180] Final Prec@1 71.5125% Time 575.19
09/28 04:34:47 AM | Start to train theta for epoch 23
09/28 04:35:08 AM | Train: [ 24/180] Step 050/312 Loss 7.474 Prec@(1,3) (71.6%, 97.9%), ce_loss 1.105, lat_loss 22.125
09/28 04:35:25 AM | Train: [ 24/180] Step 100/312 Loss 7.323 Prec@(1,3) (72.2%, 97.9%), ce_loss 1.104, lat_loss 22.125
09/28 04:35:45 AM | Train: [ 24/180] Step 150/312 Loss 7.248 Prec@(1,3) (72.2%, 97.8%), ce_loss 1.104, lat_loss 22.125
09/28 04:36:03 AM | Train: [ 24/180] Step 200/312 Loss 7.250 Prec@(1,3) (72.2%, 97.8%), ce_loss 1.104, lat_loss 22.124
09/28 04:36:22 AM | Train: [ 24/180] Step 250/312 Loss 7.187 Prec@(1,3) (72.4%, 97.9%), ce_loss 1.103, lat_loss 22.124
09/28 04:36:43 AM | Train: [ 24/180] Step 300/312 Loss 7.187 Prec@(1,3) (72.4%, 97.8%), ce_loss 1.103, lat_loss 22.124
09/28 04:36:48 AM | Train: [ 24/180] Step 312/312 Loss 7.197 Prec@(1,3) (72.3%, 97.7%), ce_loss 1.103, lat_loss 22.124
09/28 04:36:49 AM | _theta_step_train: [ 24/180] Final Prec@1 72.3300% Time 121.91
09/28 04:36:54 AM | Valid: [ 24/180] Step 050/312 Loss 8.730 Prec@(1,3) (66.9%, 97.2%), ce_loss 1.103, lat_loss 22.123
09/28 04:36:58 AM | Valid: [ 24/180] Step 100/312 Loss 8.834 Prec@(1,3) (69.0%, 97.0%), ce_loss 1.102, lat_loss 22.123
09/28 04:37:03 AM | Valid: [ 24/180] Step 150/312 Loss 8.701 Prec@(1,3) (69.2%, 97.0%), ce_loss 1.102, lat_loss 22.123
09/28 04:37:08 AM | Valid: [ 24/180] Step 200/312 Loss 8.574 Prec@(1,3) (69.1%, 97.0%), ce_loss 1.102, lat_loss 22.123
09/28 04:37:12 AM | Valid: [ 24/180] Step 250/312 Loss 8.537 Prec@(1,3) (69.3%, 96.9%), ce_loss 1.102, lat_loss 22.122
09/28 04:37:17 AM | Valid: [ 24/180] Step 300/312 Loss 8.562 Prec@(1,3) (69.3%, 97.0%), ce_loss 1.102, lat_loss 22.122
09/28 04:37:18 AM | Valid: [ 24/180] Step 312/312 Loss 8.560 Prec@(1,3) (69.2%, 97.0%), ce_loss 1.102, lat_loss 22.122
09/28 04:37:18 AM | val: [ 24/180] Final Prec@1 69.2300% Time 29.61
09/28 04:37:18 AM | Start to train weights for epoch 24
09/28 04:37:45 AM | Train: [ 25/180] Step 050/1249 Loss 7.776 Prec@(1,3) (69.6%, 97.5%), ce_loss 1.101, lat_loss 22.122
09/28 04:38:08 AM | Train: [ 25/180] Step 100/1249 Loss 7.456 Prec@(1,3) (70.8%, 97.8%), ce_loss 1.101, lat_loss 22.121
09/28 04:38:31 AM | Train: [ 25/180] Step 150/1249 Loss 7.176 Prec@(1,3) (72.1%, 97.9%), ce_loss 1.101, lat_loss 22.121
09/28 04:38:53 AM | Train: [ 25/180] Step 200/1249 Loss 7.142 Prec@(1,3) (72.5%, 97.8%), ce_loss 1.100, lat_loss 22.121
09/28 04:39:16 AM | Train: [ 25/180] Step 250/1249 Loss 7.190 Prec@(1,3) (72.2%, 97.8%), ce_loss 1.100, lat_loss 22.121
09/28 04:39:39 AM | Train: [ 25/180] Step 300/1249 Loss 7.120 Prec@(1,3) (72.3%, 97.9%), ce_loss 1.099, lat_loss 22.120
09/28 04:40:02 AM | Train: [ 25/180] Step 350/1249 Loss 7.087 Prec@(1,3) (72.6%, 97.9%), ce_loss 1.099, lat_loss 22.120
09/28 04:40:26 AM | Train: [ 25/180] Step 400/1249 Loss 7.051 Prec@(1,3) (72.8%, 97.9%), ce_loss 1.099, lat_loss 22.120
09/28 04:40:49 AM | Train: [ 25/180] Step 450/1249 Loss 7.047 Prec@(1,3) (72.7%, 97.9%), ce_loss 1.098, lat_loss 22.119
09/28 04:41:09 AM | Train: [ 25/180] Step 500/1249 Loss 7.092 Prec@(1,3) (72.5%, 98.0%), ce_loss 1.098, lat_loss 22.119
09/28 04:41:32 AM | Train: [ 25/180] Step 550/1249 Loss 7.091 Prec@(1,3) (72.6%, 98.0%), ce_loss 1.098, lat_loss 22.119
09/28 04:41:55 AM | Train: [ 25/180] Step 600/1249 Loss 7.097 Prec@(1,3) (72.5%, 97.9%), ce_loss 1.097, lat_loss 22.119
09/28 04:42:18 AM | Train: [ 25/180] Step 650/1249 Loss 7.107 Prec@(1,3) (72.5%, 98.0%), ce_loss 1.097, lat_loss 22.118
09/28 04:42:43 AM | Train: [ 25/180] Step 700/1249 Loss 7.079 Prec@(1,3) (72.6%, 98.0%), ce_loss 1.096, lat_loss 22.118
09/28 04:43:08 AM | Train: [ 25/180] Step 750/1249 Loss 7.121 Prec@(1,3) (72.4%, 97.9%), ce_loss 1.096, lat_loss 22.118
09/28 04:43:31 AM | Train: [ 25/180] Step 800/1249 Loss 7.145 Prec@(1,3) (72.4%, 97.9%), ce_loss 1.096, lat_loss 22.117
09/28 04:43:53 AM | Train: [ 25/180] Step 850/1249 Loss 7.126 Prec@(1,3) (72.5%, 97.8%), ce_loss 1.095, lat_loss 22.117
09/28 04:44:18 AM | Train: [ 25/180] Step 900/1249 Loss 7.158 Prec@(1,3) (72.4%, 97.8%), ce_loss 1.095, lat_loss 22.117
09/28 04:44:43 AM | Train: [ 25/180] Step 950/1249 Loss 7.137 Prec@(1,3) (72.5%, 97.8%), ce_loss 1.095, lat_loss 22.117
09/28 04:45:07 AM | Train: [ 25/180] Step 1000/1249 Loss 7.154 Prec@(1,3) (72.4%, 97.8%), ce_loss 1.094, lat_loss 22.116
09/28 04:45:32 AM | Train: [ 25/180] Step 1050/1249 Loss 7.140 Prec@(1,3) (72.5%, 97.8%), ce_loss 1.094, lat_loss 22.116
09/28 04:45:58 AM | Train: [ 25/180] Step 1100/1249 Loss 7.107 Prec@(1,3) (72.6%, 97.8%), ce_loss 1.094, lat_loss 22.116
09/28 04:46:22 AM | Train: [ 25/180] Step 1150/1249 Loss 7.085 Prec@(1,3) (72.8%, 97.8%), ce_loss 1.093, lat_loss 22.116
09/28 04:46:47 AM | Train: [ 25/180] Step 1200/1249 Loss 7.079 Prec@(1,3) (72.8%, 97.9%), ce_loss 1.093, lat_loss 22.115
09/28 04:47:11 AM | Train: [ 25/180] Step 1249/1249 Loss 7.086 Prec@(1,3) (72.8%, 97.9%), ce_loss 1.093, lat_loss 22.115
09/28 04:47:12 AM | _w_step_train: [ 25/180] Final Prec@1 72.8375% Time 593.39
09/28 04:47:12 AM | Start to train theta for epoch 24
09/28 04:47:33 AM | Train: [ 25/180] Step 050/312 Loss 7.120 Prec@(1,3) (73.0%, 98.0%), ce_loss 1.092, lat_loss 22.115
09/28 04:47:53 AM | Train: [ 25/180] Step 100/312 Loss 6.866 Prec@(1,3) (73.9%, 98.2%), ce_loss 1.092, lat_loss 22.114
09/28 04:48:13 AM | Train: [ 25/180] Step 150/312 Loss 6.931 Prec@(1,3) (73.5%, 98.1%), ce_loss 1.091, lat_loss 22.114
09/28 04:48:32 AM | Train: [ 25/180] Step 200/312 Loss 6.904 Prec@(1,3) (73.8%, 98.0%), ce_loss 1.091, lat_loss 22.114
09/28 04:48:49 AM | Train: [ 25/180] Step 250/312 Loss 6.885 Prec@(1,3) (73.8%, 97.9%), ce_loss 1.091, lat_loss 22.114
09/28 04:49:06 AM | Train: [ 25/180] Step 300/312 Loss 6.905 Prec@(1,3) (73.7%, 97.9%), ce_loss 1.090, lat_loss 22.113
09/28 04:49:10 AM | Train: [ 25/180] Step 312/312 Loss 6.889 Prec@(1,3) (73.7%, 97.9%), ce_loss 1.090, lat_loss 22.113
09/28 04:49:10 AM | _theta_step_train: [ 25/180] Final Prec@1 73.7400% Time 118.43
09/28 04:49:15 AM | Valid: [ 25/180] Step 050/312 Loss 7.493 Prec@(1,3) (71.8%, 97.4%), ce_loss 1.090, lat_loss 22.113
09/28 04:49:20 AM | Valid: [ 25/180] Step 100/312 Loss 7.992 Prec@(1,3) (70.3%, 97.2%), ce_loss 1.090, lat_loss 22.113
09/28 04:49:25 AM | Valid: [ 25/180] Step 150/312 Loss 7.872 Prec@(1,3) (70.8%, 97.1%), ce_loss 1.089, lat_loss 22.113
09/28 04:49:30 AM | Valid: [ 25/180] Step 200/312 Loss 7.939 Prec@(1,3) (70.2%, 97.1%), ce_loss 1.089, lat_loss 22.112
09/28 04:49:34 AM | Valid: [ 25/180] Step 250/312 Loss 7.909 Prec@(1,3) (70.5%, 97.1%), ce_loss 1.089, lat_loss 22.112
09/28 04:49:39 AM | Valid: [ 25/180] Step 300/312 Loss 7.865 Prec@(1,3) (70.6%, 97.1%), ce_loss 1.089, lat_loss 22.112
09/28 04:49:40 AM | Valid: [ 25/180] Step 312/312 Loss 7.841 Prec@(1,3) (70.8%, 97.1%), ce_loss 1.089, lat_loss 22.112
09/28 04:49:40 AM | val: [ 25/180] Final Prec@1 70.7500% Time 29.87
09/28 04:49:40 AM | Start to train weights for epoch 25
09/28 04:50:05 AM | Train: [ 26/180] Step 050/1249 Loss 6.485 Prec@(1,3) (74.5%, 98.2%), ce_loss 1.088, lat_loss 22.112
09/28 04:50:28 AM | Train: [ 26/180] Step 100/1249 Loss 6.693 Prec@(1,3) (74.3%, 98.0%), ce_loss 1.088, lat_loss 22.111
09/28 04:50:53 AM | Train: [ 26/180] Step 150/1249 Loss 6.923 Prec@(1,3) (73.9%, 97.9%), ce_loss 1.088, lat_loss 22.111
09/28 04:51:17 AM | Train: [ 26/180] Step 200/1249 Loss 6.850 Prec@(1,3) (74.2%, 97.9%), ce_loss 1.087, lat_loss 22.111
09/28 04:51:40 AM | Train: [ 26/180] Step 250/1249 Loss 6.919 Prec@(1,3) (74.1%, 97.9%), ce_loss 1.087, lat_loss 22.110
09/28 04:52:01 AM | Train: [ 26/180] Step 300/1249 Loss 6.925 Prec@(1,3) (74.0%, 97.8%), ce_loss 1.086, lat_loss 22.110
09/28 04:52:22 AM | Train: [ 26/180] Step 350/1249 Loss 6.955 Prec@(1,3) (73.8%, 97.8%), ce_loss 1.086, lat_loss 22.110
09/28 04:52:44 AM | Train: [ 26/180] Step 400/1249 Loss 6.995 Prec@(1,3) (73.7%, 97.7%), ce_loss 1.086, lat_loss 22.110
09/28 04:53:09 AM | Train: [ 26/180] Step 450/1249 Loss 7.002 Prec@(1,3) (73.6%, 97.7%), ce_loss 1.085, lat_loss 22.109
09/28 04:53:33 AM | Train: [ 26/180] Step 500/1249 Loss 7.018 Prec@(1,3) (73.5%, 97.7%), ce_loss 1.085, lat_loss 22.109
09/28 04:53:57 AM | Train: [ 26/180] Step 550/1249 Loss 7.007 Prec@(1,3) (73.4%, 97.8%), ce_loss 1.085, lat_loss 22.109
09/28 04:54:20 AM | Train: [ 26/180] Step 600/1249 Loss 6.979 Prec@(1,3) (73.5%, 97.8%), ce_loss 1.084, lat_loss 22.109
09/28 04:54:44 AM | Train: [ 26/180] Step 650/1249 Loss 7.014 Prec@(1,3) (73.5%, 97.8%), ce_loss 1.084, lat_loss 22.108
09/28 04:55:08 AM | Train: [ 26/180] Step 700/1249 Loss 7.013 Prec@(1,3) (73.5%, 97.8%), ce_loss 1.084, lat_loss 22.108
09/28 04:55:31 AM | Train: [ 26/180] Step 750/1249 Loss 7.001 Prec@(1,3) (73.5%, 97.7%), ce_loss 1.083, lat_loss 22.108
09/28 04:55:55 AM | Train: [ 26/180] Step 800/1249 Loss 7.003 Prec@(1,3) (73.5%, 97.7%), ce_loss 1.083, lat_loss 22.108
09/28 04:56:19 AM | Train: [ 26/180] Step 850/1249 Loss 6.978 Prec@(1,3) (73.6%, 97.7%), ce_loss 1.083, lat_loss 22.107
09/28 04:56:41 AM | Train: [ 26/180] Step 900/1249 Loss 6.972 Prec@(1,3) (73.7%, 97.8%), ce_loss 1.082, lat_loss 22.107
09/28 04:57:02 AM | Train: [ 26/180] Step 950/1249 Loss 6.958 Prec@(1,3) (73.7%, 97.8%), ce_loss 1.082, lat_loss 22.107
09/28 04:57:22 AM | Train: [ 26/180] Step 1000/1249 Loss 6.956 Prec@(1,3) (73.7%, 97.8%), ce_loss 1.082, lat_loss 22.107
09/28 04:57:46 AM | Train: [ 26/180] Step 1050/1249 Loss 6.954 Prec@(1,3) (73.7%, 97.8%), ce_loss 1.081, lat_loss 22.106
09/28 04:58:09 AM | Train: [ 26/180] Step 1100/1249 Loss 6.944 Prec@(1,3) (73.7%, 97.8%), ce_loss 1.081, lat_loss 22.106
09/28 04:58:33 AM | Train: [ 26/180] Step 1150/1249 Loss 6.929 Prec@(1,3) (73.7%, 97.8%), ce_loss 1.080, lat_loss 22.106
09/28 04:58:57 AM | Train: [ 26/180] Step 1200/1249 Loss 6.895 Prec@(1,3) (73.8%, 97.8%), ce_loss 1.080, lat_loss 22.105
09/28 04:59:21 AM | Train: [ 26/180] Step 1249/1249 Loss 6.917 Prec@(1,3) (73.8%, 97.8%), ce_loss 1.080, lat_loss 22.105
09/28 04:59:21 AM | _w_step_train: [ 26/180] Final Prec@1 73.7600% Time 581.08
09/28 04:59:21 AM | Start to train theta for epoch 25
09/28 04:59:41 AM | Train: [ 26/180] Step 050/312 Loss 7.154 Prec@(1,3) (73.6%, 97.5%), ce_loss 1.079, lat_loss 22.105
09/28 04:59:58 AM | Train: [ 26/180] Step 100/312 Loss 7.166 Prec@(1,3) (73.6%, 97.4%), ce_loss 1.079, lat_loss 22.105
09/28 05:00:17 AM | Train: [ 26/180] Step 150/312 Loss 7.352 Prec@(1,3) (73.1%, 97.5%), ce_loss 1.079, lat_loss 22.104
09/28 05:00:37 AM | Train: [ 26/180] Step 200/312 Loss 7.210 Prec@(1,3) (73.4%, 97.6%), ce_loss 1.078, lat_loss 22.104
09/28 05:00:56 AM | Train: [ 26/180] Step 250/312 Loss 7.113 Prec@(1,3) (73.8%, 97.6%), ce_loss 1.078, lat_loss 22.104
09/28 05:01:17 AM | Train: [ 26/180] Step 300/312 Loss 7.062 Prec@(1,3) (73.7%, 97.6%), ce_loss 1.078, lat_loss 22.104
09/28 05:01:22 AM | Train: [ 26/180] Step 312/312 Loss 7.017 Prec@(1,3) (73.9%, 97.6%), ce_loss 1.078, lat_loss 22.104
09/28 05:01:22 AM | _theta_step_train: [ 26/180] Final Prec@1 73.8600% Time 121.12
09/28 05:01:27 AM | Valid: [ 26/180] Step 050/312 Loss 7.553 Prec@(1,3) (69.4%, 97.9%), ce_loss 1.077, lat_loss 22.103
09/28 05:01:32 AM | Valid: [ 26/180] Step 100/312 Loss 7.774 Prec@(1,3) (69.5%, 97.4%), ce_loss 1.077, lat_loss 22.103
09/28 05:01:36 AM | Valid: [ 26/180] Step 150/312 Loss 7.581 Prec@(1,3) (70.6%, 97.5%), ce_loss 1.077, lat_loss 22.103
09/28 05:01:41 AM | Valid: [ 26/180] Step 200/312 Loss 7.753 Prec@(1,3) (70.1%, 97.3%), ce_loss 1.077, lat_loss 22.103
09/28 05:01:46 AM | Valid: [ 26/180] Step 250/312 Loss 7.722 Prec@(1,3) (70.0%, 97.4%), ce_loss 1.076, lat_loss 22.102
09/28 05:01:50 AM | Valid: [ 26/180] Step 300/312 Loss 7.708 Prec@(1,3) (70.2%, 97.4%), ce_loss 1.076, lat_loss 22.102
09/28 05:01:51 AM | Valid: [ 26/180] Step 312/312 Loss 7.748 Prec@(1,3) (70.1%, 97.3%), ce_loss 1.076, lat_loss 22.102
09/28 05:01:51 AM | val: [ 26/180] Final Prec@1 70.1400% Time 29.07
09/28 05:01:51 AM | Start to train weights for epoch 26
09/28 05:02:17 AM | Train: [ 27/180] Step 050/1249 Loss 6.689 Prec@(1,3) (74.4%, 97.9%), ce_loss 1.076, lat_loss 22.102
09/28 05:02:42 AM | Train: [ 27/180] Step 100/1249 Loss 6.552 Prec@(1,3) (75.4%, 98.1%), ce_loss 1.075, lat_loss 22.101
09/28 05:03:06 AM | Train: [ 27/180] Step 150/1249 Loss 6.531 Prec@(1,3) (75.3%, 98.2%), ce_loss 1.075, lat_loss 22.101
09/28 05:03:31 AM | Train: [ 27/180] Step 200/1249 Loss 6.581 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.075, lat_loss 22.101
09/28 05:03:56 AM | Train: [ 27/180] Step 250/1249 Loss 6.633 Prec@(1,3) (74.8%, 98.1%), ce_loss 1.074, lat_loss 22.101
09/28 05:04:21 AM | Train: [ 27/180] Step 300/1249 Loss 6.690 Prec@(1,3) (74.6%, 98.1%), ce_loss 1.074, lat_loss 22.100
09/28 05:04:46 AM | Train: [ 27/180] Step 350/1249 Loss 6.702 Prec@(1,3) (74.6%, 98.1%), ce_loss 1.074, lat_loss 22.100
09/28 05:05:10 AM | Train: [ 27/180] Step 400/1249 Loss 6.744 Prec@(1,3) (74.4%, 98.1%), ce_loss 1.073, lat_loss 22.100
09/28 05:05:35 AM | Train: [ 27/180] Step 450/1249 Loss 6.770 Prec@(1,3) (74.2%, 98.1%), ce_loss 1.073, lat_loss 22.100
09/28 05:05:57 AM | Train: [ 27/180] Step 500/1249 Loss 6.731 Prec@(1,3) (74.4%, 98.1%), ce_loss 1.073, lat_loss 22.099
09/28 05:06:22 AM | Train: [ 27/180] Step 550/1249 Loss 6.732 Prec@(1,3) (74.4%, 98.0%), ce_loss 1.072, lat_loss 22.099
09/28 05:06:47 AM | Train: [ 27/180] Step 600/1249 Loss 6.725 Prec@(1,3) (74.4%, 98.0%), ce_loss 1.072, lat_loss 22.099
09/28 05:07:11 AM | Train: [ 27/180] Step 650/1249 Loss 6.686 Prec@(1,3) (74.5%, 98.0%), ce_loss 1.072, lat_loss 22.099
09/28 05:07:35 AM | Train: [ 27/180] Step 700/1249 Loss 6.706 Prec@(1,3) (74.3%, 98.0%), ce_loss 1.071, lat_loss 22.098
09/28 05:08:00 AM | Train: [ 27/180] Step 750/1249 Loss 6.675 Prec@(1,3) (74.4%, 98.0%), ce_loss 1.071, lat_loss 22.098
09/28 05:08:24 AM | Train: [ 27/180] Step 800/1249 Loss 6.676 Prec@(1,3) (74.4%, 98.0%), ce_loss 1.070, lat_loss 22.098
09/28 05:08:49 AM | Train: [ 27/180] Step 850/1249 Loss 6.685 Prec@(1,3) (74.3%, 98.0%), ce_loss 1.070, lat_loss 22.097
09/28 05:09:14 AM | Train: [ 27/180] Step 900/1249 Loss 6.692 Prec@(1,3) (74.3%, 98.0%), ce_loss 1.070, lat_loss 22.097
09/28 05:09:39 AM | Train: [ 27/180] Step 950/1249 Loss 6.719 Prec@(1,3) (74.3%, 98.0%), ce_loss 1.069, lat_loss 22.097
09/28 05:10:04 AM | Train: [ 27/180] Step 1000/1249 Loss 6.693 Prec@(1,3) (74.3%, 98.0%), ce_loss 1.069, lat_loss 22.097
09/28 05:10:27 AM | Train: [ 27/180] Step 1050/1249 Loss 6.710 Prec@(1,3) (74.3%, 98.0%), ce_loss 1.069, lat_loss 22.096
09/28 05:10:52 AM | Train: [ 27/180] Step 1100/1249 Loss 6.683 Prec@(1,3) (74.4%, 98.0%), ce_loss 1.068, lat_loss 22.096
09/28 05:11:14 AM | Train: [ 27/180] Step 1150/1249 Loss 6.698 Prec@(1,3) (74.4%, 98.0%), ce_loss 1.068, lat_loss 22.096
09/28 05:11:37 AM | Train: [ 27/180] Step 1200/1249 Loss 6.691 Prec@(1,3) (74.5%, 98.0%), ce_loss 1.068, lat_loss 22.096
09/28 05:12:01 AM | Train: [ 27/180] Step 1249/1249 Loss 6.704 Prec@(1,3) (74.4%, 97.9%), ce_loss 1.067, lat_loss 22.095
09/28 05:12:01 AM | _w_step_train: [ 27/180] Final Prec@1 74.3900% Time 610.26
09/28 05:12:01 AM | Start to train theta for epoch 26
09/28 05:12:23 AM | Train: [ 27/180] Step 050/312 Loss 7.294 Prec@(1,3) (72.0%, 97.1%), ce_loss 1.067, lat_loss 22.095
09/28 05:12:43 AM | Train: [ 27/180] Step 100/312 Loss 6.917 Prec@(1,3) (73.5%, 97.8%), ce_loss 1.067, lat_loss 22.095
09/28 05:13:02 AM | Train: [ 27/180] Step 150/312 Loss 6.871 Prec@(1,3) (73.9%, 97.7%), ce_loss 1.066, lat_loss 22.095
09/28 05:13:21 AM | Train: [ 27/180] Step 200/312 Loss 6.771 Prec@(1,3) (74.1%, 97.8%), ce_loss 1.066, lat_loss 22.094
09/28 05:13:39 AM | Train: [ 27/180] Step 250/312 Loss 6.749 Prec@(1,3) (74.5%, 97.8%), ce_loss 1.066, lat_loss 22.094
09/28 05:13:58 AM | Train: [ 27/180] Step 300/312 Loss 6.769 Prec@(1,3) (74.4%, 97.8%), ce_loss 1.065, lat_loss 22.094
09/28 05:14:03 AM | Train: [ 27/180] Step 312/312 Loss 6.799 Prec@(1,3) (74.3%, 97.8%), ce_loss 1.065, lat_loss 22.094
09/28 05:14:04 AM | _theta_step_train: [ 27/180] Final Prec@1 74.3200% Time 122.09
09/28 05:14:09 AM | Valid: [ 27/180] Step 050/312 Loss 9.525 Prec@(1,3) (67.9%, 96.4%), ce_loss 1.065, lat_loss 22.094
09/28 05:14:14 AM | Valid: [ 27/180] Step 100/312 Loss 9.264 Prec@(1,3) (69.0%, 96.3%), ce_loss 1.065, lat_loss 22.093
09/28 05:14:18 AM | Valid: [ 27/180] Step 150/312 Loss 8.898 Prec@(1,3) (70.1%, 96.3%), ce_loss 1.065, lat_loss 22.093
09/28 05:14:23 AM | Valid: [ 27/180] Step 200/312 Loss 8.665 Prec@(1,3) (70.0%, 96.5%), ce_loss 1.065, lat_loss 22.093
09/28 05:14:28 AM | Valid: [ 27/180] Step 250/312 Loss 8.564 Prec@(1,3) (69.8%, 96.6%), ce_loss 1.065, lat_loss 22.093
09/28 05:14:32 AM | Valid: [ 27/180] Step 300/312 Loss 8.692 Prec@(1,3) (69.5%, 96.4%), ce_loss 1.065, lat_loss 22.092
09/28 05:14:33 AM | Valid: [ 27/180] Step 312/312 Loss 8.676 Prec@(1,3) (69.5%, 96.5%), ce_loss 1.065, lat_loss 22.092
09/28 05:14:33 AM | val: [ 27/180] Final Prec@1 69.5000% Time 29.88
09/28 05:14:33 AM | Start to train weights for epoch 27
09/28 05:15:00 AM | Train: [ 28/180] Step 050/1249 Loss 6.254 Prec@(1,3) (75.7%, 98.3%), ce_loss 1.064, lat_loss 22.092
09/28 05:15:22 AM | Train: [ 28/180] Step 100/1249 Loss 6.522 Prec@(1,3) (74.1%, 98.0%), ce_loss 1.064, lat_loss 22.092
09/28 05:15:43 AM | Train: [ 28/180] Step 150/1249 Loss 6.610 Prec@(1,3) (74.2%, 97.9%), ce_loss 1.064, lat_loss 22.091
09/28 05:16:04 AM | Train: [ 28/180] Step 200/1249 Loss 6.582 Prec@(1,3) (74.2%, 98.0%), ce_loss 1.063, lat_loss 22.091
09/28 05:16:25 AM | Train: [ 28/180] Step 250/1249 Loss 6.598 Prec@(1,3) (74.4%, 98.0%), ce_loss 1.063, lat_loss 22.091
09/28 05:16:46 AM | Train: [ 28/180] Step 300/1249 Loss 6.546 Prec@(1,3) (74.6%, 98.2%), ce_loss 1.063, lat_loss 22.091
09/28 05:17:06 AM | Train: [ 28/180] Step 350/1249 Loss 6.503 Prec@(1,3) (74.8%, 98.2%), ce_loss 1.062, lat_loss 22.090
09/28 05:17:20 AM | Train: [ 28/180] Step 400/1249 Loss 6.466 Prec@(1,3) (74.9%, 98.2%), ce_loss 1.062, lat_loss 22.090
09/28 05:17:35 AM | Train: [ 28/180] Step 450/1249 Loss 6.499 Prec@(1,3) (74.8%, 98.1%), ce_loss 1.062, lat_loss 22.090
09/28 05:17:49 AM | Train: [ 28/180] Step 500/1249 Loss 6.534 Prec@(1,3) (74.8%, 97.9%), ce_loss 1.061, lat_loss 22.090
09/28 05:18:04 AM | Train: [ 28/180] Step 550/1249 Loss 6.534 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.061, lat_loss 22.089
09/28 05:18:18 AM | Train: [ 28/180] Step 600/1249 Loss 6.565 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.061, lat_loss 22.089
09/28 05:18:34 AM | Train: [ 28/180] Step 650/1249 Loss 6.531 Prec@(1,3) (75.0%, 98.0%), ce_loss 1.060, lat_loss 22.089
09/28 05:18:48 AM | Train: [ 28/180] Step 700/1249 Loss 6.548 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.060, lat_loss 22.089
09/28 05:19:03 AM | Train: [ 28/180] Step 750/1249 Loss 6.562 Prec@(1,3) (74.8%, 97.9%), ce_loss 1.060, lat_loss 22.088
09/28 05:19:17 AM | Train: [ 28/180] Step 800/1249 Loss 6.545 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.059, lat_loss 22.088
09/28 05:19:32 AM | Train: [ 28/180] Step 850/1249 Loss 6.538 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.059, lat_loss 22.088
09/28 05:19:46 AM | Train: [ 28/180] Step 900/1249 Loss 6.560 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.059, lat_loss 22.088
09/28 05:20:01 AM | Train: [ 28/180] Step 950/1249 Loss 6.572 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.058, lat_loss 22.087
09/28 05:20:15 AM | Train: [ 28/180] Step 1000/1249 Loss 6.551 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.058, lat_loss 22.087
09/28 05:20:30 AM | Train: [ 28/180] Step 1050/1249 Loss 6.553 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.058, lat_loss 22.087
09/28 05:20:45 AM | Train: [ 28/180] Step 1100/1249 Loss 6.564 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.057, lat_loss 22.087
09/28 05:20:59 AM | Train: [ 28/180] Step 1150/1249 Loss 6.557 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.057, lat_loss 22.086
09/28 05:21:14 AM | Train: [ 28/180] Step 1200/1249 Loss 6.559 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.057, lat_loss 22.086
09/28 05:21:28 AM | Train: [ 28/180] Step 1249/1249 Loss 6.552 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.056, lat_loss 22.086
09/28 05:21:28 AM | _w_step_train: [ 28/180] Final Prec@1 74.8700% Time 414.33
09/28 05:21:28 AM | Start to train theta for epoch 27
09/28 05:21:48 AM | Train: [ 28/180] Step 050/312 Loss 6.887 Prec@(1,3) (74.0%, 97.2%), ce_loss 1.056, lat_loss 22.086
09/28 05:22:08 AM | Train: [ 28/180] Step 100/312 Loss 6.583 Prec@(1,3) (75.0%, 97.5%), ce_loss 1.055, lat_loss 22.085
09/28 05:22:27 AM | Train: [ 28/180] Step 150/312 Loss 6.422 Prec@(1,3) (75.4%, 97.7%), ce_loss 1.055, lat_loss 22.085
09/28 05:22:47 AM | Train: [ 28/180] Step 200/312 Loss 6.419 Prec@(1,3) (75.3%, 97.8%), ce_loss 1.055, lat_loss 22.085
09/28 05:23:07 AM | Train: [ 28/180] Step 250/312 Loss 6.533 Prec@(1,3) (74.9%, 97.7%), ce_loss 1.054, lat_loss 22.085
09/28 05:23:27 AM | Train: [ 28/180] Step 300/312 Loss 6.614 Prec@(1,3) (74.7%, 97.7%), ce_loss 1.054, lat_loss 22.085
09/28 05:23:32 AM | Train: [ 28/180] Step 312/312 Loss 6.637 Prec@(1,3) (74.7%, 97.6%), ce_loss 1.054, lat_loss 22.085
09/28 05:23:32 AM | _theta_step_train: [ 28/180] Final Prec@1 74.6800% Time 124.24
09/28 05:23:37 AM | Valid: [ 28/180] Step 050/312 Loss 7.816 Prec@(1,3) (70.2%, 97.2%), ce_loss 1.054, lat_loss 22.084
09/28 05:23:42 AM | Valid: [ 28/180] Step 100/312 Loss 8.293 Prec@(1,3) (68.7%, 96.3%), ce_loss 1.054, lat_loss 22.084
09/28 05:23:47 AM | Valid: [ 28/180] Step 150/312 Loss 8.280 Prec@(1,3) (68.7%, 96.3%), ce_loss 1.054, lat_loss 22.084
09/28 05:23:51 AM | Valid: [ 28/180] Step 200/312 Loss 8.483 Prec@(1,3) (68.3%, 96.5%), ce_loss 1.054, lat_loss 22.084
09/28 05:23:56 AM | Valid: [ 28/180] Step 250/312 Loss 8.417 Prec@(1,3) (68.3%, 96.6%), ce_loss 1.054, lat_loss 22.084
09/28 05:24:00 AM | Valid: [ 28/180] Step 300/312 Loss 8.272 Prec@(1,3) (68.8%, 96.7%), ce_loss 1.053, lat_loss 22.083
09/28 05:24:01 AM | Valid: [ 28/180] Step 312/312 Loss 8.274 Prec@(1,3) (68.8%, 96.7%), ce_loss 1.053, lat_loss 22.083
09/28 05:24:01 AM | val: [ 28/180] Final Prec@1 68.7600% Time 29.24
09/28 05:24:01 AM | Start to train weights for epoch 28
09/28 05:24:28 AM | Train: [ 29/180] Step 050/1249 Loss 6.528 Prec@(1,3) (75.6%, 98.3%), ce_loss 1.053, lat_loss 22.083
09/28 05:24:52 AM | Train: [ 29/180] Step 100/1249 Loss 6.349 Prec@(1,3) (76.1%, 98.4%), ce_loss 1.053, lat_loss 22.083
09/28 05:25:15 AM | Train: [ 29/180] Step 150/1249 Loss 6.509 Prec@(1,3) (75.4%, 98.0%), ce_loss 1.052, lat_loss 22.083
09/28 05:25:38 AM | Train: [ 29/180] Step 200/1249 Loss 6.627 Prec@(1,3) (75.0%, 98.0%), ce_loss 1.052, lat_loss 22.083
09/28 05:26:00 AM | Train: [ 29/180] Step 250/1249 Loss 6.500 Prec@(1,3) (75.4%, 97.9%), ce_loss 1.052, lat_loss 22.082
09/28 05:26:23 AM | Train: [ 29/180] Step 300/1249 Loss 6.511 Prec@(1,3) (75.3%, 97.9%), ce_loss 1.051, lat_loss 22.082
09/28 05:26:48 AM | Train: [ 29/180] Step 350/1249 Loss 6.575 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.051, lat_loss 22.082
09/28 05:27:11 AM | Train: [ 29/180] Step 400/1249 Loss 6.640 Prec@(1,3) (74.8%, 97.9%), ce_loss 1.051, lat_loss 22.082
09/28 05:27:34 AM | Train: [ 29/180] Step 450/1249 Loss 6.613 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.051, lat_loss 22.082
09/28 05:27:59 AM | Train: [ 29/180] Step 500/1249 Loss 6.596 Prec@(1,3) (74.8%, 97.9%), ce_loss 1.050, lat_loss 22.081
09/28 05:28:23 AM | Train: [ 29/180] Step 550/1249 Loss 6.575 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.050, lat_loss 22.081
09/28 05:28:47 AM | Train: [ 29/180] Step 600/1249 Loss 6.585 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.050, lat_loss 22.081
09/28 05:29:11 AM | Train: [ 29/180] Step 650/1249 Loss 6.567 Prec@(1,3) (75.0%, 98.0%), ce_loss 1.049, lat_loss 22.081
09/28 05:29:34 AM | Train: [ 29/180] Step 700/1249 Loss 6.578 Prec@(1,3) (75.0%, 98.0%), ce_loss 1.049, lat_loss 22.081
09/28 05:29:58 AM | Train: [ 29/180] Step 750/1249 Loss 6.614 Prec@(1,3) (74.9%, 98.0%), ce_loss 1.049, lat_loss 22.081
09/28 05:30:22 AM | Train: [ 29/180] Step 800/1249 Loss 6.624 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.048, lat_loss 22.080
09/28 05:30:47 AM | Train: [ 29/180] Step 850/1249 Loss 6.617 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.048, lat_loss 22.080
09/28 05:31:09 AM | Train: [ 29/180] Step 900/1249 Loss 6.608 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.048, lat_loss 22.080
09/28 05:31:32 AM | Train: [ 29/180] Step 950/1249 Loss 6.580 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.047, lat_loss 22.080
09/28 05:31:56 AM | Train: [ 29/180] Step 1000/1249 Loss 6.582 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.047, lat_loss 22.080
09/28 05:32:19 AM | Train: [ 29/180] Step 1050/1249 Loss 6.572 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.047, lat_loss 22.079
09/28 05:32:43 AM | Train: [ 29/180] Step 1100/1249 Loss 6.547 Prec@(1,3) (75.1%, 97.9%), ce_loss 1.046, lat_loss 22.079
09/28 05:33:07 AM | Train: [ 29/180] Step 1150/1249 Loss 6.545 Prec@(1,3) (75.1%, 97.9%), ce_loss 1.046, lat_loss 22.079
09/28 05:33:31 AM | Train: [ 29/180] Step 1200/1249 Loss 6.547 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.046, lat_loss 22.079
09/28 05:33:56 AM | Train: [ 29/180] Step 1249/1249 Loss 6.524 Prec@(1,3) (75.1%, 98.0%), ce_loss 1.045, lat_loss 22.079
09/28 05:33:56 AM | _w_step_train: [ 29/180] Final Prec@1 75.0950% Time 594.51
09/28 05:33:56 AM | Start to train theta for epoch 28
09/28 05:34:16 AM | Train: [ 29/180] Step 050/312 Loss 6.670 Prec@(1,3) (75.1%, 97.7%), ce_loss 1.045, lat_loss 22.078
09/28 05:34:35 AM | Train: [ 29/180] Step 100/312 Loss 6.456 Prec@(1,3) (75.5%, 97.9%), ce_loss 1.045, lat_loss 22.078
09/28 05:34:54 AM | Train: [ 29/180] Step 150/312 Loss 6.472 Prec@(1,3) (75.4%, 97.9%), ce_loss 1.044, lat_loss 22.078
09/28 05:35:14 AM | Train: [ 29/180] Step 200/312 Loss 6.458 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.044, lat_loss 22.078
09/28 05:35:35 AM | Train: [ 29/180] Step 250/312 Loss 6.504 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.044, lat_loss 22.078
09/28 05:35:55 AM | Train: [ 29/180] Step 300/312 Loss 6.416 Prec@(1,3) (75.4%, 98.1%), ce_loss 1.043, lat_loss 22.077
09/28 05:36:00 AM | Train: [ 29/180] Step 312/312 Loss 6.395 Prec@(1,3) (75.5%, 98.1%), ce_loss 1.043, lat_loss 22.077
09/28 05:36:01 AM | _theta_step_train: [ 29/180] Final Prec@1 75.4900% Time 124.88
09/28 05:36:06 AM | Valid: [ 29/180] Step 050/312 Loss 7.405 Prec@(1,3) (71.8%, 97.2%), ce_loss 1.043, lat_loss 22.077
09/28 05:36:11 AM | Valid: [ 29/180] Step 100/312 Loss 8.192 Prec@(1,3) (70.9%, 96.8%), ce_loss 1.043, lat_loss 22.077
09/28 05:36:15 AM | Valid: [ 29/180] Step 150/312 Loss 8.097 Prec@(1,3) (70.7%, 97.0%), ce_loss 1.043, lat_loss 22.077
09/28 05:36:20 AM | Valid: [ 29/180] Step 200/312 Loss 8.187 Prec@(1,3) (70.4%, 97.0%), ce_loss 1.043, lat_loss 22.077
09/28 05:36:24 AM | Valid: [ 29/180] Step 250/312 Loss 8.242 Prec@(1,3) (70.0%, 96.8%), ce_loss 1.043, lat_loss 22.076
09/28 05:36:29 AM | Valid: [ 29/180] Step 300/312 Loss 8.201 Prec@(1,3) (70.0%, 96.7%), ce_loss 1.043, lat_loss 22.076
09/28 05:36:30 AM | Valid: [ 29/180] Step 312/312 Loss 8.199 Prec@(1,3) (69.9%, 96.8%), ce_loss 1.043, lat_loss 22.076
09/28 05:36:30 AM | val: [ 29/180] Final Prec@1 69.8900% Time 29.39
09/28 05:36:30 AM | Start to train weights for epoch 29
09/28 05:36:46 AM | Train: [ 30/180] Step 050/1249 Loss 6.405 Prec@(1,3) (76.3%, 98.3%), ce_loss 1.042, lat_loss 22.076
09/28 05:37:01 AM | Train: [ 30/180] Step 100/1249 Loss 6.587 Prec@(1,3) (75.1%, 98.1%), ce_loss 1.042, lat_loss 22.076
09/28 05:37:16 AM | Train: [ 30/180] Step 150/1249 Loss 6.610 Prec@(1,3) (74.9%, 98.2%), ce_loss 1.042, lat_loss 22.075
09/28 05:37:31 AM | Train: [ 30/180] Step 200/1249 Loss 6.639 Prec@(1,3) (74.7%, 98.1%), ce_loss 1.041, lat_loss 22.075
09/28 05:37:45 AM | Train: [ 30/180] Step 250/1249 Loss 6.628 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.041, lat_loss 22.075
09/28 05:38:00 AM | Train: [ 30/180] Step 300/1249 Loss 6.662 Prec@(1,3) (74.6%, 98.1%), ce_loss 1.041, lat_loss 22.075
09/28 05:38:14 AM | Train: [ 30/180] Step 350/1249 Loss 6.634 Prec@(1,3) (74.8%, 98.1%), ce_loss 1.040, lat_loss 22.075
09/28 05:38:29 AM | Train: [ 30/180] Step 400/1249 Loss 6.541 Prec@(1,3) (75.1%, 98.1%), ce_loss 1.040, lat_loss 22.074
09/28 05:38:43 AM | Train: [ 30/180] Step 450/1249 Loss 6.508 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.040, lat_loss 22.074
09/28 05:38:57 AM | Train: [ 30/180] Step 500/1249 Loss 6.515 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.039, lat_loss 22.074
09/28 05:39:12 AM | Train: [ 30/180] Step 550/1249 Loss 6.507 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.039, lat_loss 22.074
09/28 05:39:27 AM | Train: [ 30/180] Step 600/1249 Loss 6.540 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.039, lat_loss 22.073
09/28 05:39:41 AM | Train: [ 30/180] Step 650/1249 Loss 6.583 Prec@(1,3) (75.1%, 98.1%), ce_loss 1.039, lat_loss 22.073
09/28 05:39:55 AM | Train: [ 30/180] Step 700/1249 Loss 6.584 Prec@(1,3) (75.1%, 98.1%), ce_loss 1.038, lat_loss 22.073
09/28 05:40:10 AM | Train: [ 30/180] Step 750/1249 Loss 6.593 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.038, lat_loss 22.073
09/28 05:40:24 AM | Train: [ 30/180] Step 800/1249 Loss 6.625 Prec@(1,3) (74.9%, 98.1%), ce_loss 1.038, lat_loss 22.073
09/28 05:40:39 AM | Train: [ 30/180] Step 850/1249 Loss 6.660 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.038, lat_loss 22.072
09/28 05:40:53 AM | Train: [ 30/180] Step 900/1249 Loss 6.665 Prec@(1,3) (74.7%, 98.0%), ce_loss 1.037, lat_loss 22.072
09/28 05:41:08 AM | Train: [ 30/180] Step 950/1249 Loss 6.672 Prec@(1,3) (74.6%, 98.0%), ce_loss 1.037, lat_loss 22.072
09/28 05:41:22 AM | Train: [ 30/180] Step 1000/1249 Loss 6.686 Prec@(1,3) (74.6%, 98.0%), ce_loss 1.037, lat_loss 22.072
09/28 05:41:37 AM | Train: [ 30/180] Step 1050/1249 Loss 6.678 Prec@(1,3) (74.6%, 98.0%), ce_loss 1.037, lat_loss 22.072
09/28 05:41:51 AM | Train: [ 30/180] Step 1100/1249 Loss 6.677 Prec@(1,3) (74.7%, 98.0%), ce_loss 1.036, lat_loss 22.071
09/28 05:42:15 AM | Train: [ 30/180] Step 1150/1249 Loss 6.672 Prec@(1,3) (74.7%, 98.0%), ce_loss 1.036, lat_loss 22.071
09/28 05:42:40 AM | Train: [ 30/180] Step 1200/1249 Loss 6.661 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.036, lat_loss 22.071
09/28 05:43:04 AM | Train: [ 30/180] Step 1249/1249 Loss 6.651 Prec@(1,3) (74.8%, 98.0%), ce_loss 1.035, lat_loss 22.071
09/28 05:43:04 AM | _w_step_train: [ 30/180] Final Prec@1 74.7850% Time 394.13
09/28 05:43:04 AM | Start to train theta for epoch 29
09/28 05:43:25 AM | Train: [ 30/180] Step 050/312 Loss 7.093 Prec@(1,3) (73.9%, 96.6%), ce_loss 1.035, lat_loss 22.070
09/28 05:43:45 AM | Train: [ 30/180] Step 100/312 Loss 6.978 Prec@(1,3) (74.2%, 96.9%), ce_loss 1.035, lat_loss 22.070
09/28 05:44:05 AM | Train: [ 30/180] Step 150/312 Loss 6.981 Prec@(1,3) (73.9%, 97.1%), ce_loss 1.035, lat_loss 22.070
09/28 05:44:26 AM | Train: [ 30/180] Step 200/312 Loss 7.037 Prec@(1,3) (73.5%, 97.2%), ce_loss 1.034, lat_loss 22.070
09/28 05:44:46 AM | Train: [ 30/180] Step 250/312 Loss 7.030 Prec@(1,3) (73.5%, 97.3%), ce_loss 1.034, lat_loss 22.070
09/28 05:45:07 AM | Train: [ 30/180] Step 300/312 Loss 7.055 Prec@(1,3) (73.3%, 97.3%), ce_loss 1.034, lat_loss 22.069
09/28 05:45:12 AM | Train: [ 30/180] Step 312/312 Loss 7.036 Prec@(1,3) (73.4%, 97.3%), ce_loss 1.034, lat_loss 22.069
09/28 05:45:12 AM | _theta_step_train: [ 30/180] Final Prec@1 73.4300% Time 128.15
09/28 05:45:18 AM | Valid: [ 30/180] Step 050/312 Loss 7.753 Prec@(1,3) (70.0%, 96.9%), ce_loss 1.034, lat_loss 22.069
09/28 05:45:22 AM | Valid: [ 30/180] Step 100/312 Loss 7.862 Prec@(1,3) (70.0%, 96.8%), ce_loss 1.034, lat_loss 22.069
09/28 05:45:27 AM | Valid: [ 30/180] Step 150/312 Loss 7.954 Prec@(1,3) (70.2%, 96.6%), ce_loss 1.034, lat_loss 22.068
09/28 05:45:32 AM | Valid: [ 30/180] Step 200/312 Loss 7.914 Prec@(1,3) (70.5%, 96.6%), ce_loss 1.033, lat_loss 22.068
09/28 05:45:36 AM | Valid: [ 30/180] Step 250/312 Loss 8.232 Prec@(1,3) (69.9%, 96.5%), ce_loss 1.033, lat_loss 22.068
09/28 05:45:41 AM | Valid: [ 30/180] Step 300/312 Loss 8.225 Prec@(1,3) (69.8%, 96.5%), ce_loss 1.033, lat_loss 22.068
09/28 05:45:42 AM | Valid: [ 30/180] Step 312/312 Loss 8.188 Prec@(1,3) (70.0%, 96.6%), ce_loss 1.033, lat_loss 22.068
09/28 05:45:42 AM | val: [ 30/180] Final Prec@1 69.9800% Time 29.52
09/28 05:45:42 AM | Start to train weights for epoch 30
09/28 05:46:06 AM | Train: [ 31/180] Step 050/1249 Loss 6.694 Prec@(1,3) (74.1%, 97.8%), ce_loss 1.033, lat_loss 22.067
09/28 05:46:28 AM | Train: [ 31/180] Step 100/1249 Loss 6.902 Prec@(1,3) (73.5%, 97.5%), ce_loss 1.033, lat_loss 22.067
09/28 05:46:49 AM | Train: [ 31/180] Step 150/1249 Loss 6.603 Prec@(1,3) (74.8%, 97.6%), ce_loss 1.032, lat_loss 22.067
09/28 05:47:10 AM | Train: [ 31/180] Step 200/1249 Loss 6.632 Prec@(1,3) (74.7%, 97.8%), ce_loss 1.032, lat_loss 22.067
09/28 05:47:33 AM | Train: [ 31/180] Step 250/1249 Loss 6.640 Prec@(1,3) (74.7%, 97.9%), ce_loss 1.032, lat_loss 22.066
09/28 05:47:55 AM | Train: [ 31/180] Step 300/1249 Loss 6.587 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.032, lat_loss 22.066
09/28 05:48:17 AM | Train: [ 31/180] Step 350/1249 Loss 6.513 Prec@(1,3) (75.1%, 98.0%), ce_loss 1.031, lat_loss 22.066
09/28 05:48:39 AM | Train: [ 31/180] Step 400/1249 Loss 6.529 Prec@(1,3) (75.1%, 97.9%), ce_loss 1.031, lat_loss 22.066
09/28 05:49:00 AM | Train: [ 31/180] Step 450/1249 Loss 6.517 Prec@(1,3) (75.1%, 97.9%), ce_loss 1.031, lat_loss 22.065
09/28 05:49:22 AM | Train: [ 31/180] Step 500/1249 Loss 6.547 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.031, lat_loss 22.065
09/28 05:49:46 AM | Train: [ 31/180] Step 550/1249 Loss 6.560 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.030, lat_loss 22.065
09/28 05:50:09 AM | Train: [ 31/180] Step 600/1249 Loss 6.565 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.030, lat_loss 22.065
09/28 05:50:34 AM | Train: [ 31/180] Step 650/1249 Loss 6.538 Prec@(1,3) (75.1%, 97.9%), ce_loss 1.030, lat_loss 22.064
09/28 05:50:58 AM | Train: [ 31/180] Step 700/1249 Loss 6.571 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.029, lat_loss 22.064
09/28 05:51:21 AM | Train: [ 31/180] Step 750/1249 Loss 6.573 Prec@(1,3) (74.9%, 97.9%), ce_loss 1.029, lat_loss 22.064
09/28 05:51:42 AM | Train: [ 31/180] Step 800/1249 Loss 6.558 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.029, lat_loss 22.064
09/28 05:52:07 AM | Train: [ 31/180] Step 850/1249 Loss 6.552 Prec@(1,3) (75.0%, 97.9%), ce_loss 1.029, lat_loss 22.063
09/28 05:52:32 AM | Train: [ 31/180] Step 900/1249 Loss 6.523 Prec@(1,3) (75.1%, 98.0%), ce_loss 1.028, lat_loss 22.063
09/28 05:52:57 AM | Train: [ 31/180] Step 950/1249 Loss 6.495 Prec@(1,3) (75.2%, 98.0%), ce_loss 1.028, lat_loss 22.063
09/28 05:53:22 AM | Train: [ 31/180] Step 1000/1249 Loss 6.502 Prec@(1,3) (75.2%, 97.9%), ce_loss 1.028, lat_loss 22.063
09/28 05:53:47 AM | Train: [ 31/180] Step 1050/1249 Loss 6.489 Prec@(1,3) (75.2%, 97.9%), ce_loss 1.027, lat_loss 22.062
09/28 05:54:11 AM | Train: [ 31/180] Step 1100/1249 Loss 6.488 Prec@(1,3) (75.3%, 98.0%), ce_loss 1.027, lat_loss 22.062
09/28 05:54:33 AM | Train: [ 31/180] Step 1150/1249 Loss 6.480 Prec@(1,3) (75.3%, 98.0%), ce_loss 1.027, lat_loss 22.062
09/28 05:54:54 AM | Train: [ 31/180] Step 1200/1249 Loss 6.502 Prec@(1,3) (75.2%, 98.0%), ce_loss 1.027, lat_loss 22.062
09/28 05:55:16 AM | Train: [ 31/180] Step 1249/1249 Loss 6.497 Prec@(1,3) (75.2%, 98.0%), ce_loss 1.026, lat_loss 22.061
09/28 05:55:16 AM | _w_step_train: [ 31/180] Final Prec@1 75.2275% Time 574.41
09/28 05:55:16 AM | Start to train theta for epoch 30
09/28 05:55:36 AM | Train: [ 31/180] Step 050/312 Loss 7.183 Prec@(1,3) (72.8%, 97.4%), ce_loss 1.026, lat_loss 22.061
09/28 05:55:56 AM | Train: [ 31/180] Step 100/312 Loss 6.718 Prec@(1,3) (74.1%, 97.9%), ce_loss 1.026, lat_loss 22.061
09/28 05:56:17 AM | Train: [ 31/180] Step 150/312 Loss 6.607 Prec@(1,3) (74.4%, 98.1%), ce_loss 1.026, lat_loss 22.060
09/28 05:56:37 AM | Train: [ 31/180] Step 200/312 Loss 6.681 Prec@(1,3) (74.5%, 97.9%), ce_loss 1.025, lat_loss 22.060
09/28 05:56:58 AM | Train: [ 31/180] Step 250/312 Loss 6.613 Prec@(1,3) (74.5%, 98.0%), ce_loss 1.025, lat_loss 22.060
09/28 05:57:19 AM | Train: [ 31/180] Step 300/312 Loss 6.601 Prec@(1,3) (74.4%, 98.0%), ce_loss 1.025, lat_loss 22.060
09/28 05:57:24 AM | Train: [ 31/180] Step 312/312 Loss 6.589 Prec@(1,3) (74.5%, 98.0%), ce_loss 1.025, lat_loss 22.060
09/28 05:57:24 AM | _theta_step_train: [ 31/180] Final Prec@1 74.4600% Time 127.57
09/28 05:57:29 AM | Valid: [ 31/180] Step 050/312 Loss 7.668 Prec@(1,3) (71.1%, 97.3%), ce_loss 1.025, lat_loss 22.059
09/28 05:57:33 AM | Valid: [ 31/180] Step 100/312 Loss 8.365 Prec@(1,3) (69.7%, 96.7%), ce_loss 1.025, lat_loss 22.059
09/28 05:57:37 AM | Valid: [ 31/180] Step 150/312 Loss 8.105 Prec@(1,3) (70.3%, 96.8%), ce_loss 1.024, lat_loss 22.059
09/28 05:57:41 AM | Valid: [ 31/180] Step 200/312 Loss 8.387 Prec@(1,3) (70.0%, 96.8%), ce_loss 1.024, lat_loss 22.058
09/28 05:57:46 AM | Valid: [ 31/180] Step 250/312 Loss 8.264 Prec@(1,3) (69.9%, 96.9%), ce_loss 1.024, lat_loss 22.058
09/28 05:57:50 AM | Valid: [ 31/180] Step 300/312 Loss 8.017 Prec@(1,3) (70.5%, 97.2%), ce_loss 1.024, lat_loss 22.058
09/28 05:57:51 AM | Valid: [ 31/180] Step 312/312 Loss 8.033 Prec@(1,3) (70.4%, 97.2%), ce_loss 1.024, lat_loss 22.058
09/28 05:57:51 AM | val: [ 31/180] Final Prec@1 70.4300% Time 26.76
09/28 05:57:51 AM | Start to train weights for epoch 31
09/28 05:58:17 AM | Train: [ 32/180] Step 050/1249 Loss 6.784 Prec@(1,3) (74.0%, 98.0%), ce_loss 1.024, lat_loss 22.058
09/28 05:58:42 AM | Train: [ 32/180] Step 100/1249 Loss 6.704 Prec@(1,3) (74.5%, 98.1%), ce_loss 1.024, lat_loss 22.057
09/28 05:59:06 AM | Train: [ 32/180] Step 150/1249 Loss 6.699 Prec@(1,3) (74.8%, 98.1%), ce_loss 1.023, lat_loss 22.057
09/28 05:59:31 AM | Train: [ 32/180] Step 200/1249 Loss 6.597 Prec@(1,3) (75.1%, 98.1%), ce_loss 1.023, lat_loss 22.057
09/28 05:59:55 AM | Train: [ 32/180] Step 250/1249 Loss 6.480 Prec@(1,3) (75.5%, 98.3%), ce_loss 1.023, lat_loss 22.056
09/28 06:00:20 AM | Train: [ 32/180] Step 300/1249 Loss 6.557 Prec@(1,3) (75.3%, 98.2%), ce_loss 1.022, lat_loss 22.056
09/28 06:00:44 AM | Train: [ 32/180] Step 350/1249 Loss 6.510 Prec@(1,3) (75.5%, 98.2%), ce_loss 1.022, lat_loss 22.056
09/28 06:01:08 AM | Train: [ 32/180] Step 400/1249 Loss 6.557 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.022, lat_loss 22.056
09/28 06:01:33 AM | Train: [ 32/180] Step 450/1249 Loss 6.560 Prec@(1,3) (75.1%, 98.2%), ce_loss 1.022, lat_loss 22.055
09/28 06:01:55 AM | Train: [ 32/180] Step 500/1249 Loss 6.510 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.021, lat_loss 22.055
09/28 06:02:17 AM | Train: [ 32/180] Step 550/1249 Loss 6.530 Prec@(1,3) (75.1%, 98.2%), ce_loss 1.021, lat_loss 22.055
09/28 06:02:42 AM | Train: [ 32/180] Step 600/1249 Loss 6.525 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.021, lat_loss 22.055
09/28 06:03:06 AM | Train: [ 32/180] Step 650/1249 Loss 6.529 Prec@(1,3) (75.1%, 98.2%), ce_loss 1.021, lat_loss 22.054
09/28 06:03:31 AM | Train: [ 32/180] Step 700/1249 Loss 6.546 Prec@(1,3) (75.1%, 98.1%), ce_loss 1.020, lat_loss 22.054
09/28 06:03:55 AM | Train: [ 32/180] Step 750/1249 Loss 6.538 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.020, lat_loss 22.054
09/28 06:04:18 AM | Train: [ 32/180] Step 800/1249 Loss 6.521 Prec@(1,3) (75.1%, 98.1%), ce_loss 1.020, lat_loss 22.053
09/28 06:04:41 AM | Train: [ 32/180] Step 850/1249 Loss 6.545 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.020, lat_loss 22.053
09/28 06:05:03 AM | Train: [ 32/180] Step 900/1249 Loss 6.548 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.019, lat_loss 22.053
09/28 06:05:26 AM | Train: [ 32/180] Step 950/1249 Loss 6.542 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.019, lat_loss 22.053
09/28 06:05:48 AM | Train: [ 32/180] Step 1000/1249 Loss 6.547 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.019, lat_loss 22.052
09/28 06:06:09 AM | Train: [ 32/180] Step 1050/1249 Loss 6.549 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.019, lat_loss 22.052
09/28 06:06:31 AM | Train: [ 32/180] Step 1100/1249 Loss 6.545 Prec@(1,3) (75.0%, 98.0%), ce_loss 1.018, lat_loss 22.052
09/28 06:06:53 AM | Train: [ 32/180] Step 1150/1249 Loss 6.551 Prec@(1,3) (75.0%, 98.0%), ce_loss 1.018, lat_loss 22.052
09/28 06:07:14 AM | Train: [ 32/180] Step 1200/1249 Loss 6.540 Prec@(1,3) (75.0%, 98.1%), ce_loss 1.018, lat_loss 22.051
09/28 06:07:39 AM | Train: [ 32/180] Step 1249/1249 Loss 6.526 Prec@(1,3) (75.1%, 98.1%), ce_loss 1.018, lat_loss 22.051
09/28 06:07:39 AM | _w_step_train: [ 32/180] Final Prec@1 75.1225% Time 587.97
09/28 06:07:39 AM | Start to train theta for epoch 31
09/28 06:08:01 AM | Train: [ 32/180] Step 050/312 Loss 6.844 Prec@(1,3) (73.5%, 97.9%), ce_loss 1.017, lat_loss 22.051
09/28 06:08:21 AM | Train: [ 32/180] Step 100/312 Loss 6.746 Prec@(1,3) (74.1%, 98.0%), ce_loss 1.017, lat_loss 22.050
09/28 06:08:42 AM | Train: [ 32/180] Step 150/312 Loss 6.603 Prec@(1,3) (75.0%, 98.0%), ce_loss 1.017, lat_loss 22.050
09/28 06:09:02 AM | Train: [ 32/180] Step 200/312 Loss 6.460 Prec@(1,3) (75.5%, 98.1%), ce_loss 1.017, lat_loss 22.050
09/28 06:09:22 AM | Train: [ 32/180] Step 250/312 Loss 6.487 Prec@(1,3) (75.6%, 98.1%), ce_loss 1.016, lat_loss 22.050
09/28 06:09:42 AM | Train: [ 32/180] Step 300/312 Loss 6.438 Prec@(1,3) (75.9%, 98.1%), ce_loss 1.016, lat_loss 22.049
09/28 06:09:47 AM | Train: [ 32/180] Step 312/312 Loss 6.452 Prec@(1,3) (75.8%, 98.1%), ce_loss 1.016, lat_loss 22.049
09/28 06:09:47 AM | _theta_step_train: [ 32/180] Final Prec@1 75.7500% Time 128.41
09/28 06:09:53 AM | Valid: [ 32/180] Step 050/312 Loss 8.256 Prec@(1,3) (68.8%, 97.4%), ce_loss 1.016, lat_loss 22.049
09/28 06:09:57 AM | Valid: [ 32/180] Step 100/312 Loss 8.320 Prec@(1,3) (68.9%, 97.2%), ce_loss 1.016, lat_loss 22.049
09/28 06:10:02 AM | Valid: [ 32/180] Step 150/312 Loss 8.270 Prec@(1,3) (68.9%, 97.3%), ce_loss 1.016, lat_loss 22.048
09/28 06:10:06 AM | Valid: [ 32/180] Step 200/312 Loss 8.320 Prec@(1,3) (68.8%, 97.3%), ce_loss 1.016, lat_loss 22.048
09/28 06:10:11 AM | Valid: [ 32/180] Step 250/312 Loss 8.264 Prec@(1,3) (69.6%, 97.3%), ce_loss 1.016, lat_loss 22.048
09/28 06:10:16 AM | Valid: [ 32/180] Step 300/312 Loss 8.253 Prec@(1,3) (69.8%, 97.1%), ce_loss 1.016, lat_loss 22.047
09/28 06:10:17 AM | Valid: [ 32/180] Step 312/312 Loss 8.240 Prec@(1,3) (69.7%, 97.2%), ce_loss 1.016, lat_loss 22.047
09/28 06:10:17 AM | val: [ 32/180] Final Prec@1 69.7200% Time 29.67
09/28 06:10:17 AM | Start to train weights for epoch 32
09/28 06:10:42 AM | Train: [ 33/180] Step 050/1249 Loss 6.384 Prec@(1,3) (76.4%, 98.3%), ce_loss 1.015, lat_loss 22.047
09/28 06:11:07 AM | Train: [ 33/180] Step 100/1249 Loss 6.153 Prec@(1,3) (77.0%, 98.3%), ce_loss 1.015, lat_loss 22.047
09/28 06:11:31 AM | Train: [ 33/180] Step 150/1249 Loss 6.346 Prec@(1,3) (76.1%, 98.2%), ce_loss 1.015, lat_loss 22.047
09/28 06:11:55 AM | Train: [ 33/180] Step 200/1249 Loss 6.502 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.015, lat_loss 22.046
09/28 06:12:19 AM | Train: [ 33/180] Step 250/1249 Loss 6.486 Prec@(1,3) (75.4%, 98.2%), ce_loss 1.014, lat_loss 22.046
09/28 06:12:43 AM | Train: [ 33/180] Step 300/1249 Loss 6.531 Prec@(1,3) (75.1%, 98.2%), ce_loss 1.014, lat_loss 22.046
09/28 06:13:07 AM | Train: [ 33/180] Step 350/1249 Loss 6.523 Prec@(1,3) (75.0%, 98.2%), ce_loss 1.014, lat_loss 22.045
09/28 06:13:32 AM | Train: [ 33/180] Step 400/1249 Loss 6.483 Prec@(1,3) (75.2%, 98.3%), ce_loss 1.014, lat_loss 22.045
09/28 06:13:56 AM | Train: [ 33/180] Step 450/1249 Loss 6.469 Prec@(1,3) (75.4%, 98.2%), ce_loss 1.013, lat_loss 22.045
09/28 06:14:21 AM | Train: [ 33/180] Step 500/1249 Loss 6.534 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.013, lat_loss 22.044
09/28 06:14:46 AM | Train: [ 33/180] Step 550/1249 Loss 6.528 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.013, lat_loss 22.044
09/28 06:15:11 AM | Train: [ 33/180] Step 600/1249 Loss 6.532 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.013, lat_loss 22.044
09/28 06:15:36 AM | Train: [ 33/180] Step 650/1249 Loss 6.554 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.012, lat_loss 22.044
09/28 06:16:01 AM | Train: [ 33/180] Step 700/1249 Loss 6.537 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.012, lat_loss 22.043
09/28 06:16:26 AM | Train: [ 33/180] Step 750/1249 Loss 6.516 Prec@(1,3) (75.2%, 98.1%), ce_loss 1.012, lat_loss 22.043
09/28 06:16:51 AM | Train: [ 33/180] Step 800/1249 Loss 6.488 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.012, lat_loss 22.043
09/28 06:17:16 AM | Train: [ 33/180] Step 850/1249 Loss 6.459 Prec@(1,3) (75.4%, 98.1%), ce_loss 1.011, lat_loss 22.042
09/28 06:17:40 AM | Train: [ 33/180] Step 900/1249 Loss 6.474 Prec@(1,3) (75.4%, 98.1%), ce_loss 1.011, lat_loss 22.042
09/28 06:18:05 AM | Train: [ 33/180] Step 950/1249 Loss 6.493 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.011, lat_loss 22.042
09/28 06:18:29 AM | Train: [ 33/180] Step 1000/1249 Loss 6.457 Prec@(1,3) (75.5%, 98.1%), ce_loss 1.011, lat_loss 22.041
09/28 06:18:54 AM | Train: [ 33/180] Step 1050/1249 Loss 6.487 Prec@(1,3) (75.4%, 98.1%), ce_loss 1.010, lat_loss 22.041
09/28 06:19:19 AM | Train: [ 33/180] Step 1100/1249 Loss 6.501 Prec@(1,3) (75.4%, 98.0%), ce_loss 1.010, lat_loss 22.041
09/28 06:19:44 AM | Train: [ 33/180] Step 1150/1249 Loss 6.504 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.010, lat_loss 22.041
09/28 06:20:07 AM | Train: [ 33/180] Step 1200/1249 Loss 6.503 Prec@(1,3) (75.4%, 98.0%), ce_loss 1.010, lat_loss 22.040
09/28 06:20:31 AM | Train: [ 33/180] Step 1249/1249 Loss 6.483 Prec@(1,3) (75.4%, 98.0%), ce_loss 1.009, lat_loss 22.040
09/28 06:20:31 AM | _w_step_train: [ 33/180] Final Prec@1 75.4275% Time 614.41
09/28 06:20:31 AM | Start to train theta for epoch 32
09/28 06:20:53 AM | Train: [ 33/180] Step 050/312 Loss 5.943 Prec@(1,3) (77.3%, 98.5%), ce_loss 1.009, lat_loss 22.040
09/28 06:21:14 AM | Train: [ 33/180] Step 100/312 Loss 6.295 Prec@(1,3) (76.0%, 98.1%), ce_loss 1.009, lat_loss 22.039
09/28 06:21:35 AM | Train: [ 33/180] Step 150/312 Loss 6.350 Prec@(1,3) (75.4%, 98.2%), ce_loss 1.009, lat_loss 22.039
09/28 06:21:55 AM | Train: [ 33/180] Step 200/312 Loss 6.384 Prec@(1,3) (75.5%, 98.1%), ce_loss 1.008, lat_loss 22.039
09/28 06:22:15 AM | Train: [ 33/180] Step 250/312 Loss 6.398 Prec@(1,3) (75.7%, 98.0%), ce_loss 1.008, lat_loss 22.039
09/28 06:22:33 AM | Train: [ 33/180] Step 300/312 Loss 6.367 Prec@(1,3) (75.9%, 97.9%), ce_loss 1.008, lat_loss 22.038
09/28 06:22:38 AM | Train: [ 33/180] Step 312/312 Loss 6.343 Prec@(1,3) (76.0%, 97.9%), ce_loss 1.008, lat_loss 22.038
09/28 06:22:38 AM | _theta_step_train: [ 33/180] Final Prec@1 75.9600% Time 126.57
09/28 06:22:43 AM | Valid: [ 33/180] Step 050/312 Loss 7.775 Prec@(1,3) (71.4%, 97.1%), ce_loss 1.008, lat_loss 22.038
09/28 06:22:48 AM | Valid: [ 33/180] Step 100/312 Loss 8.013 Prec@(1,3) (70.7%, 96.7%), ce_loss 1.008, lat_loss 22.038
09/28 06:22:52 AM | Valid: [ 33/180] Step 150/312 Loss 8.067 Prec@(1,3) (70.3%, 96.8%), ce_loss 1.008, lat_loss 22.037
09/28 06:22:57 AM | Valid: [ 33/180] Step 200/312 Loss 8.258 Prec@(1,3) (69.5%, 96.7%), ce_loss 1.008, lat_loss 22.037
09/28 06:23:02 AM | Valid: [ 33/180] Step 250/312 Loss 8.118 Prec@(1,3) (69.8%, 96.7%), ce_loss 1.008, lat_loss 22.037
09/28 06:23:06 AM | Valid: [ 33/180] Step 300/312 Loss 8.068 Prec@(1,3) (70.0%, 96.8%), ce_loss 1.007, lat_loss 22.036
09/28 06:23:07 AM | Valid: [ 33/180] Step 312/312 Loss 8.041 Prec@(1,3) (70.0%, 96.9%), ce_loss 1.007, lat_loss 22.036
09/28 06:23:07 AM | val: [ 33/180] Final Prec@1 70.0300% Time 29.42
09/28 06:23:07 AM | Start to train weights for epoch 33
09/28 06:23:33 AM | Train: [ 34/180] Step 050/1249 Loss 5.899 Prec@(1,3) (78.1%, 98.4%), ce_loss 1.007, lat_loss 22.036
09/28 06:23:57 AM | Train: [ 34/180] Step 100/1249 Loss 5.941 Prec@(1,3) (77.7%, 98.5%), ce_loss 1.007, lat_loss 22.036
09/28 06:24:20 AM | Train: [ 34/180] Step 150/1249 Loss 6.275 Prec@(1,3) (76.3%, 98.2%), ce_loss 1.007, lat_loss 22.035
09/28 06:24:43 AM | Train: [ 34/180] Step 200/1249 Loss 6.233 Prec@(1,3) (76.4%, 98.2%), ce_loss 1.006, lat_loss 22.035
09/28 06:25:07 AM | Train: [ 34/180] Step 250/1249 Loss 6.284 Prec@(1,3) (76.0%, 98.2%), ce_loss 1.006, lat_loss 22.035
09/28 06:25:31 AM | Train: [ 34/180] Step 300/1249 Loss 6.273 Prec@(1,3) (75.9%, 98.2%), ce_loss 1.006, lat_loss 22.035
09/28 06:25:55 AM | Train: [ 34/180] Step 350/1249 Loss 6.298 Prec@(1,3) (75.9%, 98.1%), ce_loss 1.006, lat_loss 22.034
09/28 06:26:19 AM | Train: [ 34/180] Step 400/1249 Loss 6.267 Prec@(1,3) (76.0%, 98.1%), ce_loss 1.005, lat_loss 22.034
09/28 06:26:44 AM | Train: [ 34/180] Step 450/1249 Loss 6.286 Prec@(1,3) (75.9%, 98.1%), ce_loss 1.005, lat_loss 22.034
09/28 06:27:09 AM | Train: [ 34/180] Step 500/1249 Loss 6.244 Prec@(1,3) (76.0%, 98.1%), ce_loss 1.005, lat_loss 22.033
09/28 06:27:33 AM | Train: [ 34/180] Step 550/1249 Loss 6.291 Prec@(1,3) (75.9%, 98.0%), ce_loss 1.005, lat_loss 22.033
09/28 06:27:58 AM | Train: [ 34/180] Step 600/1249 Loss 6.342 Prec@(1,3) (75.7%, 98.1%), ce_loss 1.004, lat_loss 22.033
09/28 06:28:22 AM | Train: [ 34/180] Step 650/1249 Loss 6.340 Prec@(1,3) (75.7%, 98.0%), ce_loss 1.004, lat_loss 22.032
09/28 06:28:46 AM | Train: [ 34/180] Step 700/1249 Loss 6.323 Prec@(1,3) (75.8%, 98.1%), ce_loss 1.004, lat_loss 22.032
09/28 06:29:10 AM | Train: [ 34/180] Step 750/1249 Loss 6.325 Prec@(1,3) (75.8%, 98.1%), ce_loss 1.004, lat_loss 22.032
09/28 06:29:35 AM | Train: [ 34/180] Step 800/1249 Loss 6.343 Prec@(1,3) (75.7%, 98.1%), ce_loss 1.003, lat_loss 22.032
09/28 06:29:57 AM | Train: [ 34/180] Step 850/1249 Loss 6.357 Prec@(1,3) (75.7%, 98.0%), ce_loss 1.003, lat_loss 22.031
09/28 06:30:21 AM | Train: [ 34/180] Step 900/1249 Loss 6.378 Prec@(1,3) (75.6%, 98.0%), ce_loss 1.003, lat_loss 22.031
09/28 06:30:45 AM | Train: [ 34/180] Step 950/1249 Loss 6.376 Prec@(1,3) (75.6%, 98.0%), ce_loss 1.003, lat_loss 22.031
09/28 06:31:10 AM | Train: [ 34/180] Step 1000/1249 Loss 6.377 Prec@(1,3) (75.6%, 98.0%), ce_loss 1.003, lat_loss 22.030
09/28 06:31:34 AM | Train: [ 34/180] Step 1050/1249 Loss 6.379 Prec@(1,3) (75.6%, 98.0%), ce_loss 1.002, lat_loss 22.030
09/28 06:31:58 AM | Train: [ 34/180] Step 1100/1249 Loss 6.374 Prec@(1,3) (75.6%, 98.0%), ce_loss 1.002, lat_loss 22.030
09/28 06:32:23 AM | Train: [ 34/180] Step 1150/1249 Loss 6.358 Prec@(1,3) (75.7%, 98.0%), ce_loss 1.002, lat_loss 22.030
09/28 06:32:48 AM | Train: [ 34/180] Step 1200/1249 Loss 6.356 Prec@(1,3) (75.7%, 98.0%), ce_loss 1.002, lat_loss 22.029
09/28 06:33:12 AM | Train: [ 34/180] Step 1249/1249 Loss 6.350 Prec@(1,3) (75.7%, 98.0%), ce_loss 1.001, lat_loss 22.029
09/28 06:33:12 AM | _w_step_train: [ 34/180] Final Prec@1 75.7350% Time 605.03
09/28 06:33:12 AM | Start to train theta for epoch 33
09/28 06:33:34 AM | Train: [ 34/180] Step 050/312 Loss 6.704 Prec@(1,3) (74.8%, 97.9%), ce_loss 1.001, lat_loss 22.029
09/28 06:33:54 AM | Train: [ 34/180] Step 100/312 Loss 6.385 Prec@(1,3) (75.7%, 98.2%), ce_loss 1.001, lat_loss 22.028
09/28 06:34:15 AM | Train: [ 34/180] Step 150/312 Loss 6.436 Prec@(1,3) (75.3%, 98.1%), ce_loss 1.001, lat_loss 22.028
09/28 06:34:35 AM | Train: [ 34/180] Step 200/312 Loss 6.316 Prec@(1,3) (76.0%, 98.0%), ce_loss 1.000, lat_loss 22.028
09/28 06:34:55 AM | Train: [ 34/180] Step 250/312 Loss 6.341 Prec@(1,3) (75.9%, 97.8%), ce_loss 1.000, lat_loss 22.028
09/28 06:35:16 AM | Train: [ 34/180] Step 300/312 Loss 6.327 Prec@(1,3) (75.9%, 97.8%), ce_loss 1.000, lat_loss 22.027
09/28 06:35:21 AM | Train: [ 34/180] Step 312/312 Loss 6.338 Prec@(1,3) (75.8%, 97.8%), ce_loss 1.000, lat_loss 22.027
09/28 06:35:21 AM | _theta_step_train: [ 34/180] Final Prec@1 75.7700% Time 128.54
09/28 06:35:26 AM | Valid: [ 34/180] Step 050/312 Loss 7.980 Prec@(1,3) (71.9%, 97.0%), ce_loss 1.000, lat_loss 22.027
09/28 06:35:31 AM | Valid: [ 34/180] Step 100/312 Loss 8.474 Prec@(1,3) (71.5%, 96.8%), ce_loss 1.000, lat_loss 22.027
09/28 06:35:36 AM | Valid: [ 34/180] Step 150/312 Loss 8.157 Prec@(1,3) (72.2%, 97.0%), ce_loss 1.000, lat_loss 22.027
09/28 06:35:40 AM | Valid: [ 34/180] Step 200/312 Loss 8.160 Prec@(1,3) (72.0%, 97.1%), ce_loss 1.000, lat_loss 22.026
09/28 06:35:45 AM | Valid: [ 34/180] Step 250/312 Loss 8.099 Prec@(1,3) (72.2%, 97.2%), ce_loss 1.000, lat_loss 22.026
09/28 06:35:49 AM | Valid: [ 34/180] Step 300/312 Loss 8.202 Prec@(1,3) (72.0%, 97.1%), ce_loss 1.000, lat_loss 22.026
09/28 06:35:50 AM | Valid: [ 34/180] Step 312/312 Loss 8.160 Prec@(1,3) (72.0%, 97.2%), ce_loss 1.000, lat_loss 22.026
09/28 06:35:50 AM | val: [ 34/180] Final Prec@1 72.0200% Time 29.48
09/28 06:35:50 AM | Start to train weights for epoch 34
09/28 06:36:15 AM | Train: [ 35/180] Step 050/1249 Loss 6.245 Prec@(1,3) (76.8%, 98.0%), ce_loss 0.999, lat_loss 22.026
09/28 06:36:40 AM | Train: [ 35/180] Step 100/1249 Loss 6.300 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.999, lat_loss 22.026
09/28 06:37:05 AM | Train: [ 35/180] Step 150/1249 Loss 6.258 Prec@(1,3) (76.1%, 98.4%), ce_loss 0.999, lat_loss 22.025
09/28 06:37:30 AM | Train: [ 35/180] Step 200/1249 Loss 6.242 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.999, lat_loss 22.025
09/28 06:37:56 AM | Train: [ 35/180] Step 250/1249 Loss 6.270 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.998, lat_loss 22.025
09/28 06:38:21 AM | Train: [ 35/180] Step 300/1249 Loss 6.175 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.998, lat_loss 22.025
09/28 06:38:45 AM | Train: [ 35/180] Step 350/1249 Loss 6.217 Prec@(1,3) (76.4%, 98.1%), ce_loss 0.998, lat_loss 22.024
09/28 06:39:10 AM | Train: [ 35/180] Step 400/1249 Loss 6.190 Prec@(1,3) (76.5%, 98.1%), ce_loss 0.998, lat_loss 22.024
09/28 06:39:35 AM | Train: [ 35/180] Step 450/1249 Loss 6.275 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.997, lat_loss 22.024
09/28 06:39:59 AM | Train: [ 35/180] Step 500/1249 Loss 6.324 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.997, lat_loss 22.024
09/28 06:40:22 AM | Train: [ 35/180] Step 550/1249 Loss 6.356 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.997, lat_loss 22.023
09/28 06:40:47 AM | Train: [ 35/180] Step 600/1249 Loss 6.354 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.997, lat_loss 22.023
09/28 06:41:10 AM | Train: [ 35/180] Step 650/1249 Loss 6.344 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.996, lat_loss 22.023
09/28 06:41:34 AM | Train: [ 35/180] Step 700/1249 Loss 6.362 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.996, lat_loss 22.023
09/28 06:41:58 AM | Train: [ 35/180] Step 750/1249 Loss 6.384 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.996, lat_loss 22.022
09/28 06:42:20 AM | Train: [ 35/180] Step 800/1249 Loss 6.356 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.996, lat_loss 22.022
09/28 06:42:44 AM | Train: [ 35/180] Step 850/1249 Loss 6.349 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.996, lat_loss 22.022
09/28 06:43:07 AM | Train: [ 35/180] Step 900/1249 Loss 6.355 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.995, lat_loss 22.022
09/28 06:43:32 AM | Train: [ 35/180] Step 950/1249 Loss 6.335 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.995, lat_loss 22.021
09/28 06:43:54 AM | Train: [ 35/180] Step 1000/1249 Loss 6.320 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.995, lat_loss 22.021
09/28 06:44:18 AM | Train: [ 35/180] Step 1050/1249 Loss 6.339 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.995, lat_loss 22.021
09/28 06:44:42 AM | Train: [ 35/180] Step 1100/1249 Loss 6.333 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.994, lat_loss 22.021
09/28 06:45:07 AM | Train: [ 35/180] Step 1150/1249 Loss 6.323 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.994, lat_loss 22.020
09/28 06:45:32 AM | Train: [ 35/180] Step 1200/1249 Loss 6.317 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.994, lat_loss 22.020
09/28 06:45:56 AM | Train: [ 35/180] Step 1249/1249 Loss 6.319 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.994, lat_loss 22.020
09/28 06:45:57 AM | _w_step_train: [ 35/180] Final Prec@1 75.8350% Time 606.19
09/28 06:45:57 AM | Start to train theta for epoch 34
09/28 06:46:18 AM | Train: [ 35/180] Step 050/312 Loss 6.900 Prec@(1,3) (74.9%, 97.1%), ce_loss 0.994, lat_loss 22.020
09/28 06:46:38 AM | Train: [ 35/180] Step 100/312 Loss 6.792 Prec@(1,3) (74.3%, 97.3%), ce_loss 0.993, lat_loss 22.020
09/28 06:46:59 AM | Train: [ 35/180] Step 150/312 Loss 6.725 Prec@(1,3) (74.5%, 97.4%), ce_loss 0.993, lat_loss 22.019
09/28 06:47:18 AM | Train: [ 35/180] Step 200/312 Loss 6.599 Prec@(1,3) (74.7%, 97.7%), ce_loss 0.993, lat_loss 22.019
09/28 06:47:39 AM | Train: [ 35/180] Step 250/312 Loss 6.566 Prec@(1,3) (74.9%, 97.8%), ce_loss 0.993, lat_loss 22.019
09/28 06:47:59 AM | Train: [ 35/180] Step 300/312 Loss 6.551 Prec@(1,3) (75.1%, 97.8%), ce_loss 0.992, lat_loss 22.019
09/28 06:48:04 AM | Train: [ 35/180] Step 312/312 Loss 6.543 Prec@(1,3) (75.1%, 97.8%), ce_loss 0.992, lat_loss 22.018
09/28 06:48:05 AM | _theta_step_train: [ 35/180] Final Prec@1 75.1500% Time 128.03
09/28 06:48:10 AM | Valid: [ 35/180] Step 050/312 Loss 7.876 Prec@(1,3) (68.9%, 97.9%), ce_loss 0.992, lat_loss 22.018
09/28 06:48:15 AM | Valid: [ 35/180] Step 100/312 Loss 7.927 Prec@(1,3) (69.1%, 97.5%), ce_loss 0.992, lat_loss 22.018
09/28 06:48:19 AM | Valid: [ 35/180] Step 150/312 Loss 8.281 Prec@(1,3) (68.1%, 97.1%), ce_loss 0.992, lat_loss 22.018
09/28 06:48:24 AM | Valid: [ 35/180] Step 200/312 Loss 8.409 Prec@(1,3) (67.5%, 97.0%), ce_loss 0.992, lat_loss 22.017
09/28 06:48:29 AM | Valid: [ 35/180] Step 250/312 Loss 8.400 Prec@(1,3) (67.5%, 97.0%), ce_loss 0.992, lat_loss 22.017
09/28 06:48:33 AM | Valid: [ 35/180] Step 300/312 Loss 8.374 Prec@(1,3) (67.9%, 96.9%), ce_loss 0.992, lat_loss 22.017
09/28 06:48:35 AM | Valid: [ 35/180] Step 312/312 Loss 8.423 Prec@(1,3) (67.8%, 96.8%), ce_loss 0.992, lat_loss 22.017
09/28 06:48:35 AM | val: [ 35/180] Final Prec@1 67.7900% Time 29.96
09/28 06:48:35 AM | Start to train weights for epoch 35
09/28 06:48:51 AM | Train: [ 36/180] Step 050/1249 Loss 6.318 Prec@(1,3) (76.2%, 97.8%), ce_loss 0.992, lat_loss 22.017
09/28 06:49:06 AM | Train: [ 36/180] Step 100/1249 Loss 6.239 Prec@(1,3) (76.4%, 97.9%), ce_loss 0.992, lat_loss 22.016
09/28 06:49:20 AM | Train: [ 36/180] Step 150/1249 Loss 6.118 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.992, lat_loss 22.016
09/28 06:49:35 AM | Train: [ 36/180] Step 200/1249 Loss 6.156 Prec@(1,3) (76.4%, 98.1%), ce_loss 0.991, lat_loss 22.016
09/28 06:49:49 AM | Train: [ 36/180] Step 250/1249 Loss 6.141 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.991, lat_loss 22.016
09/28 06:50:06 AM | Train: [ 36/180] Step 300/1249 Loss 6.211 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.991, lat_loss 22.015
09/28 06:50:23 AM | Train: [ 36/180] Step 350/1249 Loss 6.170 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.991, lat_loss 22.015
09/28 06:50:40 AM | Train: [ 36/180] Step 400/1249 Loss 6.176 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.990, lat_loss 22.015
09/28 06:50:57 AM | Train: [ 36/180] Step 450/1249 Loss 6.238 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.990, lat_loss 22.014
09/28 06:51:14 AM | Train: [ 36/180] Step 500/1249 Loss 6.254 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.990, lat_loss 22.014
09/28 06:51:28 AM | Train: [ 36/180] Step 550/1249 Loss 6.240 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.990, lat_loss 22.014
09/28 06:51:43 AM | Train: [ 36/180] Step 600/1249 Loss 6.252 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.990, lat_loss 22.014
09/28 06:51:58 AM | Train: [ 36/180] Step 650/1249 Loss 6.279 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.989, lat_loss 22.013
09/28 06:52:12 AM | Train: [ 36/180] Step 700/1249 Loss 6.317 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.989, lat_loss 22.013
09/28 06:52:27 AM | Train: [ 36/180] Step 750/1249 Loss 6.357 Prec@(1,3) (75.6%, 98.0%), ce_loss 0.989, lat_loss 22.013
09/28 06:52:41 AM | Train: [ 36/180] Step 800/1249 Loss 6.330 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.989, lat_loss 22.013
09/28 06:52:56 AM | Train: [ 36/180] Step 850/1249 Loss 6.316 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.989, lat_loss 22.012
09/28 06:53:11 AM | Train: [ 36/180] Step 900/1249 Loss 6.337 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.988, lat_loss 22.012
09/28 06:53:25 AM | Train: [ 36/180] Step 950/1249 Loss 6.350 Prec@(1,3) (75.7%, 98.0%), ce_loss 0.988, lat_loss 22.012
09/28 06:53:40 AM | Train: [ 36/180] Step 1000/1249 Loss 6.336 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.988, lat_loss 22.012
09/28 06:53:54 AM | Train: [ 36/180] Step 1050/1249 Loss 6.321 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.988, lat_loss 22.011
09/28 06:54:09 AM | Train: [ 36/180] Step 1100/1249 Loss 6.323 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.988, lat_loss 22.011
09/28 06:54:23 AM | Train: [ 36/180] Step 1150/1249 Loss 6.319 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.987, lat_loss 22.011
09/28 06:54:38 AM | Train: [ 36/180] Step 1200/1249 Loss 6.323 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.987, lat_loss 22.011
09/28 06:54:52 AM | Train: [ 36/180] Step 1249/1249 Loss 6.324 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.987, lat_loss 22.010
09/28 06:54:52 AM | _w_step_train: [ 36/180] Final Prec@1 75.7250% Time 377.03
09/28 06:54:52 AM | Start to train theta for epoch 35
09/28 06:55:13 AM | Train: [ 36/180] Step 050/312 Loss 7.123 Prec@(1,3) (73.9%, 98.0%), ce_loss 0.987, lat_loss 22.010
09/28 06:55:33 AM | Train: [ 36/180] Step 100/312 Loss 7.015 Prec@(1,3) (73.5%, 97.7%), ce_loss 0.987, lat_loss 22.010
09/28 06:55:53 AM | Train: [ 36/180] Step 150/312 Loss 6.948 Prec@(1,3) (74.0%, 97.9%), ce_loss 0.986, lat_loss 22.010
09/28 06:56:13 AM | Train: [ 36/180] Step 200/312 Loss 6.750 Prec@(1,3) (74.6%, 97.8%), ce_loss 0.986, lat_loss 22.009
09/28 06:56:32 AM | Train: [ 36/180] Step 250/312 Loss 6.714 Prec@(1,3) (74.6%, 97.9%), ce_loss 0.986, lat_loss 22.009
09/28 06:56:53 AM | Train: [ 36/180] Step 300/312 Loss 6.635 Prec@(1,3) (74.9%, 97.9%), ce_loss 0.986, lat_loss 22.009
09/28 06:56:58 AM | Train: [ 36/180] Step 312/312 Loss 6.582 Prec@(1,3) (75.1%, 97.9%), ce_loss 0.986, lat_loss 22.009
09/28 06:56:58 AM | _theta_step_train: [ 36/180] Final Prec@1 75.1100% Time 125.65
09/28 06:57:03 AM | Valid: [ 36/180] Step 050/312 Loss 8.012 Prec@(1,3) (72.9%, 97.5%), ce_loss 0.986, lat_loss 22.009
09/28 06:57:08 AM | Valid: [ 36/180] Step 100/312 Loss 7.994 Prec@(1,3) (71.6%, 97.7%), ce_loss 0.986, lat_loss 22.008
09/28 06:57:12 AM | Valid: [ 36/180] Step 150/312 Loss 8.066 Prec@(1,3) (71.3%, 97.4%), ce_loss 0.986, lat_loss 22.008
09/28 06:57:17 AM | Valid: [ 36/180] Step 200/312 Loss 8.019 Prec@(1,3) (70.9%, 97.2%), ce_loss 0.985, lat_loss 22.008
09/28 06:57:21 AM | Valid: [ 36/180] Step 250/312 Loss 7.991 Prec@(1,3) (70.6%, 97.2%), ce_loss 0.985, lat_loss 22.008
09/28 06:57:26 AM | Valid: [ 36/180] Step 300/312 Loss 7.966 Prec@(1,3) (70.8%, 97.3%), ce_loss 0.985, lat_loss 22.007
09/28 06:57:27 AM | Valid: [ 36/180] Step 312/312 Loss 7.966 Prec@(1,3) (70.8%, 97.3%), ce_loss 0.985, lat_loss 22.007
09/28 06:57:27 AM | val: [ 36/180] Final Prec@1 70.7900% Time 29.42
09/28 06:57:27 AM | Start to train weights for epoch 36
09/28 06:57:51 AM | Train: [ 37/180] Step 050/1249 Loss 6.093 Prec@(1,3) (78.0%, 98.1%), ce_loss 0.985, lat_loss 22.007
09/28 06:58:13 AM | Train: [ 37/180] Step 100/1249 Loss 6.078 Prec@(1,3) (77.2%, 98.4%), ce_loss 0.985, lat_loss 22.007
09/28 06:58:36 AM | Train: [ 37/180] Step 150/1249 Loss 6.171 Prec@(1,3) (76.9%, 98.2%), ce_loss 0.985, lat_loss 22.006
09/28 06:58:59 AM | Train: [ 37/180] Step 200/1249 Loss 6.266 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.984, lat_loss 22.006
09/28 06:59:21 AM | Train: [ 37/180] Step 250/1249 Loss 6.264 Prec@(1,3) (76.2%, 98.0%), ce_loss 0.984, lat_loss 22.006
09/28 06:59:45 AM | Train: [ 37/180] Step 300/1249 Loss 6.264 Prec@(1,3) (76.2%, 98.0%), ce_loss 0.984, lat_loss 22.006
09/28 07:00:07 AM | Train: [ 37/180] Step 350/1249 Loss 6.234 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.984, lat_loss 22.005
09/28 07:00:29 AM | Train: [ 37/180] Step 400/1249 Loss 6.250 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.984, lat_loss 22.005
09/28 07:00:52 AM | Train: [ 37/180] Step 450/1249 Loss 6.264 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.983, lat_loss 22.005
09/28 07:01:15 AM | Train: [ 37/180] Step 500/1249 Loss 6.285 Prec@(1,3) (76.0%, 98.2%), ce_loss 0.983, lat_loss 22.005
09/28 07:01:37 AM | Train: [ 37/180] Step 550/1249 Loss 6.315 Prec@(1,3) (75.9%, 98.2%), ce_loss 0.983, lat_loss 22.005
09/28 07:02:00 AM | Train: [ 37/180] Step 600/1249 Loss 6.324 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.983, lat_loss 22.004
09/28 07:02:23 AM | Train: [ 37/180] Step 650/1249 Loss 6.312 Prec@(1,3) (75.8%, 98.2%), ce_loss 0.983, lat_loss 22.004
09/28 07:02:46 AM | Train: [ 37/180] Step 700/1249 Loss 6.326 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.982, lat_loss 22.004
09/28 07:03:07 AM | Train: [ 37/180] Step 750/1249 Loss 6.315 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.982, lat_loss 22.003
09/28 07:03:27 AM | Train: [ 37/180] Step 800/1249 Loss 6.324 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.982, lat_loss 22.003
09/28 07:03:48 AM | Train: [ 37/180] Step 850/1249 Loss 6.305 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.982, lat_loss 22.003
09/28 07:04:10 AM | Train: [ 37/180] Step 900/1249 Loss 6.318 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.982, lat_loss 22.003
09/28 07:04:33 AM | Train: [ 37/180] Step 950/1249 Loss 6.344 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.981, lat_loss 22.003
09/28 07:04:53 AM | Train: [ 37/180] Step 1000/1249 Loss 6.343 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.981, lat_loss 22.002
09/28 07:05:16 AM | Train: [ 37/180] Step 1050/1249 Loss 6.330 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.981, lat_loss 22.002
09/28 07:05:39 AM | Train: [ 37/180] Step 1100/1249 Loss 6.320 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.981, lat_loss 22.002
09/28 07:06:01 AM | Train: [ 37/180] Step 1150/1249 Loss 6.318 Prec@(1,3) (75.6%, 98.2%), ce_loss 0.981, lat_loss 22.002
09/28 07:06:24 AM | Train: [ 37/180] Step 1200/1249 Loss 6.316 Prec@(1,3) (75.7%, 98.2%), ce_loss 0.980, lat_loss 22.001
09/28 07:06:48 AM | Train: [ 37/180] Step 1249/1249 Loss 6.312 Prec@(1,3) (75.7%, 98.2%), ce_loss 0.980, lat_loss 22.001
09/28 07:06:48 AM | _w_step_train: [ 37/180] Final Prec@1 75.6975% Time 561.04
09/28 07:06:48 AM | Start to train theta for epoch 36
09/28 07:07:10 AM | Train: [ 37/180] Step 050/312 Loss 6.060 Prec@(1,3) (77.2%, 97.5%), ce_loss 0.980, lat_loss 22.001
09/28 07:07:29 AM | Train: [ 37/180] Step 100/312 Loss 6.275 Prec@(1,3) (76.3%, 97.4%), ce_loss 0.980, lat_loss 22.001
09/28 07:07:49 AM | Train: [ 37/180] Step 150/312 Loss 6.301 Prec@(1,3) (76.1%, 97.6%), ce_loss 0.980, lat_loss 22.000
09/28 07:08:10 AM | Train: [ 37/180] Step 200/312 Loss 6.331 Prec@(1,3) (75.9%, 97.7%), ce_loss 0.979, lat_loss 22.000
09/28 07:08:31 AM | Train: [ 37/180] Step 250/312 Loss 6.286 Prec@(1,3) (76.1%, 97.8%), ce_loss 0.979, lat_loss 22.000
09/28 07:08:52 AM | Train: [ 37/180] Step 300/312 Loss 6.321 Prec@(1,3) (75.9%, 97.8%), ce_loss 0.979, lat_loss 22.000
09/28 07:08:57 AM | Train: [ 37/180] Step 312/312 Loss 6.324 Prec@(1,3) (75.9%, 97.9%), ce_loss 0.979, lat_loss 21.999
09/28 07:08:57 AM | _theta_step_train: [ 37/180] Final Prec@1 75.9000% Time 129.04
09/28 07:09:03 AM | Valid: [ 37/180] Step 050/312 Loss 7.945 Prec@(1,3) (72.8%, 96.8%), ce_loss 0.979, lat_loss 21.999
09/28 07:09:07 AM | Valid: [ 37/180] Step 100/312 Loss 7.601 Prec@(1,3) (72.7%, 97.1%), ce_loss 0.979, lat_loss 21.999
09/28 07:09:12 AM | Valid: [ 37/180] Step 150/312 Loss 7.463 Prec@(1,3) (73.0%, 97.1%), ce_loss 0.979, lat_loss 21.999
09/28 07:09:16 AM | Valid: [ 37/180] Step 200/312 Loss 7.522 Prec@(1,3) (72.6%, 97.0%), ce_loss 0.979, lat_loss 21.998
09/28 07:09:21 AM | Valid: [ 37/180] Step 250/312 Loss 7.544 Prec@(1,3) (72.5%, 97.2%), ce_loss 0.979, lat_loss 21.998
09/28 07:09:26 AM | Valid: [ 37/180] Step 300/312 Loss 7.457 Prec@(1,3) (72.7%, 97.2%), ce_loss 0.978, lat_loss 21.998
09/28 07:09:27 AM | Valid: [ 37/180] Step 312/312 Loss 7.506 Prec@(1,3) (72.6%, 97.2%), ce_loss 0.978, lat_loss 21.998
09/28 07:09:27 AM | val: [ 37/180] Final Prec@1 72.5900% Time 29.54
09/28 07:09:27 AM | Start to train weights for epoch 37
09/28 07:09:53 AM | Train: [ 38/180] Step 050/1249 Loss 6.395 Prec@(1,3) (74.3%, 98.6%), ce_loss 0.978, lat_loss 21.998
09/28 07:10:18 AM | Train: [ 38/180] Step 100/1249 Loss 6.286 Prec@(1,3) (75.3%, 98.1%), ce_loss 0.978, lat_loss 21.997
09/28 07:10:42 AM | Train: [ 38/180] Step 150/1249 Loss 6.400 Prec@(1,3) (75.1%, 98.3%), ce_loss 0.978, lat_loss 21.997
09/28 07:10:58 AM | Train: [ 38/180] Step 200/1249 Loss 6.482 Prec@(1,3) (74.8%, 98.3%), ce_loss 0.978, lat_loss 21.997
09/28 07:11:13 AM | Train: [ 38/180] Step 250/1249 Loss 6.559 Prec@(1,3) (74.7%, 98.1%), ce_loss 0.978, lat_loss 21.996
09/28 07:11:27 AM | Train: [ 38/180] Step 300/1249 Loss 6.502 Prec@(1,3) (75.0%, 98.0%), ce_loss 0.977, lat_loss 21.996
09/28 07:11:42 AM | Train: [ 38/180] Step 350/1249 Loss 6.454 Prec@(1,3) (75.2%, 98.0%), ce_loss 0.977, lat_loss 21.996
09/28 07:11:56 AM | Train: [ 38/180] Step 400/1249 Loss 6.437 Prec@(1,3) (75.3%, 98.0%), ce_loss 0.977, lat_loss 21.996
09/28 07:12:11 AM | Train: [ 38/180] Step 450/1249 Loss 6.422 Prec@(1,3) (75.4%, 98.0%), ce_loss 0.977, lat_loss 21.995
09/28 07:12:25 AM | Train: [ 38/180] Step 500/1249 Loss 6.415 Prec@(1,3) (75.4%, 98.0%), ce_loss 0.977, lat_loss 21.995
09/28 07:12:40 AM | Train: [ 38/180] Step 550/1249 Loss 6.401 Prec@(1,3) (75.4%, 98.0%), ce_loss 0.976, lat_loss 21.995
09/28 07:12:54 AM | Train: [ 38/180] Step 600/1249 Loss 6.349 Prec@(1,3) (75.6%, 98.0%), ce_loss 0.976, lat_loss 21.995
09/28 07:13:11 AM | Train: [ 38/180] Step 650/1249 Loss 6.351 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.976, lat_loss 21.994
09/28 07:13:33 AM | Train: [ 38/180] Step 700/1249 Loss 6.326 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.976, lat_loss 21.994
09/28 07:13:54 AM | Train: [ 38/180] Step 750/1249 Loss 6.322 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.975, lat_loss 21.994
09/28 07:14:18 AM | Train: [ 38/180] Step 800/1249 Loss 6.321 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.975, lat_loss 21.994
09/28 07:14:42 AM | Train: [ 38/180] Step 850/1249 Loss 6.341 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.975, lat_loss 21.993
09/28 07:15:04 AM | Train: [ 38/180] Step 900/1249 Loss 6.330 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.975, lat_loss 21.993
09/28 07:15:28 AM | Train: [ 38/180] Step 950/1249 Loss 6.318 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.975, lat_loss 21.993
09/28 07:15:50 AM | Train: [ 38/180] Step 1000/1249 Loss 6.326 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.975, lat_loss 21.993
09/28 07:16:11 AM | Train: [ 38/180] Step 1050/1249 Loss 6.336 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.974, lat_loss 21.992
09/28 07:16:33 AM | Train: [ 38/180] Step 1100/1249 Loss 6.325 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.974, lat_loss 21.992
09/28 07:16:55 AM | Train: [ 38/180] Step 1150/1249 Loss 6.309 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.974, lat_loss 21.992
09/28 07:17:18 AM | Train: [ 38/180] Step 1200/1249 Loss 6.329 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.974, lat_loss 21.992
09/28 07:17:42 AM | Train: [ 38/180] Step 1249/1249 Loss 6.316 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.974, lat_loss 21.991
09/28 07:17:42 AM | _w_step_train: [ 38/180] Final Prec@1 75.8725% Time 495.25
09/28 07:17:42 AM | Start to train theta for epoch 37
09/28 07:18:04 AM | Train: [ 38/180] Step 050/312 Loss 6.661 Prec@(1,3) (73.8%, 97.7%), ce_loss 0.973, lat_loss 21.991
09/28 07:18:24 AM | Train: [ 38/180] Step 100/312 Loss 6.700 Prec@(1,3) (74.3%, 97.6%), ce_loss 0.973, lat_loss 21.991
09/28 07:18:43 AM | Train: [ 38/180] Step 150/312 Loss 6.714 Prec@(1,3) (73.9%, 97.8%), ce_loss 0.973, lat_loss 21.991
09/28 07:19:02 AM | Train: [ 38/180] Step 200/312 Loss 6.609 Prec@(1,3) (74.5%, 97.9%), ce_loss 0.973, lat_loss 21.990
09/28 07:19:22 AM | Train: [ 38/180] Step 250/312 Loss 6.579 Prec@(1,3) (74.6%, 98.0%), ce_loss 0.973, lat_loss 21.990
09/28 07:19:42 AM | Train: [ 38/180] Step 300/312 Loss 6.487 Prec@(1,3) (75.1%, 98.1%), ce_loss 0.972, lat_loss 21.990
09/28 07:19:47 AM | Train: [ 38/180] Step 312/312 Loss 6.516 Prec@(1,3) (75.1%, 98.1%), ce_loss 0.972, lat_loss 21.990
09/28 07:19:47 AM | _theta_step_train: [ 38/180] Final Prec@1 75.0800% Time 124.69
09/28 07:19:52 AM | Valid: [ 38/180] Step 050/312 Loss 7.046 Prec@(1,3) (74.0%, 97.6%), ce_loss 0.972, lat_loss 21.990
09/28 07:19:57 AM | Valid: [ 38/180] Step 100/312 Loss 7.345 Prec@(1,3) (72.6%, 97.0%), ce_loss 0.972, lat_loss 21.989
09/28 07:20:01 AM | Valid: [ 38/180] Step 150/312 Loss 7.567 Prec@(1,3) (72.1%, 96.9%), ce_loss 0.972, lat_loss 21.989
09/28 07:20:06 AM | Valid: [ 38/180] Step 200/312 Loss 7.811 Prec@(1,3) (71.1%, 96.9%), ce_loss 0.972, lat_loss 21.989
09/28 07:20:10 AM | Valid: [ 38/180] Step 250/312 Loss 7.864 Prec@(1,3) (70.9%, 96.7%), ce_loss 0.972, lat_loss 21.989
09/28 07:20:15 AM | Valid: [ 38/180] Step 300/312 Loss 7.963 Prec@(1,3) (70.7%, 96.7%), ce_loss 0.972, lat_loss 21.988
09/28 07:20:16 AM | Valid: [ 38/180] Step 312/312 Loss 7.916 Prec@(1,3) (70.8%, 96.7%), ce_loss 0.972, lat_loss 21.988
09/28 07:20:16 AM | val: [ 38/180] Final Prec@1 70.7500% Time 29.21
09/28 07:20:16 AM | Start to train weights for epoch 38
09/28 07:20:42 AM | Train: [ 39/180] Step 050/1249 Loss 6.574 Prec@(1,3) (74.4%, 97.9%), ce_loss 0.972, lat_loss 21.988
09/28 07:21:07 AM | Train: [ 39/180] Step 100/1249 Loss 6.432 Prec@(1,3) (75.0%, 98.3%), ce_loss 0.972, lat_loss 21.988
09/28 07:21:31 AM | Train: [ 39/180] Step 150/1249 Loss 6.451 Prec@(1,3) (74.9%, 98.1%), ce_loss 0.972, lat_loss 21.987
09/28 07:21:55 AM | Train: [ 39/180] Step 200/1249 Loss 6.347 Prec@(1,3) (75.2%, 98.1%), ce_loss 0.971, lat_loss 21.987
09/28 07:22:20 AM | Train: [ 39/180] Step 250/1249 Loss 6.365 Prec@(1,3) (75.5%, 98.1%), ce_loss 0.971, lat_loss 21.987
09/28 07:22:44 AM | Train: [ 39/180] Step 300/1249 Loss 6.402 Prec@(1,3) (75.7%, 98.0%), ce_loss 0.971, lat_loss 21.987
09/28 07:23:07 AM | Train: [ 39/180] Step 350/1249 Loss 6.393 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.971, lat_loss 21.987
09/28 07:23:31 AM | Train: [ 39/180] Step 400/1249 Loss 6.396 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.971, lat_loss 21.986
09/28 07:23:56 AM | Train: [ 39/180] Step 450/1249 Loss 6.379 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.971, lat_loss 21.986
09/28 07:24:20 AM | Train: [ 39/180] Step 500/1249 Loss 6.394 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.970, lat_loss 21.986
09/28 07:24:43 AM | Train: [ 39/180] Step 550/1249 Loss 6.397 Prec@(1,3) (75.7%, 98.0%), ce_loss 0.970, lat_loss 21.986
09/28 07:25:07 AM | Train: [ 39/180] Step 600/1249 Loss 6.404 Prec@(1,3) (75.7%, 98.0%), ce_loss 0.970, lat_loss 21.985
09/28 07:25:32 AM | Train: [ 39/180] Step 650/1249 Loss 6.362 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.970, lat_loss 21.985
09/28 07:25:56 AM | Train: [ 39/180] Step 700/1249 Loss 6.337 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.970, lat_loss 21.985
09/28 07:26:21 AM | Train: [ 39/180] Step 750/1249 Loss 6.335 Prec@(1,3) (76.0%, 98.0%), ce_loss 0.969, lat_loss 21.984
09/28 07:26:43 AM | Train: [ 39/180] Step 800/1249 Loss 6.349 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.969, lat_loss 21.984
09/28 07:27:06 AM | Train: [ 39/180] Step 850/1249 Loss 6.318 Prec@(1,3) (76.0%, 98.0%), ce_loss 0.969, lat_loss 21.984
09/28 07:27:31 AM | Train: [ 39/180] Step 900/1249 Loss 6.330 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.969, lat_loss 21.984
09/28 07:27:56 AM | Train: [ 39/180] Step 950/1249 Loss 6.318 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.969, lat_loss 21.983
09/28 07:28:20 AM | Train: [ 39/180] Step 1000/1249 Loss 6.344 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.969, lat_loss 21.983
09/28 07:28:44 AM | Train: [ 39/180] Step 1050/1249 Loss 6.362 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.968, lat_loss 21.983
09/28 07:29:08 AM | Train: [ 39/180] Step 1100/1249 Loss 6.375 Prec@(1,3) (75.7%, 98.0%), ce_loss 0.968, lat_loss 21.983
09/28 07:29:32 AM | Train: [ 39/180] Step 1150/1249 Loss 6.368 Prec@(1,3) (75.7%, 98.0%), ce_loss 0.968, lat_loss 21.982
09/28 07:29:57 AM | Train: [ 39/180] Step 1200/1249 Loss 6.353 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.968, lat_loss 21.982
09/28 07:30:21 AM | Train: [ 39/180] Step 1249/1249 Loss 6.350 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.968, lat_loss 21.982
09/28 07:30:22 AM | _w_step_train: [ 39/180] Final Prec@1 75.7650% Time 605.48
09/28 07:30:22 AM | Start to train theta for epoch 38
09/28 07:30:43 AM | Train: [ 39/180] Step 050/312 Loss 6.415 Prec@(1,3) (75.9%, 97.8%), ce_loss 0.967, lat_loss 21.982
09/28 07:31:03 AM | Train: [ 39/180] Step 100/312 Loss 6.428 Prec@(1,3) (75.9%, 97.9%), ce_loss 0.967, lat_loss 21.982
09/28 07:31:24 AM | Train: [ 39/180] Step 150/312 Loss 6.594 Prec@(1,3) (75.4%, 97.6%), ce_loss 0.967, lat_loss 21.981
09/28 07:31:40 AM | Train: [ 39/180] Step 200/312 Loss 6.609 Prec@(1,3) (75.2%, 97.6%), ce_loss 0.967, lat_loss 21.981
09/28 07:31:52 AM | Train: [ 39/180] Step 250/312 Loss 6.600 Prec@(1,3) (75.3%, 97.6%), ce_loss 0.967, lat_loss 21.981
09/28 07:32:12 AM | Train: [ 39/180] Step 300/312 Loss 6.526 Prec@(1,3) (75.6%, 97.7%), ce_loss 0.967, lat_loss 21.981
09/28 07:32:17 AM | Train: [ 39/180] Step 312/312 Loss 6.527 Prec@(1,3) (75.5%, 97.8%), ce_loss 0.967, lat_loss 21.981
09/28 07:32:17 AM | _theta_step_train: [ 39/180] Final Prec@1 75.5100% Time 115.14
09/28 07:32:23 AM | Valid: [ 39/180] Step 050/312 Loss 7.486 Prec@(1,3) (70.9%, 97.9%), ce_loss 0.967, lat_loss 21.980
09/28 07:32:27 AM | Valid: [ 39/180] Step 100/312 Loss 7.757 Prec@(1,3) (70.0%, 97.1%), ce_loss 0.967, lat_loss 21.980
09/28 07:32:32 AM | Valid: [ 39/180] Step 150/312 Loss 8.049 Prec@(1,3) (69.8%, 96.6%), ce_loss 0.967, lat_loss 21.980
09/28 07:32:36 AM | Valid: [ 39/180] Step 200/312 Loss 8.064 Prec@(1,3) (69.8%, 96.4%), ce_loss 0.967, lat_loss 21.980
09/28 07:32:41 AM | Valid: [ 39/180] Step 250/312 Loss 8.138 Prec@(1,3) (69.7%, 96.4%), ce_loss 0.967, lat_loss 21.979
09/28 07:32:45 AM | Valid: [ 39/180] Step 300/312 Loss 8.043 Prec@(1,3) (70.2%, 96.5%), ce_loss 0.966, lat_loss 21.979
09/28 07:32:46 AM | Valid: [ 39/180] Step 312/312 Loss 8.063 Prec@(1,3) (70.0%, 96.5%), ce_loss 0.966, lat_loss 21.979
09/28 07:32:46 AM | val: [ 39/180] Final Prec@1 70.0400% Time 29.67
09/28 07:32:46 AM | Start to train weights for epoch 39
09/28 07:33:13 AM | Train: [ 40/180] Step 050/1249 Loss 6.154 Prec@(1,3) (77.0%, 98.0%), ce_loss 0.966, lat_loss 21.979
09/28 07:33:38 AM | Train: [ 40/180] Step 100/1249 Loss 6.190 Prec@(1,3) (76.8%, 97.9%), ce_loss 0.966, lat_loss 21.979
09/28 07:34:02 AM | Train: [ 40/180] Step 150/1249 Loss 6.179 Prec@(1,3) (76.9%, 98.1%), ce_loss 0.966, lat_loss 21.979
09/28 07:34:26 AM | Train: [ 40/180] Step 200/1249 Loss 6.181 Prec@(1,3) (76.7%, 98.0%), ce_loss 0.966, lat_loss 21.978
09/28 07:34:49 AM | Train: [ 40/180] Step 250/1249 Loss 6.179 Prec@(1,3) (76.6%, 98.1%), ce_loss 0.966, lat_loss 21.978
09/28 07:35:11 AM | Train: [ 40/180] Step 300/1249 Loss 6.175 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.965, lat_loss 21.978
09/28 07:35:35 AM | Train: [ 40/180] Step 350/1249 Loss 6.123 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.965, lat_loss 21.978
09/28 07:36:00 AM | Train: [ 40/180] Step 400/1249 Loss 6.130 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.965, lat_loss 21.977
09/28 07:36:25 AM | Train: [ 40/180] Step 450/1249 Loss 6.152 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.965, lat_loss 21.977
09/28 07:36:50 AM | Train: [ 40/180] Step 500/1249 Loss 6.159 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.965, lat_loss 21.977
09/28 07:37:15 AM | Train: [ 40/180] Step 550/1249 Loss 6.139 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.964, lat_loss 21.977
09/28 07:37:40 AM | Train: [ 40/180] Step 600/1249 Loss 6.123 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.964, lat_loss 21.977
09/28 07:38:05 AM | Train: [ 40/180] Step 650/1249 Loss 6.153 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.964, lat_loss 21.976
09/28 07:38:30 AM | Train: [ 40/180] Step 700/1249 Loss 6.164 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.964, lat_loss 21.976
09/28 07:38:55 AM | Train: [ 40/180] Step 750/1249 Loss 6.174 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.964, lat_loss 21.976
09/28 07:39:20 AM | Train: [ 40/180] Step 800/1249 Loss 6.173 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.963, lat_loss 21.976
09/28 07:39:44 AM | Train: [ 40/180] Step 850/1249 Loss 6.146 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.963, lat_loss 21.975
09/28 07:40:09 AM | Train: [ 40/180] Step 900/1249 Loss 6.142 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.963, lat_loss 21.975
09/28 07:40:34 AM | Train: [ 40/180] Step 950/1249 Loss 6.159 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.963, lat_loss 21.975
09/28 07:40:58 AM | Train: [ 40/180] Step 1000/1249 Loss 6.195 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.963, lat_loss 21.975
09/28 07:41:23 AM | Train: [ 40/180] Step 1050/1249 Loss 6.199 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.963, lat_loss 21.975
09/28 07:41:48 AM | Train: [ 40/180] Step 1100/1249 Loss 6.211 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.962, lat_loss 21.974
09/28 07:42:13 AM | Train: [ 40/180] Step 1150/1249 Loss 6.230 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.962, lat_loss 21.974
09/28 07:42:38 AM | Train: [ 40/180] Step 1200/1249 Loss 6.213 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.962, lat_loss 21.974
09/28 07:43:03 AM | Train: [ 40/180] Step 1249/1249 Loss 6.219 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.962, lat_loss 21.974
09/28 07:43:03 AM | _w_step_train: [ 40/180] Final Prec@1 76.1750% Time 616.51
09/28 07:43:03 AM | Start to train theta for epoch 39
09/28 07:43:24 AM | Train: [ 40/180] Step 050/312 Loss 6.904 Prec@(1,3) (74.0%, 97.7%), ce_loss 0.962, lat_loss 21.974
09/28 07:43:44 AM | Train: [ 40/180] Step 100/312 Loss 6.647 Prec@(1,3) (74.6%, 98.0%), ce_loss 0.962, lat_loss 21.973
09/28 07:44:04 AM | Train: [ 40/180] Step 150/312 Loss 6.610 Prec@(1,3) (75.0%, 98.0%), ce_loss 0.961, lat_loss 21.973
09/28 07:44:24 AM | Train: [ 40/180] Step 200/312 Loss 6.552 Prec@(1,3) (75.3%, 97.9%), ce_loss 0.961, lat_loss 21.973
09/28 07:44:45 AM | Train: [ 40/180] Step 250/312 Loss 6.630 Prec@(1,3) (75.1%, 97.8%), ce_loss 0.961, lat_loss 21.973
09/28 07:45:05 AM | Train: [ 40/180] Step 300/312 Loss 6.658 Prec@(1,3) (74.9%, 97.8%), ce_loss 0.961, lat_loss 21.973
09/28 07:45:10 AM | Train: [ 40/180] Step 312/312 Loss 6.640 Prec@(1,3) (74.9%, 97.8%), ce_loss 0.961, lat_loss 21.972
09/28 07:45:11 AM | _theta_step_train: [ 40/180] Final Prec@1 74.9300% Time 127.62
09/28 07:45:16 AM | Valid: [ 40/180] Step 050/312 Loss 7.916 Prec@(1,3) (69.7%, 96.6%), ce_loss 0.961, lat_loss 21.972
09/28 07:45:20 AM | Valid: [ 40/180] Step 100/312 Loss 7.968 Prec@(1,3) (69.5%, 96.2%), ce_loss 0.961, lat_loss 21.972
09/28 07:45:25 AM | Valid: [ 40/180] Step 150/312 Loss 8.013 Prec@(1,3) (69.6%, 96.2%), ce_loss 0.961, lat_loss 21.972
09/28 07:45:30 AM | Valid: [ 40/180] Step 200/312 Loss 7.900 Prec@(1,3) (70.1%, 96.4%), ce_loss 0.961, lat_loss 21.972
09/28 07:45:34 AM | Valid: [ 40/180] Step 250/312 Loss 7.925 Prec@(1,3) (70.2%, 96.4%), ce_loss 0.961, lat_loss 21.971
09/28 07:45:39 AM | Valid: [ 40/180] Step 300/312 Loss 7.799 Prec@(1,3) (70.7%, 96.5%), ce_loss 0.961, lat_loss 21.971
09/28 07:45:40 AM | Valid: [ 40/180] Step 312/312 Loss 7.760 Prec@(1,3) (70.8%, 96.5%), ce_loss 0.961, lat_loss 21.971
09/28 07:45:40 AM | val: [ 40/180] Final Prec@1 70.8000% Time 29.28
09/28 07:45:40 AM | Start to train weights for epoch 40
09/28 07:46:06 AM | Train: [ 41/180] Step 050/1249 Loss 6.469 Prec@(1,3) (74.8%, 97.5%), ce_loss 0.961, lat_loss 21.971
09/28 07:46:28 AM | Train: [ 41/180] Step 100/1249 Loss 6.114 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.960, lat_loss 21.971
09/28 07:46:52 AM | Train: [ 41/180] Step 150/1249 Loss 6.157 Prec@(1,3) (76.2%, 98.0%), ce_loss 0.960, lat_loss 21.971
09/28 07:47:14 AM | Train: [ 41/180] Step 200/1249 Loss 6.216 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.960, lat_loss 21.970
09/28 07:47:36 AM | Train: [ 41/180] Step 250/1249 Loss 6.309 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.960, lat_loss 21.970
09/28 07:47:59 AM | Train: [ 41/180] Step 300/1249 Loss 6.402 Prec@(1,3) (75.3%, 98.1%), ce_loss 0.960, lat_loss 21.970
09/28 07:48:21 AM | Train: [ 41/180] Step 350/1249 Loss 6.420 Prec@(1,3) (75.1%, 98.1%), ce_loss 0.960, lat_loss 21.970
09/28 07:48:43 AM | Train: [ 41/180] Step 400/1249 Loss 6.390 Prec@(1,3) (75.3%, 98.1%), ce_loss 0.959, lat_loss 21.970
09/28 07:49:05 AM | Train: [ 41/180] Step 450/1249 Loss 6.350 Prec@(1,3) (75.5%, 98.1%), ce_loss 0.959, lat_loss 21.969
09/28 07:49:27 AM | Train: [ 41/180] Step 500/1249 Loss 6.313 Prec@(1,3) (75.6%, 98.2%), ce_loss 0.959, lat_loss 21.969
09/28 07:49:50 AM | Train: [ 41/180] Step 550/1249 Loss 6.313 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.959, lat_loss 21.969
09/28 07:50:14 AM | Train: [ 41/180] Step 600/1249 Loss 6.329 Prec@(1,3) (75.4%, 98.2%), ce_loss 0.959, lat_loss 21.969
09/28 07:50:37 AM | Train: [ 41/180] Step 650/1249 Loss 6.333 Prec@(1,3) (75.4%, 98.2%), ce_loss 0.959, lat_loss 21.969
09/28 07:51:00 AM | Train: [ 41/180] Step 700/1249 Loss 6.326 Prec@(1,3) (75.5%, 98.1%), ce_loss 0.958, lat_loss 21.968
09/28 07:51:24 AM | Train: [ 41/180] Step 750/1249 Loss 6.343 Prec@(1,3) (75.5%, 98.1%), ce_loss 0.958, lat_loss 21.968
09/28 07:51:47 AM | Train: [ 41/180] Step 800/1249 Loss 6.357 Prec@(1,3) (75.5%, 98.1%), ce_loss 0.958, lat_loss 21.968
09/28 07:52:09 AM | Train: [ 41/180] Step 850/1249 Loss 6.330 Prec@(1,3) (75.5%, 98.1%), ce_loss 0.958, lat_loss 21.968
09/28 07:52:30 AM | Train: [ 41/180] Step 900/1249 Loss 6.306 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.958, lat_loss 21.967
09/28 07:52:55 AM | Train: [ 41/180] Step 950/1249 Loss 6.332 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.958, lat_loss 21.967
09/28 07:53:18 AM | Train: [ 41/180] Step 1000/1249 Loss 6.315 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.957, lat_loss 21.967
09/28 07:53:41 AM | Train: [ 41/180] Step 1050/1249 Loss 6.331 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.957, lat_loss 21.967
09/28 07:54:03 AM | Train: [ 41/180] Step 1100/1249 Loss 6.341 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.957, lat_loss 21.967
09/28 07:54:27 AM | Train: [ 41/180] Step 1150/1249 Loss 6.336 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.957, lat_loss 21.966
09/28 07:54:52 AM | Train: [ 41/180] Step 1200/1249 Loss 6.318 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.957, lat_loss 21.966
09/28 07:55:16 AM | Train: [ 41/180] Step 1249/1249 Loss 6.317 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.957, lat_loss 21.966
09/28 07:55:17 AM | _w_step_train: [ 41/180] Final Prec@1 75.7375% Time 576.69
09/28 07:55:17 AM | Start to train theta for epoch 40
09/28 07:55:36 AM | Train: [ 41/180] Step 050/312 Loss 6.344 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.956, lat_loss 21.966
09/28 07:55:55 AM | Train: [ 41/180] Step 100/312 Loss 6.436 Prec@(1,3) (75.6%, 98.0%), ce_loss 0.956, lat_loss 21.966
09/28 07:56:13 AM | Train: [ 41/180] Step 150/312 Loss 6.453 Prec@(1,3) (75.4%, 98.0%), ce_loss 0.956, lat_loss 21.965
09/28 07:56:32 AM | Train: [ 41/180] Step 200/312 Loss 6.489 Prec@(1,3) (75.0%, 98.0%), ce_loss 0.956, lat_loss 21.965
09/28 07:56:53 AM | Train: [ 41/180] Step 250/312 Loss 6.541 Prec@(1,3) (74.8%, 97.9%), ce_loss 0.956, lat_loss 21.965
09/28 07:57:13 AM | Train: [ 41/180] Step 300/312 Loss 6.517 Prec@(1,3) (75.0%, 98.0%), ce_loss 0.956, lat_loss 21.965
09/28 07:57:18 AM | Train: [ 41/180] Step 312/312 Loss 6.533 Prec@(1,3) (75.0%, 98.0%), ce_loss 0.956, lat_loss 21.965
09/28 07:57:18 AM | _theta_step_train: [ 41/180] Final Prec@1 74.9600% Time 121.82
09/28 07:57:24 AM | Valid: [ 41/180] Step 050/312 Loss 8.549 Prec@(1,3) (66.6%, 97.7%), ce_loss 0.956, lat_loss 21.965
09/28 07:57:28 AM | Valid: [ 41/180] Step 100/312 Loss 8.498 Prec@(1,3) (68.0%, 96.6%), ce_loss 0.956, lat_loss 21.964
09/28 07:57:33 AM | Valid: [ 41/180] Step 150/312 Loss 8.534 Prec@(1,3) (68.3%, 96.4%), ce_loss 0.956, lat_loss 21.964
09/28 07:57:38 AM | Valid: [ 41/180] Step 200/312 Loss 8.711 Prec@(1,3) (67.7%, 96.2%), ce_loss 0.956, lat_loss 21.964
09/28 07:57:42 AM | Valid: [ 41/180] Step 250/312 Loss 8.683 Prec@(1,3) (67.6%, 96.4%), ce_loss 0.956, lat_loss 21.964
09/28 07:57:47 AM | Valid: [ 41/180] Step 300/312 Loss 8.496 Prec@(1,3) (68.3%, 96.4%), ce_loss 0.956, lat_loss 21.964
09/28 07:57:48 AM | Valid: [ 41/180] Step 312/312 Loss 8.469 Prec@(1,3) (68.3%, 96.5%), ce_loss 0.956, lat_loss 21.964
09/28 07:57:48 AM | val: [ 41/180] Final Prec@1 68.3400% Time 29.73
09/28 07:57:48 AM | Start to train weights for epoch 41
09/28 07:58:14 AM | Train: [ 42/180] Step 050/1249 Loss 5.994 Prec@(1,3) (77.0%, 98.1%), ce_loss 0.956, lat_loss 21.963
09/28 07:58:37 AM | Train: [ 42/180] Step 100/1249 Loss 6.107 Prec@(1,3) (76.5%, 98.1%), ce_loss 0.955, lat_loss 21.963
09/28 07:58:59 AM | Train: [ 42/180] Step 150/1249 Loss 6.201 Prec@(1,3) (76.0%, 98.2%), ce_loss 0.955, lat_loss 21.963
09/28 07:59:22 AM | Train: [ 42/180] Step 200/1249 Loss 6.201 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.955, lat_loss 21.963
09/28 07:59:46 AM | Train: [ 42/180] Step 250/1249 Loss 6.175 Prec@(1,3) (76.1%, 98.3%), ce_loss 0.955, lat_loss 21.962
09/28 08:00:10 AM | Train: [ 42/180] Step 300/1249 Loss 6.227 Prec@(1,3) (75.9%, 98.2%), ce_loss 0.955, lat_loss 21.962
09/28 08:00:34 AM | Train: [ 42/180] Step 350/1249 Loss 6.328 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.955, lat_loss 21.962
09/28 08:00:58 AM | Train: [ 42/180] Step 400/1249 Loss 6.305 Prec@(1,3) (75.6%, 98.2%), ce_loss 0.954, lat_loss 21.962
09/28 08:01:21 AM | Train: [ 42/180] Step 450/1249 Loss 6.233 Prec@(1,3) (75.9%, 98.2%), ce_loss 0.954, lat_loss 21.962
09/28 08:01:45 AM | Train: [ 42/180] Step 500/1249 Loss 6.215 Prec@(1,3) (75.9%, 98.2%), ce_loss 0.954, lat_loss 21.961
09/28 08:02:09 AM | Train: [ 42/180] Step 550/1249 Loss 6.213 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.954, lat_loss 21.961
09/28 08:02:32 AM | Train: [ 42/180] Step 600/1249 Loss 6.213 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.954, lat_loss 21.961
09/28 08:02:55 AM | Train: [ 42/180] Step 650/1249 Loss 6.211 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.954, lat_loss 21.961
09/28 08:03:19 AM | Train: [ 42/180] Step 700/1249 Loss 6.207 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.953, lat_loss 21.961
09/28 08:03:42 AM | Train: [ 42/180] Step 750/1249 Loss 6.195 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.953, lat_loss 21.960
09/28 08:04:06 AM | Train: [ 42/180] Step 800/1249 Loss 6.199 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.953, lat_loss 21.960
09/28 08:04:29 AM | Train: [ 42/180] Step 850/1249 Loss 6.213 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.953, lat_loss 21.960
09/28 08:04:52 AM | Train: [ 42/180] Step 900/1249 Loss 6.229 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.953, lat_loss 21.960
09/28 08:05:15 AM | Train: [ 42/180] Step 950/1249 Loss 6.292 Prec@(1,3) (75.8%, 98.0%), ce_loss 0.953, lat_loss 21.960
09/28 08:05:40 AM | Train: [ 42/180] Step 1000/1249 Loss 6.272 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.953, lat_loss 21.959
09/28 08:06:06 AM | Train: [ 42/180] Step 1050/1249 Loss 6.298 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.952, lat_loss 21.959
09/28 08:06:31 AM | Train: [ 42/180] Step 1100/1249 Loss 6.314 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.952, lat_loss 21.959
09/28 08:06:56 AM | Train: [ 42/180] Step 1150/1249 Loss 6.306 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.952, lat_loss 21.959
09/28 08:07:21 AM | Train: [ 42/180] Step 1200/1249 Loss 6.308 Prec@(1,3) (75.9%, 98.0%), ce_loss 0.952, lat_loss 21.958
09/28 08:07:46 AM | Train: [ 42/180] Step 1249/1249 Loss 6.301 Prec@(1,3) (76.0%, 98.0%), ce_loss 0.952, lat_loss 21.958
09/28 08:07:46 AM | _w_step_train: [ 42/180] Final Prec@1 75.9550% Time 597.83
09/28 08:07:46 AM | Start to train theta for epoch 41
09/28 08:08:07 AM | Train: [ 42/180] Step 050/312 Loss 6.436 Prec@(1,3) (76.0%, 97.6%), ce_loss 0.952, lat_loss 21.958
09/28 08:08:28 AM | Train: [ 42/180] Step 100/312 Loss 6.226 Prec@(1,3) (76.7%, 97.8%), ce_loss 0.951, lat_loss 21.958
09/28 08:08:49 AM | Train: [ 42/180] Step 150/312 Loss 6.211 Prec@(1,3) (76.8%, 97.9%), ce_loss 0.951, lat_loss 21.958
09/28 08:09:09 AM | Train: [ 42/180] Step 200/312 Loss 6.301 Prec@(1,3) (76.6%, 97.8%), ce_loss 0.951, lat_loss 21.957
09/28 08:09:30 AM | Train: [ 42/180] Step 250/312 Loss 6.263 Prec@(1,3) (76.7%, 97.9%), ce_loss 0.951, lat_loss 21.957
09/28 08:09:51 AM | Train: [ 42/180] Step 300/312 Loss 6.199 Prec@(1,3) (76.8%, 97.9%), ce_loss 0.951, lat_loss 21.957
09/28 08:09:56 AM | Train: [ 42/180] Step 312/312 Loss 6.213 Prec@(1,3) (76.8%, 98.0%), ce_loss 0.951, lat_loss 21.957
09/28 08:09:56 AM | _theta_step_train: [ 42/180] Final Prec@1 76.7600% Time 130.24
09/28 08:10:02 AM | Valid: [ 42/180] Step 050/312 Loss 8.895 Prec@(1,3) (70.3%, 96.9%), ce_loss 0.951, lat_loss 21.957
09/28 08:10:06 AM | Valid: [ 42/180] Step 100/312 Loss 8.483 Prec@(1,3) (71.0%, 96.8%), ce_loss 0.951, lat_loss 21.957
09/28 08:10:11 AM | Valid: [ 42/180] Step 150/312 Loss 8.415 Prec@(1,3) (71.7%, 96.6%), ce_loss 0.951, lat_loss 21.956
09/28 08:10:15 AM | Valid: [ 42/180] Step 200/312 Loss 8.250 Prec@(1,3) (72.2%, 96.9%), ce_loss 0.951, lat_loss 21.956
09/28 08:10:20 AM | Valid: [ 42/180] Step 250/312 Loss 7.992 Prec@(1,3) (72.4%, 96.9%), ce_loss 0.951, lat_loss 21.956
09/28 08:10:24 AM | Valid: [ 42/180] Step 300/312 Loss 7.934 Prec@(1,3) (72.6%, 96.8%), ce_loss 0.951, lat_loss 21.956
09/28 08:10:25 AM | Valid: [ 42/180] Step 312/312 Loss 7.969 Prec@(1,3) (72.5%, 96.8%), ce_loss 0.951, lat_loss 21.956
09/28 08:10:26 AM | val: [ 42/180] Final Prec@1 72.4800% Time 29.27
09/28 08:10:26 AM | Start to train weights for epoch 42
09/28 08:10:52 AM | Train: [ 43/180] Step 050/1249 Loss 6.325 Prec@(1,3) (76.6%, 98.0%), ce_loss 0.950, lat_loss 21.956
09/28 08:11:16 AM | Train: [ 43/180] Step 100/1249 Loss 6.321 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.950, lat_loss 21.956
09/28 08:11:41 AM | Train: [ 43/180] Step 150/1249 Loss 6.335 Prec@(1,3) (76.1%, 98.2%), ce_loss 0.950, lat_loss 21.955
09/28 08:12:06 AM | Train: [ 43/180] Step 200/1249 Loss 6.335 Prec@(1,3) (75.9%, 98.3%), ce_loss 0.950, lat_loss 21.955
09/28 08:12:31 AM | Train: [ 43/180] Step 250/1249 Loss 6.293 Prec@(1,3) (76.2%, 98.3%), ce_loss 0.950, lat_loss 21.955
09/28 08:12:55 AM | Train: [ 43/180] Step 300/1249 Loss 6.328 Prec@(1,3) (76.1%, 98.2%), ce_loss 0.950, lat_loss 21.955
09/28 08:13:20 AM | Train: [ 43/180] Step 350/1249 Loss 6.334 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.950, lat_loss 21.955
09/28 08:13:45 AM | Train: [ 43/180] Step 400/1249 Loss 6.306 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.949, lat_loss 21.955
09/28 08:14:09 AM | Train: [ 43/180] Step 450/1249 Loss 6.297 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.949, lat_loss 21.954
09/28 08:14:34 AM | Train: [ 43/180] Step 500/1249 Loss 6.332 Prec@(1,3) (75.9%, 98.1%), ce_loss 0.949, lat_loss 21.954
09/28 08:14:58 AM | Train: [ 43/180] Step 550/1249 Loss 6.307 Prec@(1,3) (76.0%, 98.0%), ce_loss 0.949, lat_loss 21.954
09/28 08:15:22 AM | Train: [ 43/180] Step 600/1249 Loss 6.286 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.949, lat_loss 21.954
09/28 08:15:47 AM | Train: [ 43/180] Step 650/1249 Loss 6.244 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.949, lat_loss 21.954
09/28 08:16:11 AM | Train: [ 43/180] Step 700/1249 Loss 6.218 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.948, lat_loss 21.953
09/28 08:16:35 AM | Train: [ 43/180] Step 750/1249 Loss 6.187 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.948, lat_loss 21.953
09/28 08:16:59 AM | Train: [ 43/180] Step 800/1249 Loss 6.173 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.948, lat_loss 21.953
09/28 08:17:24 AM | Train: [ 43/180] Step 850/1249 Loss 6.163 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.948, lat_loss 21.953
09/28 08:17:48 AM | Train: [ 43/180] Step 900/1249 Loss 6.166 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.948, lat_loss 21.953
09/28 08:18:12 AM | Train: [ 43/180] Step 950/1249 Loss 6.158 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.948, lat_loss 21.953
09/28 08:18:34 AM | Train: [ 43/180] Step 1000/1249 Loss 6.161 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.947, lat_loss 21.952
09/28 08:18:57 AM | Train: [ 43/180] Step 1050/1249 Loss 6.165 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.947, lat_loss 21.952
09/28 08:19:21 AM | Train: [ 43/180] Step 1100/1249 Loss 6.207 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.947, lat_loss 21.952
09/28 08:19:41 AM | Train: [ 43/180] Step 1150/1249 Loss 6.198 Prec@(1,3) (76.4%, 98.2%), ce_loss 0.947, lat_loss 21.952
09/28 08:20:01 AM | Train: [ 43/180] Step 1200/1249 Loss 6.184 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.947, lat_loss 21.952
09/28 08:20:24 AM | Train: [ 43/180] Step 1249/1249 Loss 6.185 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.947, lat_loss 21.951
09/28 08:20:25 AM | _w_step_train: [ 43/180] Final Prec@1 76.4600% Time 598.99
09/28 08:20:25 AM | Start to train theta for epoch 42
09/28 08:20:46 AM | Train: [ 43/180] Step 050/312 Loss 7.254 Prec@(1,3) (74.4%, 97.7%), ce_loss 0.947, lat_loss 21.951
09/28 08:21:07 AM | Train: [ 43/180] Step 100/312 Loss 7.096 Prec@(1,3) (74.4%, 97.7%), ce_loss 0.946, lat_loss 21.951
09/28 08:21:26 AM | Train: [ 43/180] Step 150/312 Loss 7.063 Prec@(1,3) (74.5%, 97.6%), ce_loss 0.946, lat_loss 21.951
09/28 08:21:46 AM | Train: [ 43/180] Step 200/312 Loss 6.920 Prec@(1,3) (74.6%, 97.9%), ce_loss 0.946, lat_loss 21.951
09/28 08:22:06 AM | Train: [ 43/180] Step 250/312 Loss 6.832 Prec@(1,3) (74.7%, 98.0%), ce_loss 0.946, lat_loss 21.951
09/28 08:22:24 AM | Train: [ 43/180] Step 300/312 Loss 6.808 Prec@(1,3) (74.6%, 98.0%), ce_loss 0.946, lat_loss 21.950
09/28 08:22:29 AM | Train: [ 43/180] Step 312/312 Loss 6.794 Prec@(1,3) (74.6%, 98.0%), ce_loss 0.946, lat_loss 21.950
09/28 08:22:29 AM | _theta_step_train: [ 43/180] Final Prec@1 74.5900% Time 124.95
09/28 08:22:35 AM | Valid: [ 43/180] Step 050/312 Loss 8.368 Prec@(1,3) (69.0%, 97.2%), ce_loss 0.946, lat_loss 21.950
09/28 08:22:39 AM | Valid: [ 43/180] Step 100/312 Loss 8.061 Prec@(1,3) (70.6%, 97.3%), ce_loss 0.946, lat_loss 21.950
09/28 08:22:44 AM | Valid: [ 43/180] Step 150/312 Loss 8.571 Prec@(1,3) (71.0%, 97.0%), ce_loss 0.946, lat_loss 21.950
09/28 08:22:48 AM | Valid: [ 43/180] Step 200/312 Loss 8.344 Prec@(1,3) (71.2%, 97.1%), ce_loss 0.946, lat_loss 21.950
09/28 08:22:53 AM | Valid: [ 43/180] Step 250/312 Loss 8.268 Prec@(1,3) (71.0%, 97.1%), ce_loss 0.946, lat_loss 21.950
09/28 08:22:58 AM | Valid: [ 43/180] Step 300/312 Loss 8.308 Prec@(1,3) (70.8%, 96.9%), ce_loss 0.946, lat_loss 21.950
09/28 08:22:59 AM | Valid: [ 43/180] Step 312/312 Loss 8.322 Prec@(1,3) (70.7%, 97.0%), ce_loss 0.946, lat_loss 21.949
09/28 08:22:59 AM | val: [ 43/180] Final Prec@1 70.6900% Time 29.17
09/28 08:22:59 AM | Start to train weights for epoch 43
09/28 08:23:25 AM | Train: [ 44/180] Step 050/1249 Loss 6.443 Prec@(1,3) (76.3%, 97.9%), ce_loss 0.946, lat_loss 21.949
09/28 08:23:50 AM | Train: [ 44/180] Step 100/1249 Loss 6.268 Prec@(1,3) (75.9%, 98.2%), ce_loss 0.946, lat_loss 21.949
09/28 08:24:14 AM | Train: [ 44/180] Step 150/1249 Loss 6.222 Prec@(1,3) (75.8%, 98.3%), ce_loss 0.945, lat_loss 21.949
09/28 08:24:39 AM | Train: [ 44/180] Step 200/1249 Loss 6.157 Prec@(1,3) (76.2%, 98.3%), ce_loss 0.945, lat_loss 21.949
09/28 08:25:03 AM | Train: [ 44/180] Step 250/1249 Loss 6.104 Prec@(1,3) (76.6%, 98.3%), ce_loss 0.945, lat_loss 21.949
09/28 08:25:27 AM | Train: [ 44/180] Step 300/1249 Loss 6.157 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.945, lat_loss 21.949
09/28 08:25:51 AM | Train: [ 44/180] Step 350/1249 Loss 6.203 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.945, lat_loss 21.948
09/28 08:26:15 AM | Train: [ 44/180] Step 400/1249 Loss 6.156 Prec@(1,3) (76.3%, 98.3%), ce_loss 0.945, lat_loss 21.948
09/28 08:26:38 AM | Train: [ 44/180] Step 450/1249 Loss 6.143 Prec@(1,3) (76.3%, 98.3%), ce_loss 0.945, lat_loss 21.948
09/28 08:27:01 AM | Train: [ 44/180] Step 500/1249 Loss 6.191 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.944, lat_loss 21.948
09/28 08:27:25 AM | Train: [ 44/180] Step 550/1249 Loss 6.186 Prec@(1,3) (76.3%, 98.3%), ce_loss 0.944, lat_loss 21.948
09/28 08:27:49 AM | Train: [ 44/180] Step 600/1249 Loss 6.167 Prec@(1,3) (76.4%, 98.3%), ce_loss 0.944, lat_loss 21.948
09/28 08:28:10 AM | Train: [ 44/180] Step 650/1249 Loss 6.148 Prec@(1,3) (76.5%, 98.3%), ce_loss 0.944, lat_loss 21.948
09/28 08:28:33 AM | Train: [ 44/180] Step 700/1249 Loss 6.130 Prec@(1,3) (76.6%, 98.3%), ce_loss 0.944, lat_loss 21.947
09/28 08:28:56 AM | Train: [ 44/180] Step 750/1249 Loss 6.121 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.944, lat_loss 21.947
09/28 08:29:19 AM | Train: [ 44/180] Step 800/1249 Loss 6.119 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.943, lat_loss 21.947
09/28 08:29:43 AM | Train: [ 44/180] Step 850/1249 Loss 6.126 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.943, lat_loss 21.947
09/28 08:30:06 AM | Train: [ 44/180] Step 900/1249 Loss 6.108 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.943, lat_loss 21.947
09/28 08:30:28 AM | Train: [ 44/180] Step 950/1249 Loss 6.139 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.943, lat_loss 21.947
09/28 08:30:52 AM | Train: [ 44/180] Step 1000/1249 Loss 6.161 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.943, lat_loss 21.946
09/28 08:31:15 AM | Train: [ 44/180] Step 1050/1249 Loss 6.159 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.943, lat_loss 21.946
09/28 08:31:39 AM | Train: [ 44/180] Step 1100/1249 Loss 6.173 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.943, lat_loss 21.946
09/28 08:32:03 AM | Train: [ 44/180] Step 1150/1249 Loss 6.172 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.942, lat_loss 21.946
09/28 08:32:26 AM | Train: [ 44/180] Step 1200/1249 Loss 6.165 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.942, lat_loss 21.946
09/28 08:32:50 AM | Train: [ 44/180] Step 1249/1249 Loss 6.166 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.942, lat_loss 21.946
09/28 08:32:50 AM | _w_step_train: [ 44/180] Final Prec@1 76.7125% Time 591.52
09/28 08:32:50 AM | Start to train theta for epoch 43
09/28 08:33:11 AM | Train: [ 44/180] Step 050/312 Loss 6.296 Prec@(1,3) (76.7%, 97.8%), ce_loss 0.942, lat_loss 21.946
09/28 08:33:31 AM | Train: [ 44/180] Step 100/312 Loss 6.224 Prec@(1,3) (77.0%, 97.9%), ce_loss 0.942, lat_loss 21.945
09/28 08:33:51 AM | Train: [ 44/180] Step 150/312 Loss 6.159 Prec@(1,3) (77.0%, 98.1%), ce_loss 0.942, lat_loss 21.945
09/28 08:34:11 AM | Train: [ 44/180] Step 200/312 Loss 6.103 Prec@(1,3) (77.0%, 98.2%), ce_loss 0.941, lat_loss 21.945
09/28 08:34:31 AM | Train: [ 44/180] Step 250/312 Loss 6.137 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.941, lat_loss 21.945
09/28 08:34:49 AM | Train: [ 44/180] Step 300/312 Loss 6.105 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.941, lat_loss 21.945
09/28 08:34:53 AM | Train: [ 44/180] Step 312/312 Loss 6.125 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.941, lat_loss 21.945
09/28 08:34:53 AM | _theta_step_train: [ 44/180] Final Prec@1 76.8500% Time 123.23
09/28 08:34:59 AM | Valid: [ 44/180] Step 050/312 Loss 7.489 Prec@(1,3) (71.0%, 97.0%), ce_loss 0.941, lat_loss 21.945
09/28 08:35:03 AM | Valid: [ 44/180] Step 100/312 Loss 7.564 Prec@(1,3) (71.2%, 97.0%), ce_loss 0.941, lat_loss 21.945
09/28 08:35:08 AM | Valid: [ 44/180] Step 150/312 Loss 7.887 Prec@(1,3) (70.2%, 96.5%), ce_loss 0.941, lat_loss 21.944
09/28 08:35:13 AM | Valid: [ 44/180] Step 200/312 Loss 7.854 Prec@(1,3) (70.3%, 96.5%), ce_loss 0.941, lat_loss 21.944
09/28 08:35:17 AM | Valid: [ 44/180] Step 250/312 Loss 7.838 Prec@(1,3) (70.4%, 96.7%), ce_loss 0.941, lat_loss 21.944
09/28 08:35:22 AM | Valid: [ 44/180] Step 300/312 Loss 7.734 Prec@(1,3) (70.9%, 96.7%), ce_loss 0.941, lat_loss 21.944
09/28 08:35:23 AM | Valid: [ 44/180] Step 312/312 Loss 7.732 Prec@(1,3) (70.9%, 96.7%), ce_loss 0.941, lat_loss 21.944
09/28 08:35:23 AM | val: [ 44/180] Final Prec@1 70.8700% Time 29.77
09/28 08:35:23 AM | Start to train weights for epoch 44
09/28 08:35:41 AM | Train: [ 45/180] Step 050/1249 Loss 5.879 Prec@(1,3) (78.7%, 98.5%), ce_loss 0.941, lat_loss 21.944
09/28 08:35:56 AM | Train: [ 45/180] Step 100/1249 Loss 6.013 Prec@(1,3) (77.8%, 98.5%), ce_loss 0.941, lat_loss 21.944
09/28 08:36:12 AM | Train: [ 45/180] Step 150/1249 Loss 6.264 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.940, lat_loss 21.944
09/28 08:36:28 AM | Train: [ 45/180] Step 200/1249 Loss 6.190 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.940, lat_loss 21.944
09/28 08:36:44 AM | Train: [ 45/180] Step 250/1249 Loss 6.143 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.940, lat_loss 21.943
09/28 08:37:00 AM | Train: [ 45/180] Step 300/1249 Loss 6.141 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.940, lat_loss 21.943
09/28 08:37:16 AM | Train: [ 45/180] Step 350/1249 Loss 6.185 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.940, lat_loss 21.943
09/28 08:37:32 AM | Train: [ 45/180] Step 400/1249 Loss 6.152 Prec@(1,3) (76.6%, 98.3%), ce_loss 0.940, lat_loss 21.943
09/28 08:37:48 AM | Train: [ 45/180] Step 450/1249 Loss 6.139 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.939, lat_loss 21.943
09/28 08:38:03 AM | Train: [ 45/180] Step 500/1249 Loss 6.124 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.939, lat_loss 21.943
09/28 08:38:19 AM | Train: [ 45/180] Step 550/1249 Loss 6.109 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.939, lat_loss 21.943
09/28 08:38:35 AM | Train: [ 45/180] Step 600/1249 Loss 6.110 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.939, lat_loss 21.943
09/28 08:38:51 AM | Train: [ 45/180] Step 650/1249 Loss 6.064 Prec@(1,3) (76.8%, 98.4%), ce_loss 0.939, lat_loss 21.942
09/28 08:39:07 AM | Train: [ 45/180] Step 700/1249 Loss 6.104 Prec@(1,3) (76.6%, 98.3%), ce_loss 0.939, lat_loss 21.942
09/28 08:39:23 AM | Train: [ 45/180] Step 750/1249 Loss 6.067 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.938, lat_loss 21.942
09/28 08:39:39 AM | Train: [ 45/180] Step 800/1249 Loss 6.069 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.938, lat_loss 21.942
09/28 08:39:55 AM | Train: [ 45/180] Step 850/1249 Loss 6.043 Prec@(1,3) (76.9%, 98.3%), ce_loss 0.938, lat_loss 21.942
09/28 08:40:11 AM | Train: [ 45/180] Step 900/1249 Loss 6.041 Prec@(1,3) (76.9%, 98.3%), ce_loss 0.938, lat_loss 21.942
09/28 08:40:26 AM | Train: [ 45/180] Step 950/1249 Loss 6.036 Prec@(1,3) (77.0%, 98.3%), ce_loss 0.938, lat_loss 21.942
09/28 08:40:42 AM | Train: [ 45/180] Step 1000/1249 Loss 6.065 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.938, lat_loss 21.942
09/28 08:40:58 AM | Train: [ 45/180] Step 1050/1249 Loss 6.060 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.938, lat_loss 21.942
09/28 08:41:14 AM | Train: [ 45/180] Step 1100/1249 Loss 6.074 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.937, lat_loss 21.941
09/28 08:41:36 AM | Train: [ 45/180] Step 1150/1249 Loss 6.094 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.937, lat_loss 21.941
09/28 08:42:00 AM | Train: [ 45/180] Step 1200/1249 Loss 6.108 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.937, lat_loss 21.941
09/28 08:42:24 AM | Train: [ 45/180] Step 1249/1249 Loss 6.116 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.937, lat_loss 21.941
09/28 08:42:24 AM | _w_step_train: [ 45/180] Final Prec@1 76.6800% Time 421.14
09/28 08:42:24 AM | Start to train theta for epoch 44
09/28 08:42:46 AM | Train: [ 45/180] Step 050/312 Loss 6.203 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.937, lat_loss 21.941
09/28 08:43:05 AM | Train: [ 45/180] Step 100/312 Loss 6.111 Prec@(1,3) (76.1%, 98.5%), ce_loss 0.937, lat_loss 21.941
09/28 08:43:24 AM | Train: [ 45/180] Step 150/312 Loss 6.312 Prec@(1,3) (76.0%, 98.2%), ce_loss 0.937, lat_loss 21.941
09/28 08:43:43 AM | Train: [ 45/180] Step 200/312 Loss 6.269 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.936, lat_loss 21.941
09/28 08:44:00 AM | Train: [ 45/180] Step 250/312 Loss 6.379 Prec@(1,3) (76.0%, 98.1%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:19 AM | Train: [ 45/180] Step 300/312 Loss 6.361 Prec@(1,3) (76.2%, 98.0%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:24 AM | Train: [ 45/180] Step 312/312 Loss 6.328 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:24 AM | _theta_step_train: [ 45/180] Final Prec@1 76.3100% Time 119.87
09/28 08:44:30 AM | Valid: [ 45/180] Step 050/312 Loss 7.056 Prec@(1,3) (74.5%, 97.7%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:34 AM | Valid: [ 45/180] Step 100/312 Loss 7.595 Prec@(1,3) (72.4%, 97.2%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:39 AM | Valid: [ 45/180] Step 150/312 Loss 7.315 Prec@(1,3) (73.5%, 97.4%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:43 AM | Valid: [ 45/180] Step 200/312 Loss 7.403 Prec@(1,3) (73.1%, 97.4%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:48 AM | Valid: [ 45/180] Step 250/312 Loss 7.509 Prec@(1,3) (72.7%, 97.3%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:53 AM | Valid: [ 45/180] Step 300/312 Loss 7.363 Prec@(1,3) (73.2%, 97.5%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:54 AM | Valid: [ 45/180] Step 312/312 Loss 7.351 Prec@(1,3) (73.3%, 97.5%), ce_loss 0.936, lat_loss 21.940
09/28 08:44:54 AM | val: [ 45/180] Final Prec@1 73.2600% Time 29.86
09/28 08:44:54 AM | Best top1 acc by now. Save model
09/28 08:44:54 AM | Start to train weights for epoch 45
09/28 08:45:17 AM | Train: [ 46/180] Step 050/1249 Loss 5.833 Prec@(1,3) (77.5%, 98.6%), ce_loss 0.936, lat_loss 21.939
09/28 08:45:37 AM | Train: [ 46/180] Step 100/1249 Loss 5.962 Prec@(1,3) (77.1%, 98.5%), ce_loss 0.935, lat_loss 21.939
09/28 08:46:02 AM | Train: [ 46/180] Step 150/1249 Loss 5.959 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.935, lat_loss 21.939
09/28 08:46:26 AM | Train: [ 46/180] Step 200/1249 Loss 5.994 Prec@(1,3) (77.0%, 98.4%), ce_loss 0.935, lat_loss 21.939
09/28 08:46:50 AM | Train: [ 46/180] Step 250/1249 Loss 5.975 Prec@(1,3) (77.0%, 98.4%), ce_loss 0.935, lat_loss 21.939
09/28 08:47:14 AM | Train: [ 46/180] Step 300/1249 Loss 5.868 Prec@(1,3) (77.6%, 98.4%), ce_loss 0.935, lat_loss 21.939
09/28 08:47:37 AM | Train: [ 46/180] Step 350/1249 Loss 5.899 Prec@(1,3) (77.5%, 98.4%), ce_loss 0.935, lat_loss 21.939
09/28 08:48:01 AM | Train: [ 46/180] Step 400/1249 Loss 5.871 Prec@(1,3) (77.5%, 98.4%), ce_loss 0.934, lat_loss 21.939
09/28 08:48:25 AM | Train: [ 46/180] Step 450/1249 Loss 5.897 Prec@(1,3) (77.5%, 98.4%), ce_loss 0.934, lat_loss 21.938
09/28 08:48:50 AM | Train: [ 46/180] Step 500/1249 Loss 5.920 Prec@(1,3) (77.5%, 98.4%), ce_loss 0.934, lat_loss 21.938
09/28 08:49:14 AM | Train: [ 46/180] Step 550/1249 Loss 5.971 Prec@(1,3) (77.3%, 98.3%), ce_loss 0.934, lat_loss 21.938
09/28 08:49:39 AM | Train: [ 46/180] Step 600/1249 Loss 5.946 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.934, lat_loss 21.938
09/28 08:50:04 AM | Train: [ 46/180] Step 650/1249 Loss 5.952 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.934, lat_loss 21.938
09/28 08:50:28 AM | Train: [ 46/180] Step 700/1249 Loss 5.950 Prec@(1,3) (77.3%, 98.4%), ce_loss 0.934, lat_loss 21.938
09/28 08:50:53 AM | Train: [ 46/180] Step 750/1249 Loss 5.963 Prec@(1,3) (77.3%, 98.3%), ce_loss 0.933, lat_loss 21.938
09/28 08:51:17 AM | Train: [ 46/180] Step 800/1249 Loss 5.973 Prec@(1,3) (77.3%, 98.3%), ce_loss 0.933, lat_loss 21.938
09/28 08:51:42 AM | Train: [ 46/180] Step 850/1249 Loss 5.986 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.933, lat_loss 21.937
09/28 08:52:05 AM | Train: [ 46/180] Step 900/1249 Loss 5.980 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.933, lat_loss 21.937
09/28 08:52:28 AM | Train: [ 46/180] Step 950/1249 Loss 5.969 Prec@(1,3) (77.3%, 98.4%), ce_loss 0.933, lat_loss 21.937
09/28 08:52:51 AM | Train: [ 46/180] Step 1000/1249 Loss 5.981 Prec@(1,3) (77.2%, 98.4%), ce_loss 0.933, lat_loss 21.937
09/28 08:53:15 AM | Train: [ 46/180] Step 1050/1249 Loss 5.993 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.933, lat_loss 21.937
09/28 08:53:40 AM | Train: [ 46/180] Step 1100/1249 Loss 6.000 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.932, lat_loss 21.937
09/28 08:54:04 AM | Train: [ 46/180] Step 1150/1249 Loss 6.011 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.932, lat_loss 21.937
09/28 08:54:29 AM | Train: [ 46/180] Step 1200/1249 Loss 5.998 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.932, lat_loss 21.936
09/28 08:54:53 AM | Train: [ 46/180] Step 1249/1249 Loss 6.002 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.932, lat_loss 21.936
09/28 08:54:53 AM | _w_step_train: [ 46/180] Final Prec@1 77.1675% Time 599.09
09/28 08:54:53 AM | Start to train theta for epoch 45
09/28 08:55:12 AM | Train: [ 46/180] Step 050/312 Loss 6.025 Prec@(1,3) (76.5%, 98.6%), ce_loss 0.932, lat_loss 21.936
09/28 08:55:30 AM | Train: [ 46/180] Step 100/312 Loss 6.204 Prec@(1,3) (76.5%, 98.1%), ce_loss 0.932, lat_loss 21.936
09/28 08:55:50 AM | Train: [ 46/180] Step 150/312 Loss 6.152 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.932, lat_loss 21.936
09/28 08:56:09 AM | Train: [ 46/180] Step 200/312 Loss 6.241 Prec@(1,3) (76.4%, 98.1%), ce_loss 0.931, lat_loss 21.936
09/28 08:56:29 AM | Train: [ 46/180] Step 250/312 Loss 6.240 Prec@(1,3) (76.4%, 98.0%), ce_loss 0.931, lat_loss 21.936
09/28 08:56:49 AM | Train: [ 46/180] Step 300/312 Loss 6.295 Prec@(1,3) (76.1%, 98.0%), ce_loss 0.931, lat_loss 21.936
09/28 08:56:54 AM | Train: [ 46/180] Step 312/312 Loss 6.284 Prec@(1,3) (76.3%, 98.0%), ce_loss 0.931, lat_loss 21.936
09/28 08:56:54 AM | _theta_step_train: [ 46/180] Final Prec@1 76.3000% Time 120.33
09/28 08:57:00 AM | Valid: [ 46/180] Step 050/312 Loss 7.206 Prec@(1,3) (72.7%, 97.2%), ce_loss 0.931, lat_loss 21.935
09/28 08:57:04 AM | Valid: [ 46/180] Step 100/312 Loss 7.379 Prec@(1,3) (72.0%, 96.8%), ce_loss 0.931, lat_loss 21.935
09/28 08:57:09 AM | Valid: [ 46/180] Step 150/312 Loss 7.140 Prec@(1,3) (73.4%, 97.0%), ce_loss 0.931, lat_loss 21.935
09/28 08:57:13 AM | Valid: [ 46/180] Step 200/312 Loss 7.069 Prec@(1,3) (73.5%, 97.4%), ce_loss 0.931, lat_loss 21.935
09/28 08:57:18 AM | Valid: [ 46/180] Step 250/312 Loss 6.973 Prec@(1,3) (73.8%, 97.6%), ce_loss 0.931, lat_loss 21.935
09/28 08:57:23 AM | Valid: [ 46/180] Step 300/312 Loss 7.001 Prec@(1,3) (73.8%, 97.5%), ce_loss 0.931, lat_loss 21.935
09/28 08:57:24 AM | Valid: [ 46/180] Step 312/312 Loss 7.003 Prec@(1,3) (73.8%, 97.5%), ce_loss 0.931, lat_loss 21.935
09/28 08:57:24 AM | val: [ 46/180] Final Prec@1 73.7700% Time 29.96
09/28 08:57:24 AM | Best top1 acc by now. Save model
09/28 08:57:24 AM | Start to train weights for epoch 46
09/28 08:57:49 AM | Train: [ 47/180] Step 050/1249 Loss 5.651 Prec@(1,3) (77.9%, 98.6%), ce_loss 0.930, lat_loss 21.935
09/28 08:58:13 AM | Train: [ 47/180] Step 100/1249 Loss 5.549 Prec@(1,3) (79.2%, 98.2%), ce_loss 0.930, lat_loss 21.935
09/28 08:58:36 AM | Train: [ 47/180] Step 150/1249 Loss 5.505 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.930, lat_loss 21.934
09/28 08:59:00 AM | Train: [ 47/180] Step 200/1249 Loss 5.713 Prec@(1,3) (78.3%, 98.5%), ce_loss 0.930, lat_loss 21.934
09/28 08:59:25 AM | Train: [ 47/180] Step 250/1249 Loss 5.807 Prec@(1,3) (77.8%, 98.5%), ce_loss 0.930, lat_loss 21.934
09/28 08:59:49 AM | Train: [ 47/180] Step 300/1249 Loss 5.938 Prec@(1,3) (77.4%, 98.4%), ce_loss 0.930, lat_loss 21.934
09/28 09:00:14 AM | Train: [ 47/180] Step 350/1249 Loss 5.956 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.930, lat_loss 21.934
09/28 09:00:39 AM | Train: [ 47/180] Step 400/1249 Loss 5.955 Prec@(1,3) (77.3%, 98.3%), ce_loss 0.929, lat_loss 21.934
09/28 09:01:03 AM | Train: [ 47/180] Step 450/1249 Loss 5.907 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.929, lat_loss 21.934
09/28 09:01:28 AM | Train: [ 47/180] Step 500/1249 Loss 5.905 Prec@(1,3) (77.5%, 98.3%), ce_loss 0.929, lat_loss 21.934
09/28 09:01:52 AM | Train: [ 47/180] Step 550/1249 Loss 5.950 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.929, lat_loss 21.933
09/28 09:02:17 AM | Train: [ 47/180] Step 600/1249 Loss 5.946 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.929, lat_loss 21.933
09/28 09:02:41 AM | Train: [ 47/180] Step 650/1249 Loss 5.959 Prec@(1,3) (77.3%, 98.3%), ce_loss 0.929, lat_loss 21.933
09/28 09:03:05 AM | Train: [ 47/180] Step 700/1249 Loss 5.959 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.928, lat_loss 21.933
09/28 09:03:30 AM | Train: [ 47/180] Step 750/1249 Loss 5.975 Prec@(1,3) (77.4%, 98.2%), ce_loss 0.928, lat_loss 21.933
09/28 09:03:54 AM | Train: [ 47/180] Step 800/1249 Loss 5.980 Prec@(1,3) (77.4%, 98.2%), ce_loss 0.928, lat_loss 21.933
09/28 09:04:18 AM | Train: [ 47/180] Step 850/1249 Loss 5.992 Prec@(1,3) (77.3%, 98.2%), ce_loss 0.928, lat_loss 21.933
09/28 09:04:42 AM | Train: [ 47/180] Step 900/1249 Loss 5.982 Prec@(1,3) (77.3%, 98.2%), ce_loss 0.928, lat_loss 21.933
09/28 09:05:06 AM | Train: [ 47/180] Step 950/1249 Loss 6.008 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.928, lat_loss 21.932
09/28 09:05:31 AM | Train: [ 47/180] Step 1000/1249 Loss 6.021 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.928, lat_loss 21.932
09/28 09:05:55 AM | Train: [ 47/180] Step 1050/1249 Loss 6.021 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.928, lat_loss 21.932
09/28 09:06:19 AM | Train: [ 47/180] Step 1100/1249 Loss 6.020 Prec@(1,3) (77.3%, 98.1%), ce_loss 0.927, lat_loss 21.932
09/28 09:06:44 AM | Train: [ 47/180] Step 1150/1249 Loss 6.023 Prec@(1,3) (77.2%, 98.1%), ce_loss 0.927, lat_loss 21.932
09/28 09:07:08 AM | Train: [ 47/180] Step 1200/1249 Loss 6.030 Prec@(1,3) (77.2%, 98.1%), ce_loss 0.927, lat_loss 21.932
09/28 09:07:32 AM | Train: [ 47/180] Step 1249/1249 Loss 6.035 Prec@(1,3) (77.2%, 98.1%), ce_loss 0.927, lat_loss 21.932
09/28 09:07:32 AM | _w_step_train: [ 47/180] Final Prec@1 77.2400% Time 607.72
09/28 09:07:32 AM | Start to train theta for epoch 46
09/28 09:07:53 AM | Train: [ 47/180] Step 050/312 Loss 6.407 Prec@(1,3) (76.4%, 97.9%), ce_loss 0.927, lat_loss 21.932
09/28 09:08:14 AM | Train: [ 47/180] Step 100/312 Loss 6.524 Prec@(1,3) (75.6%, 98.0%), ce_loss 0.927, lat_loss 21.931
09/28 09:08:35 AM | Train: [ 47/180] Step 150/312 Loss 6.613 Prec@(1,3) (75.4%, 97.9%), ce_loss 0.927, lat_loss 21.931
09/28 09:08:52 AM | Train: [ 47/180] Step 200/312 Loss 6.626 Prec@(1,3) (75.3%, 98.0%), ce_loss 0.927, lat_loss 21.931
09/28 09:09:12 AM | Train: [ 47/180] Step 250/312 Loss 6.585 Prec@(1,3) (75.4%, 98.0%), ce_loss 0.926, lat_loss 21.931
09/28 09:09:33 AM | Train: [ 47/180] Step 300/312 Loss 6.600 Prec@(1,3) (75.4%, 98.0%), ce_loss 0.926, lat_loss 21.931
09/28 09:09:38 AM | Train: [ 47/180] Step 312/312 Loss 6.566 Prec@(1,3) (75.4%, 98.0%), ce_loss 0.926, lat_loss 21.931
09/28 09:09:38 AM | _theta_step_train: [ 47/180] Final Prec@1 75.4100% Time 126.42
09/28 09:09:44 AM | Valid: [ 47/180] Step 050/312 Loss 7.337 Prec@(1,3) (72.2%, 97.2%), ce_loss 0.926, lat_loss 21.931
09/28 09:09:48 AM | Valid: [ 47/180] Step 100/312 Loss 7.785 Prec@(1,3) (71.4%, 96.3%), ce_loss 0.926, lat_loss 21.931
09/28 09:09:53 AM | Valid: [ 47/180] Step 150/312 Loss 7.800 Prec@(1,3) (72.0%, 96.6%), ce_loss 0.926, lat_loss 21.931
09/28 09:09:57 AM | Valid: [ 47/180] Step 200/312 Loss 9.734 Prec@(1,3) (71.9%, 96.7%), ce_loss 0.927, lat_loss 21.931
09/28 09:10:02 AM | Valid: [ 47/180] Step 250/312 Loss 9.347 Prec@(1,3) (71.9%, 96.6%), ce_loss 0.927, lat_loss 21.930
09/28 09:10:07 AM | Valid: [ 47/180] Step 300/312 Loss 8.961 Prec@(1,3) (72.5%, 96.7%), ce_loss 0.927, lat_loss 21.930
09/28 09:10:08 AM | Valid: [ 47/180] Step 312/312 Loss 8.949 Prec@(1,3) (72.2%, 96.7%), ce_loss 0.927, lat_loss 21.930
09/28 09:10:08 AM | val: [ 47/180] Final Prec@1 72.2300% Time 29.38
09/28 09:10:08 AM | Start to train weights for epoch 47
09/28 09:10:31 AM | Train: [ 48/180] Step 050/1249 Loss 6.239 Prec@(1,3) (75.9%, 98.8%), ce_loss 0.927, lat_loss 21.930
09/28 09:10:52 AM | Train: [ 48/180] Step 100/1249 Loss 6.273 Prec@(1,3) (76.3%, 98.3%), ce_loss 0.926, lat_loss 21.930
09/28 09:11:16 AM | Train: [ 48/180] Step 150/1249 Loss 6.220 Prec@(1,3) (76.5%, 98.3%), ce_loss 0.926, lat_loss 21.930
09/28 09:11:39 AM | Train: [ 48/180] Step 200/1249 Loss 6.098 Prec@(1,3) (77.1%, 98.3%), ce_loss 0.926, lat_loss 21.930
09/28 09:12:02 AM | Train: [ 48/180] Step 250/1249 Loss 6.144 Prec@(1,3) (77.0%, 98.3%), ce_loss 0.926, lat_loss 21.930
09/28 09:12:26 AM | Train: [ 48/180] Step 300/1249 Loss 6.141 Prec@(1,3) (76.9%, 98.2%), ce_loss 0.926, lat_loss 21.930
09/28 09:12:49 AM | Train: [ 48/180] Step 350/1249 Loss 6.085 Prec@(1,3) (77.0%, 98.3%), ce_loss 0.926, lat_loss 21.930
09/28 09:13:12 AM | Train: [ 48/180] Step 400/1249 Loss 6.152 Prec@(1,3) (76.6%, 98.3%), ce_loss 0.926, lat_loss 21.929
09/28 09:13:32 AM | Train: [ 48/180] Step 450/1249 Loss 6.178 Prec@(1,3) (76.6%, 98.3%), ce_loss 0.926, lat_loss 21.929
09/28 09:13:53 AM | Train: [ 48/180] Step 500/1249 Loss 6.169 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.925, lat_loss 21.929
09/28 09:14:14 AM | Train: [ 48/180] Step 550/1249 Loss 6.167 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.925, lat_loss 21.929
09/28 09:14:34 AM | Train: [ 48/180] Step 600/1249 Loss 6.162 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.925, lat_loss 21.929
09/28 09:14:55 AM | Train: [ 48/180] Step 650/1249 Loss 6.157 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.925, lat_loss 21.929
09/28 09:15:19 AM | Train: [ 48/180] Step 700/1249 Loss 6.163 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.925, lat_loss 21.929
09/28 09:15:42 AM | Train: [ 48/180] Step 750/1249 Loss 6.156 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.925, lat_loss 21.929
09/28 09:16:06 AM | Train: [ 48/180] Step 800/1249 Loss 6.164 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.925, lat_loss 21.929
09/28 09:16:29 AM | Train: [ 48/180] Step 850/1249 Loss 6.158 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.924, lat_loss 21.928
09/28 09:16:53 AM | Train: [ 48/180] Step 900/1249 Loss 6.164 Prec@(1,3) (76.6%, 98.3%), ce_loss 0.924, lat_loss 21.928
09/28 09:17:17 AM | Train: [ 48/180] Step 950/1249 Loss 6.150 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.924, lat_loss 21.928
09/28 09:17:41 AM | Train: [ 48/180] Step 1000/1249 Loss 6.144 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.924, lat_loss 21.928
09/28 09:18:03 AM | Train: [ 48/180] Step 1050/1249 Loss 6.133 Prec@(1,3) (76.8%, 98.3%), ce_loss 0.924, lat_loss 21.928
09/28 09:18:27 AM | Train: [ 48/180] Step 1100/1249 Loss 6.155 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.924, lat_loss 21.928
09/28 09:18:50 AM | Train: [ 48/180] Step 1150/1249 Loss 6.139 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.924, lat_loss 21.928
09/28 09:19:14 AM | Train: [ 48/180] Step 1200/1249 Loss 6.159 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.924, lat_loss 21.928
09/28 09:19:37 AM | Train: [ 48/180] Step 1249/1249 Loss 6.150 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.923, lat_loss 21.928
09/28 09:19:37 AM | _w_step_train: [ 48/180] Final Prec@1 76.7175% Time 569.46
09/28 09:19:37 AM | Start to train theta for epoch 47
09/28 09:19:57 AM | Train: [ 48/180] Step 050/312 Loss 5.699 Prec@(1,3) (79.2%, 98.3%), ce_loss 0.923, lat_loss 21.927
09/28 09:20:16 AM | Train: [ 48/180] Step 100/312 Loss 6.188 Prec@(1,3) (77.0%, 98.0%), ce_loss 0.923, lat_loss 21.927
09/28 09:20:34 AM | Train: [ 48/180] Step 150/312 Loss 6.366 Prec@(1,3) (76.4%, 98.1%), ce_loss 0.923, lat_loss 21.927
09/28 09:20:53 AM | Train: [ 48/180] Step 200/312 Loss 6.415 Prec@(1,3) (76.3%, 98.1%), ce_loss 0.923, lat_loss 21.927
09/28 09:21:14 AM | Train: [ 48/180] Step 250/312 Loss 6.405 Prec@(1,3) (76.2%, 98.1%), ce_loss 0.923, lat_loss 21.927
09/28 09:21:33 AM | Train: [ 48/180] Step 300/312 Loss 6.441 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.923, lat_loss 21.927
09/28 09:21:38 AM | Train: [ 48/180] Step 312/312 Loss 6.420 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.923, lat_loss 21.927
09/28 09:21:38 AM | _theta_step_train: [ 48/180] Final Prec@1 76.1900% Time 120.50
09/28 09:21:44 AM | Valid: [ 48/180] Step 050/312 Loss 7.098 Prec@(1,3) (74.5%, 97.2%), ce_loss 0.923, lat_loss 21.927
09/28 09:21:48 AM | Valid: [ 48/180] Step 100/312 Loss 7.485 Prec@(1,3) (72.9%, 96.8%), ce_loss 0.923, lat_loss 21.927
09/28 09:21:53 AM | Valid: [ 48/180] Step 150/312 Loss 7.570 Prec@(1,3) (72.6%, 96.5%), ce_loss 0.923, lat_loss 21.926
09/28 09:21:58 AM | Valid: [ 48/180] Step 200/312 Loss 7.544 Prec@(1,3) (72.9%, 96.7%), ce_loss 0.923, lat_loss 21.926
09/28 09:22:02 AM | Valid: [ 48/180] Step 250/312 Loss 7.398 Prec@(1,3) (73.2%, 97.0%), ce_loss 0.923, lat_loss 21.926
09/28 09:22:07 AM | Valid: [ 48/180] Step 300/312 Loss 7.360 Prec@(1,3) (73.2%, 97.0%), ce_loss 0.922, lat_loss 21.926
09/28 09:22:08 AM | Valid: [ 48/180] Step 312/312 Loss 7.381 Prec@(1,3) (73.1%, 97.0%), ce_loss 0.922, lat_loss 21.926
09/28 09:22:08 AM | val: [ 48/180] Final Prec@1 73.0800% Time 30.08
09/28 09:22:08 AM | Start to train weights for epoch 48
09/28 09:22:34 AM | Train: [ 49/180] Step 050/1249 Loss 6.354 Prec@(1,3) (75.1%, 98.3%), ce_loss 0.922, lat_loss 21.926
09/28 09:22:57 AM | Train: [ 49/180] Step 100/1249 Loss 6.380 Prec@(1,3) (75.4%, 98.1%), ce_loss 0.922, lat_loss 21.926
09/28 09:23:17 AM | Train: [ 49/180] Step 150/1249 Loss 6.160 Prec@(1,3) (76.2%, 98.3%), ce_loss 0.922, lat_loss 21.926
09/28 09:23:33 AM | Train: [ 49/180] Step 200/1249 Loss 6.051 Prec@(1,3) (76.9%, 98.3%), ce_loss 0.922, lat_loss 21.926
09/28 09:23:49 AM | Train: [ 49/180] Step 250/1249 Loss 6.140 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.922, lat_loss 21.925
09/28 09:24:05 AM | Train: [ 49/180] Step 300/1249 Loss 6.118 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.922, lat_loss 21.925
09/28 09:24:21 AM | Train: [ 49/180] Step 350/1249 Loss 6.138 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.922, lat_loss 21.925
09/28 09:24:37 AM | Train: [ 49/180] Step 400/1249 Loss 6.120 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.921, lat_loss 21.925
09/28 09:24:53 AM | Train: [ 49/180] Step 450/1249 Loss 6.112 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.921, lat_loss 21.925
09/28 09:25:09 AM | Train: [ 49/180] Step 500/1249 Loss 6.102 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.921, lat_loss 21.925
09/28 09:25:24 AM | Train: [ 49/180] Step 550/1249 Loss 6.118 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.921, lat_loss 21.925
09/28 09:25:40 AM | Train: [ 49/180] Step 600/1249 Loss 6.107 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.921, lat_loss 21.925
09/28 09:26:02 AM | Train: [ 49/180] Step 650/1249 Loss 6.091 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.921, lat_loss 21.924
09/28 09:26:26 AM | Train: [ 49/180] Step 700/1249 Loss 6.071 Prec@(1,3) (76.9%, 98.2%), ce_loss 0.921, lat_loss 21.924
09/28 09:26:50 AM | Train: [ 49/180] Step 750/1249 Loss 6.081 Prec@(1,3) (76.8%, 98.1%), ce_loss 0.920, lat_loss 21.924
09/28 09:27:14 AM | Train: [ 49/180] Step 800/1249 Loss 6.069 Prec@(1,3) (76.9%, 98.1%), ce_loss 0.920, lat_loss 21.924
09/28 09:27:39 AM | Train: [ 49/180] Step 850/1249 Loss 6.054 Prec@(1,3) (77.0%, 98.2%), ce_loss 0.920, lat_loss 21.924
09/28 09:28:02 AM | Train: [ 49/180] Step 900/1249 Loss 6.027 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.920, lat_loss 21.924
09/28 09:28:26 AM | Train: [ 49/180] Step 950/1249 Loss 6.026 Prec@(1,3) (77.1%, 98.2%), ce_loss 0.920, lat_loss 21.924
09/28 09:28:51 AM | Train: [ 49/180] Step 1000/1249 Loss 6.028 Prec@(1,3) (77.1%, 98.2%), ce_loss 0.920, lat_loss 21.924
09/28 09:29:16 AM | Train: [ 49/180] Step 1050/1249 Loss 6.036 Prec@(1,3) (77.1%, 98.2%), ce_loss 0.920, lat_loss 21.924
09/28 09:29:41 AM | Train: [ 49/180] Step 1100/1249 Loss 6.038 Prec@(1,3) (77.1%, 98.2%), ce_loss 0.920, lat_loss 21.923
09/28 09:30:06 AM | Train: [ 49/180] Step 1150/1249 Loss 6.038 Prec@(1,3) (77.1%, 98.2%), ce_loss 0.919, lat_loss 21.923
09/28 09:30:30 AM | Train: [ 49/180] Step 1200/1249 Loss 6.037 Prec@(1,3) (77.1%, 98.2%), ce_loss 0.919, lat_loss 21.923
09/28 09:30:55 AM | Train: [ 49/180] Step 1249/1249 Loss 6.033 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.919, lat_loss 21.923
09/28 09:30:55 AM | _w_step_train: [ 49/180] Final Prec@1 77.1750% Time 526.87
09/28 09:30:55 AM | Start to train theta for epoch 48
09/28 09:31:15 AM | Train: [ 49/180] Step 050/312 Loss 6.938 Prec@(1,3) (73.6%, 98.1%), ce_loss 0.919, lat_loss 21.923
09/28 09:31:35 AM | Train: [ 49/180] Step 100/312 Loss 6.835 Prec@(1,3) (74.1%, 97.9%), ce_loss 0.919, lat_loss 21.923
09/28 09:31:55 AM | Train: [ 49/180] Step 150/312 Loss 6.752 Prec@(1,3) (74.7%, 97.7%), ce_loss 0.919, lat_loss 21.923
09/28 09:32:15 AM | Train: [ 49/180] Step 200/312 Loss 6.572 Prec@(1,3) (75.3%, 97.9%), ce_loss 0.919, lat_loss 21.923
09/28 09:32:35 AM | Train: [ 49/180] Step 250/312 Loss 6.448 Prec@(1,3) (75.5%, 98.0%), ce_loss 0.919, lat_loss 21.922
09/28 09:32:54 AM | Train: [ 49/180] Step 300/312 Loss 6.424 Prec@(1,3) (75.7%, 98.1%), ce_loss 0.918, lat_loss 21.922
09/28 09:32:59 AM | Train: [ 49/180] Step 312/312 Loss 6.422 Prec@(1,3) (75.6%, 98.1%), ce_loss 0.918, lat_loss 21.922
09/28 09:32:59 AM | _theta_step_train: [ 49/180] Final Prec@1 75.6500% Time 124.39
09/28 09:33:04 AM | Valid: [ 49/180] Step 050/312 Loss 6.888 Prec@(1,3) (74.5%, 97.7%), ce_loss 0.918, lat_loss 21.922
09/28 09:33:08 AM | Valid: [ 49/180] Step 100/312 Loss 7.263 Prec@(1,3) (73.9%, 97.7%), ce_loss 0.918, lat_loss 21.922
09/28 09:33:12 AM | Valid: [ 49/180] Step 150/312 Loss 7.260 Prec@(1,3) (73.6%, 97.5%), ce_loss 0.918, lat_loss 21.922
09/28 09:33:17 AM | Valid: [ 49/180] Step 200/312 Loss 7.175 Prec@(1,3) (73.3%, 97.6%), ce_loss 0.918, lat_loss 21.922
09/28 09:33:21 AM | Valid: [ 49/180] Step 250/312 Loss 7.164 Prec@(1,3) (73.2%, 97.6%), ce_loss 0.918, lat_loss 21.922
09/28 09:33:25 AM | Valid: [ 49/180] Step 300/312 Loss 7.107 Prec@(1,3) (73.6%, 97.6%), ce_loss 0.918, lat_loss 21.922
09/28 09:33:26 AM | Valid: [ 49/180] Step 312/312 Loss 7.101 Prec@(1,3) (73.6%, 97.7%), ce_loss 0.918, lat_loss 21.922
09/28 09:33:26 AM | val: [ 49/180] Final Prec@1 73.6100% Time 27.02
09/28 09:33:26 AM | Start to train weights for epoch 49
09/28 09:33:51 AM | Train: [ 50/180] Step 050/1249 Loss 6.194 Prec@(1,3) (76.1%, 97.8%), ce_loss 0.918, lat_loss 21.921
09/28 09:34:12 AM | Train: [ 50/180] Step 100/1249 Loss 6.155 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.918, lat_loss 21.921
09/28 09:34:33 AM | Train: [ 50/180] Step 150/1249 Loss 6.087 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.918, lat_loss 21.921
09/28 09:34:54 AM | Train: [ 50/180] Step 200/1249 Loss 6.080 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.918, lat_loss 21.921
09/28 09:35:17 AM | Train: [ 50/180] Step 250/1249 Loss 6.161 Prec@(1,3) (76.4%, 98.0%), ce_loss 0.917, lat_loss 21.921
09/28 09:35:42 AM | Train: [ 50/180] Step 300/1249 Loss 6.193 Prec@(1,3) (76.3%, 98.0%), ce_loss 0.917, lat_loss 21.921
09/28 09:36:07 AM | Train: [ 50/180] Step 350/1249 Loss 6.175 Prec@(1,3) (76.4%, 98.1%), ce_loss 0.917, lat_loss 21.921
09/28 09:36:32 AM | Train: [ 50/180] Step 400/1249 Loss 6.137 Prec@(1,3) (76.5%, 98.1%), ce_loss 0.917, lat_loss 21.921
09/28 09:36:57 AM | Train: [ 50/180] Step 450/1249 Loss 6.067 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.917, lat_loss 21.921
09/28 09:37:22 AM | Train: [ 50/180] Step 500/1249 Loss 6.079 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.917, lat_loss 21.921
09/28 09:37:47 AM | Train: [ 50/180] Step 550/1249 Loss 6.102 Prec@(1,3) (76.7%, 98.2%), ce_loss 0.917, lat_loss 21.920
09/28 09:38:12 AM | Train: [ 50/180] Step 600/1249 Loss 6.106 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.917, lat_loss 21.920
09/28 09:38:37 AM | Train: [ 50/180] Step 650/1249 Loss 6.098 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.916, lat_loss 21.920
09/28 09:39:01 AM | Train: [ 50/180] Step 700/1249 Loss 6.077 Prec@(1,3) (76.9%, 98.2%), ce_loss 0.916, lat_loss 21.920
09/28 09:39:26 AM | Train: [ 50/180] Step 750/1249 Loss 6.055 Prec@(1,3) (77.0%, 98.2%), ce_loss 0.916, lat_loss 21.920
09/28 09:39:51 AM | Train: [ 50/180] Step 800/1249 Loss 6.019 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.916, lat_loss 21.920
09/28 09:40:16 AM | Train: [ 50/180] Step 850/1249 Loss 6.011 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.916, lat_loss 21.920
09/28 09:40:41 AM | Train: [ 50/180] Step 900/1249 Loss 6.012 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.916, lat_loss 21.920
09/28 09:41:05 AM | Train: [ 50/180] Step 950/1249 Loss 5.989 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.916, lat_loss 21.920
09/28 09:41:31 AM | Train: [ 50/180] Step 1000/1249 Loss 6.010 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.915, lat_loss 21.919
09/28 09:41:55 AM | Train: [ 50/180] Step 1050/1249 Loss 6.020 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.915, lat_loss 21.919
09/28 09:42:20 AM | Train: [ 50/180] Step 1100/1249 Loss 6.037 Prec@(1,3) (77.1%, 98.2%), ce_loss 0.915, lat_loss 21.919
09/28 09:42:44 AM | Train: [ 50/180] Step 1150/1249 Loss 6.036 Prec@(1,3) (77.1%, 98.2%), ce_loss 0.915, lat_loss 21.919
09/28 09:43:09 AM | Train: [ 50/180] Step 1200/1249 Loss 6.054 Prec@(1,3) (77.0%, 98.2%), ce_loss 0.915, lat_loss 21.919
09/28 09:43:33 AM | Train: [ 50/180] Step 1249/1249 Loss 6.069 Prec@(1,3) (77.0%, 98.2%), ce_loss 0.915, lat_loss 21.919
09/28 09:43:33 AM | _w_step_train: [ 50/180] Final Prec@1 77.0025% Time 606.76
09/28 09:43:33 AM | Start to train theta for epoch 49
09/28 09:43:55 AM | Train: [ 50/180] Step 050/312 Loss 6.666 Prec@(1,3) (75.6%, 98.2%), ce_loss 0.915, lat_loss 21.919
09/28 09:44:16 AM | Train: [ 50/180] Step 100/312 Loss 6.319 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.915, lat_loss 21.919
09/28 09:44:36 AM | Train: [ 50/180] Step 150/312 Loss 6.248 Prec@(1,3) (76.6%, 98.3%), ce_loss 0.915, lat_loss 21.919
09/28 09:44:55 AM | Train: [ 50/180] Step 200/312 Loss 6.292 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.914, lat_loss 21.918
09/28 09:45:14 AM | Train: [ 50/180] Step 250/312 Loss 6.357 Prec@(1,3) (75.8%, 98.1%), ce_loss 0.914, lat_loss 21.918
09/28 09:45:35 AM | Train: [ 50/180] Step 300/312 Loss 6.339 Prec@(1,3) (75.9%, 98.2%), ce_loss 0.914, lat_loss 21.918
09/28 09:45:40 AM | Train: [ 50/180] Step 312/312 Loss 6.302 Prec@(1,3) (76.1%, 98.2%), ce_loss 0.914, lat_loss 21.918
09/28 09:45:41 AM | _theta_step_train: [ 50/180] Final Prec@1 76.0600% Time 127.99
09/28 09:45:46 AM | Valid: [ 50/180] Step 050/312 Loss 7.320 Prec@(1,3) (71.4%, 97.2%), ce_loss 0.914, lat_loss 21.918
09/28 09:45:50 AM | Valid: [ 50/180] Step 100/312 Loss 7.193 Prec@(1,3) (72.4%, 97.3%), ce_loss 0.914, lat_loss 21.918
09/28 09:45:54 AM | Valid: [ 50/180] Step 150/312 Loss 7.116 Prec@(1,3) (72.8%, 97.3%), ce_loss 0.914, lat_loss 21.918
09/28 09:45:59 AM | Valid: [ 50/180] Step 200/312 Loss 7.107 Prec@(1,3) (73.0%, 97.5%), ce_loss 0.914, lat_loss 21.918
09/28 09:46:03 AM | Valid: [ 50/180] Step 250/312 Loss 7.146 Prec@(1,3) (72.9%, 97.5%), ce_loss 0.914, lat_loss 21.918
09/28 09:46:07 AM | Valid: [ 50/180] Step 300/312 Loss 7.032 Prec@(1,3) (73.3%, 97.6%), ce_loss 0.914, lat_loss 21.918
09/28 09:46:08 AM | Valid: [ 50/180] Step 312/312 Loss 7.002 Prec@(1,3) (73.4%, 97.6%), ce_loss 0.914, lat_loss 21.918
09/28 09:46:08 AM | val: [ 50/180] Final Prec@1 73.4100% Time 27.05
09/28 09:46:08 AM | Start to train weights for epoch 50
09/28 09:46:34 AM | Train: [ 51/180] Step 050/1249 Loss 5.904 Prec@(1,3) (77.1%, 98.3%), ce_loss 0.914, lat_loss 21.917
09/28 09:46:59 AM | Train: [ 51/180] Step 100/1249 Loss 6.246 Prec@(1,3) (76.3%, 98.2%), ce_loss 0.914, lat_loss 21.917
09/28 09:47:24 AM | Train: [ 51/180] Step 150/1249 Loss 5.998 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.913, lat_loss 21.917
09/28 09:47:49 AM | Train: [ 51/180] Step 200/1249 Loss 5.979 Prec@(1,3) (77.3%, 98.2%), ce_loss 0.913, lat_loss 21.917
09/28 09:48:14 AM | Train: [ 51/180] Step 250/1249 Loss 5.972 Prec@(1,3) (77.2%, 98.3%), ce_loss 0.913, lat_loss 21.917
09/28 09:48:39 AM | Train: [ 51/180] Step 300/1249 Loss 5.999 Prec@(1,3) (77.3%, 98.2%), ce_loss 0.913, lat_loss 21.917
09/28 09:49:03 AM | Train: [ 51/180] Step 350/1249 Loss 5.885 Prec@(1,3) (77.8%, 98.2%), ce_loss 0.913, lat_loss 21.917
09/28 09:49:28 AM | Train: [ 51/180] Step 400/1249 Loss 5.911 Prec@(1,3) (77.7%, 98.2%), ce_loss 0.913, lat_loss 21.917
09/28 09:49:53 AM | Train: [ 51/180] Step 450/1249 Loss 5.935 Prec@(1,3) (77.6%, 98.3%), ce_loss 0.913, lat_loss 21.917
09/28 09:50:18 AM | Train: [ 51/180] Step 500/1249 Loss 5.922 Prec@(1,3) (77.7%, 98.3%), ce_loss 0.913, lat_loss 21.917
09/28 09:50:43 AM | Train: [ 51/180] Step 550/1249 Loss 5.983 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.912, lat_loss 21.916
09/28 09:51:08 AM | Train: [ 51/180] Step 600/1249 Loss 5.945 Prec@(1,3) (77.5%, 98.3%), ce_loss 0.912, lat_loss 21.916
09/28 09:51:33 AM | Train: [ 51/180] Step 650/1249 Loss 5.987 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.912, lat_loss 21.916
09/28 09:51:58 AM | Train: [ 51/180] Step 700/1249 Loss 6.016 Prec@(1,3) (77.3%, 98.2%), ce_loss 0.912, lat_loss 21.916
09/28 09:52:23 AM | Train: [ 51/180] Step 750/1249 Loss 6.027 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.912, lat_loss 21.916
09/28 09:52:47 AM | Train: [ 51/180] Step 800/1249 Loss 6.032 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.912, lat_loss 21.916
09/28 09:53:11 AM | Train: [ 51/180] Step 850/1249 Loss 6.042 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.912, lat_loss 21.916
09/28 09:53:36 AM | Train: [ 51/180] Step 900/1249 Loss 6.030 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.912, lat_loss 21.916
09/28 09:54:00 AM | Train: [ 51/180] Step 950/1249 Loss 6.032 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.911, lat_loss 21.916
09/28 09:54:25 AM | Train: [ 51/180] Step 1000/1249 Loss 6.036 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.911, lat_loss 21.916
09/28 09:54:50 AM | Train: [ 51/180] Step 1050/1249 Loss 6.039 Prec@(1,3) (77.3%, 98.2%), ce_loss 0.911, lat_loss 21.915
09/28 09:55:15 AM | Train: [ 51/180] Step 1100/1249 Loss 6.051 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.911, lat_loss 21.915
09/28 09:55:39 AM | Train: [ 51/180] Step 1150/1249 Loss 6.057 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.911, lat_loss 21.915
09/28 09:56:03 AM | Train: [ 51/180] Step 1200/1249 Loss 6.061 Prec@(1,3) (77.2%, 98.2%), ce_loss 0.911, lat_loss 21.915
09/28 09:56:27 AM | Train: [ 51/180] Step 1249/1249 Loss 6.046 Prec@(1,3) (77.3%, 98.2%), ce_loss 0.911, lat_loss 21.915
09/28 09:56:27 AM | _w_step_train: [ 51/180] Final Prec@1 77.2750% Time 619.26
09/28 09:56:27 AM | Start to train theta for epoch 50
09/28 09:56:48 AM | Train: [ 51/180] Step 050/312 Loss 6.302 Prec@(1,3) (76.9%, 98.5%), ce_loss 0.911, lat_loss 21.915
09/28 09:57:06 AM | Train: [ 51/180] Step 100/312 Loss 6.146 Prec@(1,3) (76.7%, 98.3%), ce_loss 0.911, lat_loss 21.915
09/28 09:57:25 AM | Train: [ 51/180] Step 150/312 Loss 6.213 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.910, lat_loss 21.915
09/28 09:57:44 AM | Train: [ 51/180] Step 200/312 Loss 6.166 Prec@(1,3) (76.6%, 98.1%), ce_loss 0.910, lat_loss 21.915
09/28 09:58:02 AM | Train: [ 51/180] Step 250/312 Loss 6.214 Prec@(1,3) (76.6%, 98.0%), ce_loss 0.910, lat_loss 21.915
09/28 09:58:22 AM | Train: [ 51/180] Step 300/312 Loss 6.186 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:27 AM | Train: [ 51/180] Step 312/312 Loss 6.157 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:28 AM | _theta_step_train: [ 51/180] Final Prec@1 76.7300% Time 120.22
09/28 09:58:33 AM | Valid: [ 51/180] Step 050/312 Loss 6.538 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:37 AM | Valid: [ 51/180] Step 100/312 Loss 6.650 Prec@(1,3) (76.0%, 97.9%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:42 AM | Valid: [ 51/180] Step 150/312 Loss 6.639 Prec@(1,3) (76.0%, 97.8%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:47 AM | Valid: [ 51/180] Step 200/312 Loss 6.726 Prec@(1,3) (75.4%, 97.8%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:51 AM | Valid: [ 51/180] Step 250/312 Loss 6.681 Prec@(1,3) (75.7%, 97.9%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:56 AM | Valid: [ 51/180] Step 300/312 Loss 6.660 Prec@(1,3) (75.7%, 97.9%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:57 AM | Valid: [ 51/180] Step 312/312 Loss 6.647 Prec@(1,3) (75.8%, 97.9%), ce_loss 0.910, lat_loss 21.914
09/28 09:58:57 AM | val: [ 51/180] Final Prec@1 75.8000% Time 29.51
09/28 09:58:57 AM | Best top1 acc by now. Save model
09/28 09:58:57 AM | Start to train weights for epoch 51
09/28 09:59:22 AM | Train: [ 52/180] Step 050/1249 Loss 5.936 Prec@(1,3) (77.5%, 98.5%), ce_loss 0.909, lat_loss 21.914
09/28 09:59:44 AM | Train: [ 52/180] Step 100/1249 Loss 5.774 Prec@(1,3) (77.8%, 98.3%), ce_loss 0.909, lat_loss 21.914
09/28 10:00:05 AM | Train: [ 52/180] Step 150/1249 Loss 5.788 Prec@(1,3) (77.7%, 98.4%), ce_loss 0.909, lat_loss 21.914
09/28 10:00:27 AM | Train: [ 52/180] Step 200/1249 Loss 5.835 Prec@(1,3) (77.8%, 98.5%), ce_loss 0.909, lat_loss 21.914
09/28 10:00:50 AM | Train: [ 52/180] Step 250/1249 Loss 5.883 Prec@(1,3) (77.8%, 98.4%), ce_loss 0.909, lat_loss 21.913
09/28 10:01:11 AM | Train: [ 52/180] Step 300/1249 Loss 5.914 Prec@(1,3) (77.6%, 98.3%), ce_loss 0.909, lat_loss 21.913
09/28 10:01:35 AM | Train: [ 52/180] Step 350/1249 Loss 5.872 Prec@(1,3) (77.8%, 98.4%), ce_loss 0.909, lat_loss 21.913
09/28 10:01:58 AM | Train: [ 52/180] Step 400/1249 Loss 5.835 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.908, lat_loss 21.913
09/28 10:02:22 AM | Train: [ 52/180] Step 450/1249 Loss 5.805 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.908, lat_loss 21.913
09/28 10:02:45 AM | Train: [ 52/180] Step 500/1249 Loss 5.811 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.908, lat_loss 21.913
09/28 10:03:08 AM | Train: [ 52/180] Step 550/1249 Loss 5.820 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.908, lat_loss 21.913
09/28 10:03:33 AM | Train: [ 52/180] Step 600/1249 Loss 5.831 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.908, lat_loss 21.913
09/28 10:03:56 AM | Train: [ 52/180] Step 650/1249 Loss 5.822 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.908, lat_loss 21.913
09/28 10:04:19 AM | Train: [ 52/180] Step 700/1249 Loss 5.797 Prec@(1,3) (78.3%, 98.4%), ce_loss 0.908, lat_loss 21.913
09/28 10:04:43 AM | Train: [ 52/180] Step 750/1249 Loss 5.804 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.908, lat_loss 21.913
09/28 10:05:06 AM | Train: [ 52/180] Step 800/1249 Loss 5.801 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.907, lat_loss 21.913
09/28 10:05:29 AM | Train: [ 52/180] Step 850/1249 Loss 5.814 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.907, lat_loss 21.912
09/28 10:05:53 AM | Train: [ 52/180] Step 900/1249 Loss 5.826 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.907, lat_loss 21.912
09/28 10:06:16 AM | Train: [ 52/180] Step 950/1249 Loss 5.823 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.907, lat_loss 21.912
09/28 10:06:40 AM | Train: [ 52/180] Step 1000/1249 Loss 5.822 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.907, lat_loss 21.912
09/28 10:07:03 AM | Train: [ 52/180] Step 1050/1249 Loss 5.809 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.907, lat_loss 21.912
09/28 10:07:28 AM | Train: [ 52/180] Step 1100/1249 Loss 5.828 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.907, lat_loss 21.912
09/28 10:07:50 AM | Train: [ 52/180] Step 1150/1249 Loss 5.829 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.906, lat_loss 21.912
09/28 10:08:12 AM | Train: [ 52/180] Step 1200/1249 Loss 5.833 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.906, lat_loss 21.912
09/28 10:08:35 AM | Train: [ 52/180] Step 1249/1249 Loss 5.846 Prec@(1,3) (78.0%, 98.3%), ce_loss 0.906, lat_loss 21.912
09/28 10:08:35 AM | _w_step_train: [ 52/180] Final Prec@1 78.0150% Time 577.42
09/28 10:08:35 AM | Start to train theta for epoch 51
09/28 10:08:56 AM | Train: [ 52/180] Step 050/312 Loss 6.857 Prec@(1,3) (73.9%, 97.9%), ce_loss 0.906, lat_loss 21.912
09/28 10:09:16 AM | Train: [ 52/180] Step 100/312 Loss 6.849 Prec@(1,3) (74.2%, 97.9%), ce_loss 0.906, lat_loss 21.912
09/28 10:09:37 AM | Train: [ 52/180] Step 150/312 Loss 6.559 Prec@(1,3) (75.4%, 97.7%), ce_loss 0.906, lat_loss 21.911
09/28 10:09:58 AM | Train: [ 52/180] Step 200/312 Loss 6.599 Prec@(1,3) (75.2%, 97.8%), ce_loss 0.906, lat_loss 21.911
09/28 10:10:18 AM | Train: [ 52/180] Step 250/312 Loss 6.549 Prec@(1,3) (75.6%, 97.6%), ce_loss 0.906, lat_loss 21.911
09/28 10:10:39 AM | Train: [ 52/180] Step 300/312 Loss 6.501 Prec@(1,3) (75.9%, 97.7%), ce_loss 0.906, lat_loss 21.911
09/28 10:10:44 AM | Train: [ 52/180] Step 312/312 Loss 6.496 Prec@(1,3) (75.9%, 97.8%), ce_loss 0.906, lat_loss 21.911
09/28 10:10:44 AM | _theta_step_train: [ 52/180] Final Prec@1 75.9400% Time 129.25
09/28 10:10:49 AM | Valid: [ 52/180] Step 050/312 Loss 7.008 Prec@(1,3) (74.3%, 98.3%), ce_loss 0.906, lat_loss 21.911
09/28 10:10:54 AM | Valid: [ 52/180] Step 100/312 Loss 6.886 Prec@(1,3) (74.9%, 98.1%), ce_loss 0.906, lat_loss 21.911
09/28 10:10:59 AM | Valid: [ 52/180] Step 150/312 Loss 6.984 Prec@(1,3) (74.7%, 98.1%), ce_loss 0.906, lat_loss 21.911
09/28 10:11:03 AM | Valid: [ 52/180] Step 200/312 Loss 7.020 Prec@(1,3) (74.4%, 98.1%), ce_loss 0.905, lat_loss 21.911
09/28 10:11:08 AM | Valid: [ 52/180] Step 250/312 Loss 7.032 Prec@(1,3) (74.5%, 98.1%), ce_loss 0.905, lat_loss 21.911
09/28 10:11:12 AM | Valid: [ 52/180] Step 300/312 Loss 6.995 Prec@(1,3) (74.7%, 98.0%), ce_loss 0.905, lat_loss 21.911
09/28 10:11:13 AM | Valid: [ 52/180] Step 312/312 Loss 6.992 Prec@(1,3) (74.7%, 98.0%), ce_loss 0.905, lat_loss 21.911
09/28 10:11:13 AM | val: [ 52/180] Final Prec@1 74.6900% Time 29.45
09/28 10:11:13 AM | Start to train weights for epoch 52
09/28 10:11:39 AM | Train: [ 53/180] Step 050/1249 Loss 5.809 Prec@(1,3) (78.4%, 98.0%), ce_loss 0.905, lat_loss 21.911
09/28 10:12:03 AM | Train: [ 53/180] Step 100/1249 Loss 5.880 Prec@(1,3) (78.1%, 97.9%), ce_loss 0.905, lat_loss 21.910
09/28 10:12:27 AM | Train: [ 53/180] Step 150/1249 Loss 5.750 Prec@(1,3) (78.6%, 98.1%), ce_loss 0.905, lat_loss 21.910
09/28 10:12:50 AM | Train: [ 53/180] Step 200/1249 Loss 5.871 Prec@(1,3) (78.0%, 98.1%), ce_loss 0.905, lat_loss 21.910
09/28 10:13:10 AM | Train: [ 53/180] Step 250/1249 Loss 5.881 Prec@(1,3) (78.1%, 98.1%), ce_loss 0.905, lat_loss 21.910
09/28 10:13:31 AM | Train: [ 53/180] Step 300/1249 Loss 5.981 Prec@(1,3) (77.8%, 98.1%), ce_loss 0.905, lat_loss 21.910
09/28 10:13:52 AM | Train: [ 53/180] Step 350/1249 Loss 5.998 Prec@(1,3) (77.7%, 98.2%), ce_loss 0.905, lat_loss 21.910
09/28 10:14:13 AM | Train: [ 53/180] Step 400/1249 Loss 6.033 Prec@(1,3) (77.5%, 98.1%), ce_loss 0.904, lat_loss 21.910
09/28 10:14:34 AM | Train: [ 53/180] Step 450/1249 Loss 6.074 Prec@(1,3) (77.5%, 98.1%), ce_loss 0.904, lat_loss 21.910
09/28 10:14:50 AM | Train: [ 53/180] Step 500/1249 Loss 6.055 Prec@(1,3) (77.4%, 98.1%), ce_loss 0.904, lat_loss 21.910
09/28 10:15:04 AM | Train: [ 53/180] Step 550/1249 Loss 6.056 Prec@(1,3) (77.5%, 98.1%), ce_loss 0.904, lat_loss 21.910
09/28 10:15:19 AM | Train: [ 53/180] Step 600/1249 Loss 6.031 Prec@(1,3) (77.5%, 98.2%), ce_loss 0.904, lat_loss 21.910
09/28 10:15:33 AM | Train: [ 53/180] Step 650/1249 Loss 5.968 Prec@(1,3) (77.7%, 98.2%), ce_loss 0.904, lat_loss 21.910
09/28 10:15:48 AM | Train: [ 53/180] Step 700/1249 Loss 5.991 Prec@(1,3) (77.5%, 98.2%), ce_loss 0.904, lat_loss 21.909
09/28 10:16:03 AM | Train: [ 53/180] Step 750/1249 Loss 5.969 Prec@(1,3) (77.5%, 98.2%), ce_loss 0.904, lat_loss 21.909
09/28 10:16:17 AM | Train: [ 53/180] Step 800/1249 Loss 5.936 Prec@(1,3) (77.6%, 98.2%), ce_loss 0.903, lat_loss 21.909
09/28 10:16:32 AM | Train: [ 53/180] Step 850/1249 Loss 5.942 Prec@(1,3) (77.6%, 98.2%), ce_loss 0.903, lat_loss 21.909
09/28 10:16:46 AM | Train: [ 53/180] Step 900/1249 Loss 5.947 Prec@(1,3) (77.6%, 98.2%), ce_loss 0.903, lat_loss 21.909
09/28 10:17:01 AM | Train: [ 53/180] Step 950/1249 Loss 5.975 Prec@(1,3) (77.4%, 98.2%), ce_loss 0.903, lat_loss 21.909
09/28 10:17:16 AM | Train: [ 53/180] Step 1000/1249 Loss 5.955 Prec@(1,3) (77.5%, 98.1%), ce_loss 0.903, lat_loss 21.909
09/28 10:17:30 AM | Train: [ 53/180] Step 1050/1249 Loss 5.953 Prec@(1,3) (77.6%, 98.1%), ce_loss 0.903, lat_loss 21.909
09/28 10:17:45 AM | Train: [ 53/180] Step 1100/1249 Loss 5.933 Prec@(1,3) (77.6%, 98.1%), ce_loss 0.903, lat_loss 21.909
09/28 10:18:00 AM | Train: [ 53/180] Step 1150/1249 Loss 5.928 Prec@(1,3) (77.6%, 98.2%), ce_loss 0.903, lat_loss 21.909
09/28 10:18:15 AM | Train: [ 53/180] Step 1200/1249 Loss 5.923 Prec@(1,3) (77.5%, 98.2%), ce_loss 0.902, lat_loss 21.909
09/28 10:18:30 AM | Train: [ 53/180] Step 1249/1249 Loss 5.921 Prec@(1,3) (77.6%, 98.2%), ce_loss 0.902, lat_loss 21.909
09/28 10:18:30 AM | _w_step_train: [ 53/180] Final Prec@1 77.5700% Time 436.65
09/28 10:18:30 AM | Start to train theta for epoch 52
09/28 10:18:43 AM | Train: [ 53/180] Step 050/312 Loss 6.412 Prec@(1,3) (76.0%, 97.6%), ce_loss 0.902, lat_loss 21.908
09/28 10:18:55 AM | Train: [ 53/180] Step 100/312 Loss 6.257 Prec@(1,3) (76.4%, 97.9%), ce_loss 0.902, lat_loss 21.908
09/28 10:19:06 AM | Train: [ 53/180] Step 150/312 Loss 6.237 Prec@(1,3) (76.5%, 98.1%), ce_loss 0.902, lat_loss 21.908
09/28 10:19:19 AM | Train: [ 53/180] Step 200/312 Loss 6.312 Prec@(1,3) (76.1%, 98.2%), ce_loss 0.902, lat_loss 21.908
09/28 10:19:30 AM | Train: [ 53/180] Step 250/312 Loss 6.284 Prec@(1,3) (76.2%, 98.2%), ce_loss 0.902, lat_loss 21.908
09/28 10:19:42 AM | Train: [ 53/180] Step 300/312 Loss 6.251 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.902, lat_loss 21.908
09/28 10:19:44 AM | Train: [ 53/180] Step 312/312 Loss 6.233 Prec@(1,3) (76.6%, 98.2%), ce_loss 0.902, lat_loss 21.908
09/28 10:19:45 AM | _theta_step_train: [ 53/180] Final Prec@1 76.5600% Time 74.35
09/28 10:19:50 AM | Valid: [ 53/180] Step 050/312 Loss 6.649 Prec@(1,3) (75.4%, 99.0%), ce_loss 0.902, lat_loss 21.908
09/28 10:19:54 AM | Valid: [ 53/180] Step 100/312 Loss 7.249 Prec@(1,3) (73.9%, 97.8%), ce_loss 0.902, lat_loss 21.908
09/28 10:19:59 AM | Valid: [ 53/180] Step 150/312 Loss 7.217 Prec@(1,3) (73.8%, 97.4%), ce_loss 0.902, lat_loss 21.908
09/28 10:20:04 AM | Valid: [ 53/180] Step 200/312 Loss 7.098 Prec@(1,3) (74.1%, 97.5%), ce_loss 0.902, lat_loss 21.908
09/28 10:20:08 AM | Valid: [ 53/180] Step 250/312 Loss 7.071 Prec@(1,3) (74.1%, 97.5%), ce_loss 0.901, lat_loss 21.908
09/28 10:20:13 AM | Valid: [ 53/180] Step 300/312 Loss 7.039 Prec@(1,3) (74.0%, 97.6%), ce_loss 0.901, lat_loss 21.908
09/28 10:20:14 AM | Valid: [ 53/180] Step 312/312 Loss 7.037 Prec@(1,3) (74.0%, 97.6%), ce_loss 0.901, lat_loss 21.908
09/28 10:20:14 AM | val: [ 53/180] Final Prec@1 74.0300% Time 29.69
09/28 10:20:14 AM | Start to train weights for epoch 53
09/28 10:20:41 AM | Train: [ 54/180] Step 050/1249 Loss 5.989 Prec@(1,3) (78.4%, 98.3%), ce_loss 0.901, lat_loss 21.907
09/28 10:21:06 AM | Train: [ 54/180] Step 100/1249 Loss 5.954 Prec@(1,3) (77.7%, 98.3%), ce_loss 0.901, lat_loss 21.907
09/28 10:21:31 AM | Train: [ 54/180] Step 150/1249 Loss 5.859 Prec@(1,3) (77.9%, 98.4%), ce_loss 0.901, lat_loss 21.907
09/28 10:21:56 AM | Train: [ 54/180] Step 200/1249 Loss 5.766 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.901, lat_loss 21.907
09/28 10:22:20 AM | Train: [ 54/180] Step 250/1249 Loss 5.819 Prec@(1,3) (78.0%, 98.3%), ce_loss 0.901, lat_loss 21.907
09/28 10:22:45 AM | Train: [ 54/180] Step 300/1249 Loss 5.818 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.901, lat_loss 21.907
09/28 10:23:10 AM | Train: [ 54/180] Step 350/1249 Loss 5.806 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.900, lat_loss 21.907
09/28 10:23:35 AM | Train: [ 54/180] Step 400/1249 Loss 5.832 Prec@(1,3) (77.9%, 98.4%), ce_loss 0.900, lat_loss 21.907
09/28 10:23:57 AM | Train: [ 54/180] Step 450/1249 Loss 5.829 Prec@(1,3) (77.9%, 98.4%), ce_loss 0.900, lat_loss 21.907
09/28 10:24:18 AM | Train: [ 54/180] Step 500/1249 Loss 5.873 Prec@(1,3) (77.8%, 98.4%), ce_loss 0.900, lat_loss 21.907
09/28 10:24:40 AM | Train: [ 54/180] Step 550/1249 Loss 5.876 Prec@(1,3) (77.9%, 98.4%), ce_loss 0.900, lat_loss 21.907
09/28 10:25:02 AM | Train: [ 54/180] Step 600/1249 Loss 5.854 Prec@(1,3) (77.9%, 98.4%), ce_loss 0.900, lat_loss 21.907
09/28 10:25:24 AM | Train: [ 54/180] Step 650/1249 Loss 5.816 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.900, lat_loss 21.906
09/28 10:25:47 AM | Train: [ 54/180] Step 700/1249 Loss 5.812 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.900, lat_loss 21.906
09/28 10:26:07 AM | Train: [ 54/180] Step 750/1249 Loss 5.793 Prec@(1,3) (78.3%, 98.4%), ce_loss 0.899, lat_loss 21.906
09/28 10:26:30 AM | Train: [ 54/180] Step 800/1249 Loss 5.806 Prec@(1,3) (78.2%, 98.3%), ce_loss 0.899, lat_loss 21.906
09/28 10:26:51 AM | Train: [ 54/180] Step 850/1249 Loss 5.802 Prec@(1,3) (78.3%, 98.3%), ce_loss 0.899, lat_loss 21.906
09/28 10:27:15 AM | Train: [ 54/180] Step 900/1249 Loss 5.834 Prec@(1,3) (78.2%, 98.3%), ce_loss 0.899, lat_loss 21.906
09/28 10:27:40 AM | Train: [ 54/180] Step 950/1249 Loss 5.842 Prec@(1,3) (78.1%, 98.3%), ce_loss 0.899, lat_loss 21.906
09/28 10:28:05 AM | Train: [ 54/180] Step 1000/1249 Loss 5.834 Prec@(1,3) (78.2%, 98.3%), ce_loss 0.899, lat_loss 21.906
09/28 10:28:30 AM | Train: [ 54/180] Step 1050/1249 Loss 5.845 Prec@(1,3) (78.1%, 98.3%), ce_loss 0.899, lat_loss 21.906
09/28 10:28:55 AM | Train: [ 54/180] Step 1100/1249 Loss 5.851 Prec@(1,3) (78.1%, 98.3%), ce_loss 0.899, lat_loss 21.906
09/28 10:29:20 AM | Train: [ 54/180] Step 1150/1249 Loss 5.852 Prec@(1,3) (78.1%, 98.3%), ce_loss 0.899, lat_loss 21.906
09/28 10:29:45 AM | Train: [ 54/180] Step 1200/1249 Loss 5.842 Prec@(1,3) (78.1%, 98.3%), ce_loss 0.898, lat_loss 21.906
09/28 10:30:09 AM | Train: [ 54/180] Step 1249/1249 Loss 5.844 Prec@(1,3) (78.1%, 98.3%), ce_loss 0.898, lat_loss 21.905
09/28 10:30:09 AM | _w_step_train: [ 54/180] Final Prec@1 78.0825% Time 594.96
09/28 10:30:09 AM | Start to train theta for epoch 53
09/28 10:30:31 AM | Train: [ 54/180] Step 050/312 Loss 6.030 Prec@(1,3) (76.5%, 98.5%), ce_loss 0.898, lat_loss 21.905
09/28 10:30:52 AM | Train: [ 54/180] Step 100/312 Loss 5.918 Prec@(1,3) (77.4%, 98.5%), ce_loss 0.898, lat_loss 21.905
09/28 10:31:13 AM | Train: [ 54/180] Step 150/312 Loss 6.069 Prec@(1,3) (76.8%, 98.4%), ce_loss 0.898, lat_loss 21.905
09/28 10:31:34 AM | Train: [ 54/180] Step 200/312 Loss 6.048 Prec@(1,3) (77.1%, 98.3%), ce_loss 0.898, lat_loss 21.905
09/28 10:31:55 AM | Train: [ 54/180] Step 250/312 Loss 6.046 Prec@(1,3) (77.1%, 98.3%), ce_loss 0.898, lat_loss 21.905
09/28 10:32:16 AM | Train: [ 54/180] Step 300/312 Loss 6.131 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.898, lat_loss 21.905
09/28 10:32:21 AM | Train: [ 54/180] Step 312/312 Loss 6.124 Prec@(1,3) (76.8%, 98.1%), ce_loss 0.898, lat_loss 21.905
09/28 10:32:21 AM | _theta_step_train: [ 54/180] Final Prec@1 76.7800% Time 131.55
09/28 10:32:26 AM | Valid: [ 54/180] Step 050/312 Loss 7.574 Prec@(1,3) (74.7%, 97.8%), ce_loss 0.898, lat_loss 21.905
09/28 10:32:31 AM | Valid: [ 54/180] Step 100/312 Loss 7.378 Prec@(1,3) (74.4%, 97.7%), ce_loss 0.898, lat_loss 21.905
09/28 10:32:35 AM | Valid: [ 54/180] Step 150/312 Loss 7.217 Prec@(1,3) (74.9%, 98.0%), ce_loss 0.898, lat_loss 21.905
09/28 10:32:40 AM | Valid: [ 54/180] Step 200/312 Loss 7.084 Prec@(1,3) (74.9%, 98.1%), ce_loss 0.898, lat_loss 21.905
09/28 10:32:44 AM | Valid: [ 54/180] Step 250/312 Loss 7.150 Prec@(1,3) (74.8%, 98.0%), ce_loss 0.897, lat_loss 21.905
09/28 10:32:49 AM | Valid: [ 54/180] Step 300/312 Loss 7.175 Prec@(1,3) (74.7%, 97.8%), ce_loss 0.897, lat_loss 21.905
09/28 10:32:51 AM | Valid: [ 54/180] Step 312/312 Loss 7.229 Prec@(1,3) (74.7%, 97.8%), ce_loss 0.897, lat_loss 21.905
09/28 10:32:51 AM | val: [ 54/180] Final Prec@1 74.6700% Time 29.91
09/28 10:32:51 AM | Start to train weights for epoch 54
09/28 10:33:17 AM | Train: [ 55/180] Step 050/1249 Loss 5.429 Prec@(1,3) (79.1%, 98.8%), ce_loss 0.897, lat_loss 21.905
09/28 10:33:42 AM | Train: [ 55/180] Step 100/1249 Loss 5.460 Prec@(1,3) (79.0%, 98.8%), ce_loss 0.897, lat_loss 21.905
09/28 10:34:07 AM | Train: [ 55/180] Step 150/1249 Loss 5.551 Prec@(1,3) (79.3%, 98.7%), ce_loss 0.897, lat_loss 21.905
09/28 10:34:32 AM | Train: [ 55/180] Step 200/1249 Loss 5.604 Prec@(1,3) (79.1%, 98.5%), ce_loss 0.897, lat_loss 21.905
09/28 10:34:57 AM | Train: [ 55/180] Step 250/1249 Loss 5.617 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.897, lat_loss 21.904
09/28 10:35:22 AM | Train: [ 55/180] Step 300/1249 Loss 5.801 Prec@(1,3) (78.3%, 98.3%), ce_loss 0.897, lat_loss 21.904
09/28 10:35:47 AM | Train: [ 55/180] Step 350/1249 Loss 5.856 Prec@(1,3) (78.2%, 98.3%), ce_loss 0.897, lat_loss 21.904
09/28 10:36:12 AM | Train: [ 55/180] Step 400/1249 Loss 5.892 Prec@(1,3) (78.0%, 98.3%), ce_loss 0.897, lat_loss 21.904
09/28 10:36:37 AM | Train: [ 55/180] Step 450/1249 Loss 5.870 Prec@(1,3) (78.1%, 98.3%), ce_loss 0.896, lat_loss 21.904
09/28 10:37:02 AM | Train: [ 55/180] Step 500/1249 Loss 5.830 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.896, lat_loss 21.904
09/28 10:37:21 AM | Train: [ 55/180] Step 550/1249 Loss 5.840 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.896, lat_loss 21.904
09/28 10:37:37 AM | Train: [ 55/180] Step 600/1249 Loss 5.833 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.896, lat_loss 21.904
09/28 10:37:53 AM | Train: [ 55/180] Step 650/1249 Loss 5.859 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.896, lat_loss 21.904
09/28 10:38:11 AM | Train: [ 55/180] Step 700/1249 Loss 5.857 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.896, lat_loss 21.904
09/28 10:38:37 AM | Train: [ 55/180] Step 750/1249 Loss 5.849 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.896, lat_loss 21.904
09/28 10:39:02 AM | Train: [ 55/180] Step 800/1249 Loss 5.854 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.896, lat_loss 21.904
09/28 10:39:27 AM | Train: [ 55/180] Step 850/1249 Loss 5.858 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.895, lat_loss 21.904
09/28 10:39:52 AM | Train: [ 55/180] Step 900/1249 Loss 5.862 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.895, lat_loss 21.904
09/28 10:40:17 AM | Train: [ 55/180] Step 950/1249 Loss 5.877 Prec@(1,3) (78.0%, 98.3%), ce_loss 0.895, lat_loss 21.904
09/28 10:40:43 AM | Train: [ 55/180] Step 1000/1249 Loss 5.882 Prec@(1,3) (78.0%, 98.3%), ce_loss 0.895, lat_loss 21.904
09/28 10:41:08 AM | Train: [ 55/180] Step 1050/1249 Loss 5.881 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.895, lat_loss 21.904
09/28 10:41:33 AM | Train: [ 55/180] Step 1100/1249 Loss 5.881 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.895, lat_loss 21.903
09/28 10:41:59 AM | Train: [ 55/180] Step 1150/1249 Loss 5.872 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.895, lat_loss 21.903
09/28 10:42:24 AM | Train: [ 55/180] Step 1200/1249 Loss 5.866 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.895, lat_loss 21.903
09/28 10:42:49 AM | Train: [ 55/180] Step 1249/1249 Loss 5.856 Prec@(1,3) (78.1%, 98.4%), ce_loss 0.895, lat_loss 21.903
09/28 10:42:49 AM | _w_step_train: [ 55/180] Final Prec@1 78.0725% Time 598.21
09/28 10:42:49 AM | Start to train theta for epoch 54
09/28 10:43:10 AM | Train: [ 55/180] Step 050/312 Loss 6.371 Prec@(1,3) (75.4%, 98.4%), ce_loss 0.894, lat_loss 21.903
09/28 10:43:31 AM | Train: [ 55/180] Step 100/312 Loss 5.953 Prec@(1,3) (77.3%, 98.3%), ce_loss 0.894, lat_loss 21.903
09/28 10:43:52 AM | Train: [ 55/180] Step 150/312 Loss 5.931 Prec@(1,3) (77.7%, 98.3%), ce_loss 0.894, lat_loss 21.903
09/28 10:44:13 AM | Train: [ 55/180] Step 200/312 Loss 5.867 Prec@(1,3) (78.0%, 98.4%), ce_loss 0.894, lat_loss 21.903
09/28 10:44:33 AM | Train: [ 55/180] Step 250/312 Loss 5.917 Prec@(1,3) (77.9%, 98.3%), ce_loss 0.894, lat_loss 21.903
09/28 10:44:54 AM | Train: [ 55/180] Step 300/312 Loss 5.957 Prec@(1,3) (77.8%, 98.3%), ce_loss 0.894, lat_loss 21.903
09/28 10:44:59 AM | Train: [ 55/180] Step 312/312 Loss 5.965 Prec@(1,3) (77.8%, 98.3%), ce_loss 0.894, lat_loss 21.903
09/28 10:44:59 AM | _theta_step_train: [ 55/180] Final Prec@1 77.8500% Time 129.85
09/28 10:45:04 AM | Valid: [ 55/180] Step 050/312 Loss 5.782 Prec@(1,3) (78.5%, 98.2%), ce_loss 0.894, lat_loss 21.903
09/28 10:45:08 AM | Valid: [ 55/180] Step 100/312 Loss 5.994 Prec@(1,3) (77.7%, 98.0%), ce_loss 0.894, lat_loss 21.903
09/28 10:45:13 AM | Valid: [ 55/180] Step 150/312 Loss 6.001 Prec@(1,3) (77.6%, 98.0%), ce_loss 0.894, lat_loss 21.903
09/28 10:45:17 AM | Valid: [ 55/180] Step 200/312 Loss 6.080 Prec@(1,3) (77.2%, 98.1%), ce_loss 0.893, lat_loss 21.903
09/28 10:45:21 AM | Valid: [ 55/180] Step 250/312 Loss 6.038 Prec@(1,3) (77.2%, 98.1%), ce_loss 0.893, lat_loss 21.903
09/28 10:45:26 AM | Valid: [ 55/180] Step 300/312 Loss 6.023 Prec@(1,3) (77.5%, 98.2%), ce_loss 0.893, lat_loss 21.903
09/28 10:45:27 AM | Valid: [ 55/180] Step 312/312 Loss 6.038 Prec@(1,3) (77.5%, 98.2%), ce_loss 0.893, lat_loss 21.903
09/28 10:45:27 AM | val: [ 55/180] Final Prec@1 77.5200% Time 28.22
09/28 10:45:27 AM | Best top1 acc by now. Save model
09/28 10:45:27 AM | Start to train weights for epoch 55
09/28 10:45:54 AM | Train: [ 56/180] Step 050/1249 Loss 5.869 Prec@(1,3) (77.5%, 98.5%), ce_loss 0.893, lat_loss 21.903
09/28 10:46:18 AM | Train: [ 56/180] Step 100/1249 Loss 5.645 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.893, lat_loss 21.903
09/28 10:46:42 AM | Train: [ 56/180] Step 150/1249 Loss 5.579 Prec@(1,3) (78.8%, 98.6%), ce_loss 0.893, lat_loss 21.903
09/28 10:47:07 AM | Train: [ 56/180] Step 200/1249 Loss 5.597 Prec@(1,3) (78.7%, 98.5%), ce_loss 0.893, lat_loss 21.902
09/28 10:47:31 AM | Train: [ 56/180] Step 250/1249 Loss 5.593 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.893, lat_loss 21.902
09/28 10:47:56 AM | Train: [ 56/180] Step 300/1249 Loss 5.592 Prec@(1,3) (79.0%, 98.4%), ce_loss 0.892, lat_loss 21.902
09/28 10:48:19 AM | Train: [ 56/180] Step 350/1249 Loss 5.613 Prec@(1,3) (78.9%, 98.4%), ce_loss 0.892, lat_loss 21.902
09/28 10:48:42 AM | Train: [ 56/180] Step 400/1249 Loss 5.610 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.892, lat_loss 21.902
09/28 10:49:07 AM | Train: [ 56/180] Step 450/1249 Loss 5.671 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.892, lat_loss 21.902
09/28 10:49:32 AM | Train: [ 56/180] Step 500/1249 Loss 5.663 Prec@(1,3) (78.6%, 98.5%), ce_loss 0.892, lat_loss 21.902
09/28 10:49:57 AM | Train: [ 56/180] Step 550/1249 Loss 5.642 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.892, lat_loss 21.902
09/28 10:50:22 AM | Train: [ 56/180] Step 600/1249 Loss 5.662 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.892, lat_loss 21.902
09/28 10:50:47 AM | Train: [ 56/180] Step 650/1249 Loss 5.647 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.892, lat_loss 21.902
09/28 10:51:12 AM | Train: [ 56/180] Step 700/1249 Loss 5.649 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.891, lat_loss 21.902
09/28 10:51:35 AM | Train: [ 56/180] Step 750/1249 Loss 5.707 Prec@(1,3) (78.5%, 98.3%), ce_loss 0.891, lat_loss 21.902
09/28 10:51:59 AM | Train: [ 56/180] Step 800/1249 Loss 5.696 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.891, lat_loss 21.902
09/28 10:52:23 AM | Train: [ 56/180] Step 850/1249 Loss 5.699 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.891, lat_loss 21.902
09/28 10:52:48 AM | Train: [ 56/180] Step 900/1249 Loss 5.677 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.891, lat_loss 21.902
09/28 10:53:12 AM | Train: [ 56/180] Step 950/1249 Loss 5.684 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.891, lat_loss 21.902
09/28 10:53:37 AM | Train: [ 56/180] Step 1000/1249 Loss 5.690 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.891, lat_loss 21.902
09/28 10:54:02 AM | Train: [ 56/180] Step 1050/1249 Loss 5.698 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.891, lat_loss 21.902
09/28 10:54:24 AM | Train: [ 56/180] Step 1100/1249 Loss 5.688 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.890, lat_loss 21.902
09/28 10:54:49 AM | Train: [ 56/180] Step 1150/1249 Loss 5.712 Prec@(1,3) (78.6%, 98.3%), ce_loss 0.890, lat_loss 21.902
09/28 10:55:14 AM | Train: [ 56/180] Step 1200/1249 Loss 5.707 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.890, lat_loss 21.902
09/28 10:55:38 AM | Train: [ 56/180] Step 1249/1249 Loss 5.705 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.890, lat_loss 21.901
09/28 10:55:38 AM | _w_step_train: [ 56/180] Final Prec@1 78.6050% Time 610.98
09/28 10:55:38 AM | Start to train theta for epoch 55
09/28 10:56:00 AM | Train: [ 56/180] Step 050/312 Loss 5.922 Prec@(1,3) (79.1%, 98.4%), ce_loss 0.890, lat_loss 21.901
09/28 10:56:20 AM | Train: [ 56/180] Step 100/312 Loss 6.232 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.890, lat_loss 21.901
09/28 10:56:41 AM | Train: [ 56/180] Step 150/312 Loss 6.186 Prec@(1,3) (77.3%, 98.3%), ce_loss 0.890, lat_loss 21.901
09/28 10:57:01 AM | Train: [ 56/180] Step 200/312 Loss 6.087 Prec@(1,3) (77.4%, 98.2%), ce_loss 0.890, lat_loss 21.901
09/28 10:57:22 AM | Train: [ 56/180] Step 250/312 Loss 6.177 Prec@(1,3) (77.0%, 98.2%), ce_loss 0.890, lat_loss 21.901
09/28 10:57:43 AM | Train: [ 56/180] Step 300/312 Loss 6.215 Prec@(1,3) (76.8%, 98.1%), ce_loss 0.890, lat_loss 21.901
09/28 10:57:48 AM | Train: [ 56/180] Step 312/312 Loss 6.198 Prec@(1,3) (76.9%, 98.1%), ce_loss 0.890, lat_loss 21.901
09/28 10:57:48 AM | _theta_step_train: [ 56/180] Final Prec@1 76.9000% Time 129.41
09/28 10:57:53 AM | Valid: [ 56/180] Step 050/312 Loss 7.205 Prec@(1,3) (72.2%, 97.7%), ce_loss 0.890, lat_loss 21.901
09/28 10:57:57 AM | Valid: [ 56/180] Step 100/312 Loss 7.124 Prec@(1,3) (73.1%, 97.6%), ce_loss 0.890, lat_loss 21.901
09/28 10:58:02 AM | Valid: [ 56/180] Step 150/312 Loss 7.096 Prec@(1,3) (73.3%, 97.3%), ce_loss 0.889, lat_loss 21.901
09/28 10:58:07 AM | Valid: [ 56/180] Step 200/312 Loss 7.156 Prec@(1,3) (73.5%, 97.2%), ce_loss 0.889, lat_loss 21.901
09/28 10:58:11 AM | Valid: [ 56/180] Step 250/312 Loss 7.042 Prec@(1,3) (73.8%, 97.4%), ce_loss 0.889, lat_loss 21.901
09/28 10:58:16 AM | Valid: [ 56/180] Step 300/312 Loss 6.852 Prec@(1,3) (74.5%, 97.6%), ce_loss 0.889, lat_loss 21.901
09/28 10:58:17 AM | Valid: [ 56/180] Step 312/312 Loss 6.812 Prec@(1,3) (74.6%, 97.7%), ce_loss 0.889, lat_loss 21.901
09/28 10:58:17 AM | val: [ 56/180] Final Prec@1 74.6300% Time 29.23
09/28 10:58:17 AM | Start to train weights for epoch 56
09/28 10:58:40 AM | Train: [ 57/180] Step 050/1249 Loss 5.989 Prec@(1,3) (76.0%, 98.2%), ce_loss 0.889, lat_loss 21.901
09/28 10:59:03 AM | Train: [ 57/180] Step 100/1249 Loss 5.849 Prec@(1,3) (76.9%, 98.5%), ce_loss 0.889, lat_loss 21.901
09/28 10:59:25 AM | Train: [ 57/180] Step 150/1249 Loss 5.747 Prec@(1,3) (77.4%, 98.3%), ce_loss 0.889, lat_loss 21.901
09/28 10:59:50 AM | Train: [ 57/180] Step 200/1249 Loss 5.731 Prec@(1,3) (77.8%, 98.3%), ce_loss 0.889, lat_loss 21.901
09/28 11:00:15 AM | Train: [ 57/180] Step 250/1249 Loss 5.761 Prec@(1,3) (77.6%, 98.4%), ce_loss 0.889, lat_loss 21.901
09/28 11:00:40 AM | Train: [ 57/180] Step 300/1249 Loss 5.733 Prec@(1,3) (77.9%, 98.5%), ce_loss 0.889, lat_loss 21.900
09/28 11:01:05 AM | Train: [ 57/180] Step 350/1249 Loss 5.678 Prec@(1,3) (78.2%, 98.5%), ce_loss 0.888, lat_loss 21.900
09/28 11:01:29 AM | Train: [ 57/180] Step 400/1249 Loss 5.681 Prec@(1,3) (78.3%, 98.5%), ce_loss 0.888, lat_loss 21.900
09/28 11:01:53 AM | Train: [ 57/180] Step 450/1249 Loss 5.668 Prec@(1,3) (78.4%, 98.5%), ce_loss 0.888, lat_loss 21.900
09/28 11:02:09 AM | Train: [ 57/180] Step 500/1249 Loss 5.627 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.888, lat_loss 21.900
09/28 11:02:25 AM | Train: [ 57/180] Step 550/1249 Loss 5.648 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.888, lat_loss 21.900
09/28 11:02:41 AM | Train: [ 57/180] Step 600/1249 Loss 5.648 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.888, lat_loss 21.900
09/28 11:02:57 AM | Train: [ 57/180] Step 650/1249 Loss 5.673 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.888, lat_loss 21.900
09/28 11:03:13 AM | Train: [ 57/180] Step 700/1249 Loss 5.675 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.888, lat_loss 21.900
09/28 11:03:29 AM | Train: [ 57/180] Step 750/1249 Loss 5.692 Prec@(1,3) (78.4%, 98.5%), ce_loss 0.887, lat_loss 21.900
09/28 11:03:45 AM | Train: [ 57/180] Step 800/1249 Loss 5.686 Prec@(1,3) (78.4%, 98.5%), ce_loss 0.887, lat_loss 21.900
09/28 11:04:01 AM | Train: [ 57/180] Step 850/1249 Loss 5.674 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.887, lat_loss 21.900
09/28 11:04:17 AM | Train: [ 57/180] Step 900/1249 Loss 5.670 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.887, lat_loss 21.900
09/28 11:04:33 AM | Train: [ 57/180] Step 950/1249 Loss 5.675 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.887, lat_loss 21.900
09/28 11:04:48 AM | Train: [ 57/180] Step 1000/1249 Loss 5.672 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.887, lat_loss 21.900
09/28 11:05:13 AM | Train: [ 57/180] Step 1050/1249 Loss 5.674 Prec@(1,3) (78.5%, 98.5%), ce_loss 0.887, lat_loss 21.900
09/28 11:05:37 AM | Train: [ 57/180] Step 1100/1249 Loss 5.694 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.887, lat_loss 21.900
09/28 11:06:02 AM | Train: [ 57/180] Step 1150/1249 Loss 5.698 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.887, lat_loss 21.900
09/28 11:06:27 AM | Train: [ 57/180] Step 1200/1249 Loss 5.702 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.886, lat_loss 21.900
09/28 11:06:51 AM | Train: [ 57/180] Step 1249/1249 Loss 5.697 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.886, lat_loss 21.900
09/28 11:06:52 AM | _w_step_train: [ 57/180] Final Prec@1 78.5200% Time 514.64
09/28 11:06:52 AM | Start to train theta for epoch 56
09/28 11:07:10 AM | Train: [ 57/180] Step 050/312 Loss 6.211 Prec@(1,3) (78.1%, 97.8%), ce_loss 0.886, lat_loss 21.899
09/28 11:07:27 AM | Train: [ 57/180] Step 100/312 Loss 6.205 Prec@(1,3) (77.7%, 97.7%), ce_loss 0.886, lat_loss 21.899
09/28 11:07:43 AM | Train: [ 57/180] Step 150/312 Loss 6.276 Prec@(1,3) (77.1%, 97.8%), ce_loss 0.886, lat_loss 21.899
09/28 11:08:02 AM | Train: [ 57/180] Step 200/312 Loss 6.248 Prec@(1,3) (77.3%, 97.9%), ce_loss 0.886, lat_loss 21.899
09/28 11:08:23 AM | Train: [ 57/180] Step 250/312 Loss 6.248 Prec@(1,3) (77.2%, 97.9%), ce_loss 0.886, lat_loss 21.899
09/28 11:08:44 AM | Train: [ 57/180] Step 300/312 Loss 6.207 Prec@(1,3) (77.3%, 97.9%), ce_loss 0.886, lat_loss 21.899
09/28 11:08:49 AM | Train: [ 57/180] Step 312/312 Loss 6.191 Prec@(1,3) (77.4%, 97.9%), ce_loss 0.886, lat_loss 21.899
09/28 11:08:49 AM | _theta_step_train: [ 57/180] Final Prec@1 77.4000% Time 117.08
09/28 11:08:54 AM | Valid: [ 57/180] Step 050/312 Loss 6.232 Prec@(1,3) (76.5%, 98.2%), ce_loss 0.886, lat_loss 21.899
09/28 11:08:59 AM | Valid: [ 57/180] Step 100/312 Loss 6.526 Prec@(1,3) (76.0%, 97.6%), ce_loss 0.886, lat_loss 21.899
09/28 11:09:03 AM | Valid: [ 57/180] Step 150/312 Loss 6.502 Prec@(1,3) (76.5%, 97.5%), ce_loss 0.886, lat_loss 21.899
09/28 11:09:08 AM | Valid: [ 57/180] Step 200/312 Loss 6.539 Prec@(1,3) (76.7%, 97.5%), ce_loss 0.885, lat_loss 21.899
09/28 11:09:12 AM | Valid: [ 57/180] Step 250/312 Loss 6.526 Prec@(1,3) (76.8%, 97.7%), ce_loss 0.885, lat_loss 21.899
09/28 11:09:17 AM | Valid: [ 57/180] Step 300/312 Loss 6.519 Prec@(1,3) (76.8%, 97.7%), ce_loss 0.885, lat_loss 21.899
09/28 11:09:18 AM | Valid: [ 57/180] Step 312/312 Loss 6.521 Prec@(1,3) (76.9%, 97.7%), ce_loss 0.885, lat_loss 21.899
09/28 11:09:18 AM | val: [ 57/180] Final Prec@1 76.8600% Time 29.46
09/28 11:09:18 AM | Start to train weights for epoch 57
09/28 11:09:45 AM | Train: [ 58/180] Step 050/1249 Loss 5.928 Prec@(1,3) (77.9%, 98.0%), ce_loss 0.885, lat_loss 21.899
09/28 11:10:10 AM | Train: [ 58/180] Step 100/1249 Loss 5.806 Prec@(1,3) (77.8%, 98.3%), ce_loss 0.885, lat_loss 21.899
09/28 11:10:35 AM | Train: [ 58/180] Step 150/1249 Loss 5.789 Prec@(1,3) (78.3%, 98.2%), ce_loss 0.885, lat_loss 21.899
09/28 11:11:00 AM | Train: [ 58/180] Step 200/1249 Loss 5.760 Prec@(1,3) (78.5%, 98.1%), ce_loss 0.885, lat_loss 21.899
09/28 11:11:25 AM | Train: [ 58/180] Step 250/1249 Loss 5.773 Prec@(1,3) (78.3%, 98.1%), ce_loss 0.885, lat_loss 21.899
09/28 11:11:50 AM | Train: [ 58/180] Step 300/1249 Loss 5.720 Prec@(1,3) (78.4%, 98.3%), ce_loss 0.885, lat_loss 21.898
09/28 11:12:15 AM | Train: [ 58/180] Step 350/1249 Loss 5.658 Prec@(1,3) (78.7%, 98.3%), ce_loss 0.884, lat_loss 21.898
09/28 11:12:40 AM | Train: [ 58/180] Step 400/1249 Loss 5.646 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.884, lat_loss 21.898
09/28 11:13:05 AM | Train: [ 58/180] Step 450/1249 Loss 5.599 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.884, lat_loss 21.898
09/28 11:13:31 AM | Train: [ 58/180] Step 500/1249 Loss 5.625 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.884, lat_loss 21.898
09/28 11:13:56 AM | Train: [ 58/180] Step 550/1249 Loss 5.622 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.884, lat_loss 21.898
09/28 11:14:21 AM | Train: [ 58/180] Step 600/1249 Loss 5.622 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.884, lat_loss 21.898
09/28 11:14:46 AM | Train: [ 58/180] Step 650/1249 Loss 5.598 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.884, lat_loss 21.898
09/28 11:15:11 AM | Train: [ 58/180] Step 700/1249 Loss 5.584 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.884, lat_loss 21.898
09/28 11:15:36 AM | Train: [ 58/180] Step 750/1249 Loss 5.593 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.884, lat_loss 21.898
09/28 11:16:01 AM | Train: [ 58/180] Step 800/1249 Loss 5.607 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.883, lat_loss 21.898
09/28 11:16:26 AM | Train: [ 58/180] Step 850/1249 Loss 5.607 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.883, lat_loss 21.898
09/28 11:16:49 AM | Train: [ 58/180] Step 900/1249 Loss 5.608 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.883, lat_loss 21.898
09/28 11:17:14 AM | Train: [ 58/180] Step 950/1249 Loss 5.596 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.883, lat_loss 21.898
09/28 11:17:37 AM | Train: [ 58/180] Step 1000/1249 Loss 5.633 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.883, lat_loss 21.898
09/28 11:18:01 AM | Train: [ 58/180] Step 1050/1249 Loss 5.631 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.883, lat_loss 21.898
09/28 11:18:18 AM | Train: [ 58/180] Step 1100/1249 Loss 5.617 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.883, lat_loss 21.898
09/28 11:18:34 AM | Train: [ 58/180] Step 1150/1249 Loss 5.627 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.883, lat_loss 21.898
09/28 11:18:50 AM | Train: [ 58/180] Step 1200/1249 Loss 5.656 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.883, lat_loss 21.897
09/28 11:19:06 AM | Train: [ 58/180] Step 1249/1249 Loss 5.665 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.882, lat_loss 21.897
09/28 11:19:06 AM | _w_step_train: [ 58/180] Final Prec@1 78.6650% Time 587.74
09/28 11:19:06 AM | Start to train theta for epoch 57
09/28 11:19:27 AM | Train: [ 58/180] Step 050/312 Loss 6.236 Prec@(1,3) (77.1%, 98.0%), ce_loss 0.882, lat_loss 21.897
09/28 11:19:48 AM | Train: [ 58/180] Step 100/312 Loss 6.013 Prec@(1,3) (77.7%, 98.0%), ce_loss 0.882, lat_loss 21.897
09/28 11:20:08 AM | Train: [ 58/180] Step 150/312 Loss 6.000 Prec@(1,3) (77.6%, 98.2%), ce_loss 0.882, lat_loss 21.897
09/28 11:20:29 AM | Train: [ 58/180] Step 200/312 Loss 5.889 Prec@(1,3) (77.7%, 98.3%), ce_loss 0.882, lat_loss 21.897
09/28 11:20:50 AM | Train: [ 58/180] Step 250/312 Loss 5.910 Prec@(1,3) (77.5%, 98.3%), ce_loss 0.882, lat_loss 21.897
09/28 11:21:10 AM | Train: [ 58/180] Step 300/312 Loss 5.952 Prec@(1,3) (77.4%, 98.2%), ce_loss 0.882, lat_loss 21.897
09/28 11:21:15 AM | Train: [ 58/180] Step 312/312 Loss 5.952 Prec@(1,3) (77.4%, 98.2%), ce_loss 0.882, lat_loss 21.897
09/28 11:21:15 AM | _theta_step_train: [ 58/180] Final Prec@1 77.3600% Time 129.48
09/28 11:21:21 AM | Valid: [ 58/180] Step 050/312 Loss 6.740 Prec@(1,3) (76.5%, 97.3%), ce_loss 0.882, lat_loss 21.897
09/28 11:21:25 AM | Valid: [ 58/180] Step 100/312 Loss 6.479 Prec@(1,3) (77.5%, 97.8%), ce_loss 0.882, lat_loss 21.897
09/28 11:21:30 AM | Valid: [ 58/180] Step 150/312 Loss 6.485 Prec@(1,3) (77.5%, 97.4%), ce_loss 0.882, lat_loss 21.897
09/28 11:21:35 AM | Valid: [ 58/180] Step 200/312 Loss 6.467 Prec@(1,3) (77.3%, 97.5%), ce_loss 0.882, lat_loss 21.897
09/28 11:21:39 AM | Valid: [ 58/180] Step 250/312 Loss 6.371 Prec@(1,3) (77.7%, 97.6%), ce_loss 0.881, lat_loss 21.897
09/28 11:21:44 AM | Valid: [ 58/180] Step 300/312 Loss 6.310 Prec@(1,3) (77.8%, 97.6%), ce_loss 0.881, lat_loss 21.897
09/28 11:21:45 AM | Valid: [ 58/180] Step 312/312 Loss 6.291 Prec@(1,3) (77.7%, 97.7%), ce_loss 0.881, lat_loss 21.897
09/28 11:21:45 AM | val: [ 58/180] Final Prec@1 77.7300% Time 29.47
09/28 11:21:45 AM | Best top1 acc by now. Save model
09/28 11:21:45 AM | Start to train weights for epoch 58
09/28 11:22:12 AM | Train: [ 59/180] Step 050/1249 Loss 5.685 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.881, lat_loss 21.897
09/28 11:22:35 AM | Train: [ 59/180] Step 100/1249 Loss 5.665 Prec@(1,3) (78.9%, 98.3%), ce_loss 0.881, lat_loss 21.897
09/28 11:22:59 AM | Train: [ 59/180] Step 150/1249 Loss 5.676 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.881, lat_loss 21.897
09/28 11:23:23 AM | Train: [ 59/180] Step 200/1249 Loss 5.668 Prec@(1,3) (78.6%, 98.6%), ce_loss 0.881, lat_loss 21.897
09/28 11:23:47 AM | Train: [ 59/180] Step 250/1249 Loss 5.643 Prec@(1,3) (78.7%, 98.6%), ce_loss 0.881, lat_loss 21.896
09/28 11:24:11 AM | Train: [ 59/180] Step 300/1249 Loss 5.630 Prec@(1,3) (78.6%, 98.6%), ce_loss 0.881, lat_loss 21.896
09/28 11:24:35 AM | Train: [ 59/180] Step 350/1249 Loss 5.650 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.881, lat_loss 21.896
09/28 11:24:59 AM | Train: [ 59/180] Step 400/1249 Loss 5.649 Prec@(1,3) (78.9%, 98.4%), ce_loss 0.880, lat_loss 21.896
09/28 11:25:24 AM | Train: [ 59/180] Step 450/1249 Loss 5.648 Prec@(1,3) (78.9%, 98.4%), ce_loss 0.880, lat_loss 21.896
09/28 11:25:47 AM | Train: [ 59/180] Step 500/1249 Loss 5.604 Prec@(1,3) (79.1%, 98.5%), ce_loss 0.880, lat_loss 21.896
09/28 11:26:12 AM | Train: [ 59/180] Step 550/1249 Loss 5.612 Prec@(1,3) (79.1%, 98.4%), ce_loss 0.880, lat_loss 21.896
09/28 11:26:36 AM | Train: [ 59/180] Step 600/1249 Loss 5.602 Prec@(1,3) (79.0%, 98.5%), ce_loss 0.880, lat_loss 21.896
09/28 11:27:00 AM | Train: [ 59/180] Step 650/1249 Loss 5.640 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.880, lat_loss 21.896
09/28 11:27:25 AM | Train: [ 59/180] Step 700/1249 Loss 5.614 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.880, lat_loss 21.896
09/28 11:27:49 AM | Train: [ 59/180] Step 750/1249 Loss 5.597 Prec@(1,3) (79.0%, 98.5%), ce_loss 0.880, lat_loss 21.896
09/28 11:28:13 AM | Train: [ 59/180] Step 800/1249 Loss 5.606 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.879, lat_loss 21.896
09/28 11:28:39 AM | Train: [ 59/180] Step 850/1249 Loss 5.615 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.879, lat_loss 21.896
09/28 11:29:04 AM | Train: [ 59/180] Step 900/1249 Loss 5.619 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.879, lat_loss 21.896
09/28 11:29:29 AM | Train: [ 59/180] Step 950/1249 Loss 5.631 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.879, lat_loss 21.896
09/28 11:29:54 AM | Train: [ 59/180] Step 1000/1249 Loss 5.638 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.879, lat_loss 21.896
09/28 11:30:19 AM | Train: [ 59/180] Step 1050/1249 Loss 5.646 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.879, lat_loss 21.896
09/28 11:30:44 AM | Train: [ 59/180] Step 1100/1249 Loss 5.639 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.879, lat_loss 21.895
09/28 11:31:09 AM | Train: [ 59/180] Step 1150/1249 Loss 5.648 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.879, lat_loss 21.895
09/28 11:31:35 AM | Train: [ 59/180] Step 1200/1249 Loss 5.642 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.879, lat_loss 21.895
09/28 11:31:59 AM | Train: [ 59/180] Step 1249/1249 Loss 5.633 Prec@(1,3) (78.8%, 98.5%), ce_loss 0.879, lat_loss 21.895
09/28 11:31:59 AM | _w_step_train: [ 59/180] Final Prec@1 78.7775% Time 614.24
09/28 11:31:59 AM | Start to train theta for epoch 58
09/28 11:32:21 AM | Train: [ 59/180] Step 050/312 Loss 6.636 Prec@(1,3) (75.8%, 97.9%), ce_loss 0.878, lat_loss 21.895
09/28 11:32:42 AM | Train: [ 59/180] Step 100/312 Loss 6.441 Prec@(1,3) (76.3%, 98.0%), ce_loss 0.878, lat_loss 21.895
09/28 11:33:03 AM | Train: [ 59/180] Step 150/312 Loss 6.279 Prec@(1,3) (76.8%, 98.2%), ce_loss 0.878, lat_loss 21.895
09/28 11:33:24 AM | Train: [ 59/180] Step 200/312 Loss 6.135 Prec@(1,3) (77.0%, 98.2%), ce_loss 0.878, lat_loss 21.895
09/28 11:33:45 AM | Train: [ 59/180] Step 250/312 Loss 6.092 Prec@(1,3) (77.1%, 98.3%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:05 AM | Train: [ 59/180] Step 300/312 Loss 6.055 Prec@(1,3) (77.3%, 98.4%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:10 AM | Train: [ 59/180] Step 312/312 Loss 6.072 Prec@(1,3) (77.1%, 98.4%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:11 AM | _theta_step_train: [ 59/180] Final Prec@1 77.1500% Time 131.49
09/28 11:34:16 AM | Valid: [ 59/180] Step 050/312 Loss 6.757 Prec@(1,3) (75.4%, 98.4%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:21 AM | Valid: [ 59/180] Step 100/312 Loss 6.538 Prec@(1,3) (76.1%, 98.1%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:25 AM | Valid: [ 59/180] Step 150/312 Loss 6.442 Prec@(1,3) (76.3%, 98.0%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:30 AM | Valid: [ 59/180] Step 200/312 Loss 6.425 Prec@(1,3) (76.3%, 97.9%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:35 AM | Valid: [ 59/180] Step 250/312 Loss 6.438 Prec@(1,3) (76.2%, 98.0%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:39 AM | Valid: [ 59/180] Step 300/312 Loss 6.342 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:40 AM | Valid: [ 59/180] Step 312/312 Loss 6.356 Prec@(1,3) (76.7%, 98.1%), ce_loss 0.878, lat_loss 21.895
09/28 11:34:40 AM | val: [ 59/180] Final Prec@1 76.7100% Time 29.68
09/28 11:34:40 AM | Start to train weights for epoch 59
09/28 11:34:58 AM | Train: [ 60/180] Step 050/1249 Loss 5.408 Prec@(1,3) (79.8%, 98.7%), ce_loss 0.877, lat_loss 21.895
09/28 11:35:15 AM | Train: [ 60/180] Step 100/1249 Loss 5.372 Prec@(1,3) (79.6%, 98.6%), ce_loss 0.877, lat_loss 21.895
09/28 11:35:40 AM | Train: [ 60/180] Step 150/1249 Loss 5.517 Prec@(1,3) (79.2%, 98.6%), ce_loss 0.877, lat_loss 21.895
09/28 11:36:05 AM | Train: [ 60/180] Step 200/1249 Loss 5.605 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.877, lat_loss 21.894
09/28 11:36:30 AM | Train: [ 60/180] Step 250/1249 Loss 5.617 Prec@(1,3) (79.0%, 98.5%), ce_loss 0.877, lat_loss 21.894
09/28 11:36:54 AM | Train: [ 60/180] Step 300/1249 Loss 5.565 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.877, lat_loss 21.894
09/28 11:37:19 AM | Train: [ 60/180] Step 350/1249 Loss 5.539 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.877, lat_loss 21.894
09/28 11:37:44 AM | Train: [ 60/180] Step 400/1249 Loss 5.504 Prec@(1,3) (79.4%, 98.5%), ce_loss 0.877, lat_loss 21.894
09/28 11:38:09 AM | Train: [ 60/180] Step 450/1249 Loss 5.451 Prec@(1,3) (79.6%, 98.6%), ce_loss 0.876, lat_loss 21.894
09/28 11:38:33 AM | Train: [ 60/180] Step 500/1249 Loss 5.480 Prec@(1,3) (79.5%, 98.6%), ce_loss 0.876, lat_loss 21.894
09/28 11:38:58 AM | Train: [ 60/180] Step 550/1249 Loss 5.473 Prec@(1,3) (79.6%, 98.6%), ce_loss 0.876, lat_loss 21.894
09/28 11:39:22 AM | Train: [ 60/180] Step 600/1249 Loss 5.519 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.876, lat_loss 21.894
09/28 11:39:47 AM | Train: [ 60/180] Step 650/1249 Loss 5.533 Prec@(1,3) (79.4%, 98.5%), ce_loss 0.876, lat_loss 21.894
09/28 11:40:12 AM | Train: [ 60/180] Step 700/1249 Loss 5.547 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.876, lat_loss 21.894
09/28 11:40:36 AM | Train: [ 60/180] Step 750/1249 Loss 5.559 Prec@(1,3) (79.4%, 98.5%), ce_loss 0.876, lat_loss 21.894
09/28 11:41:01 AM | Train: [ 60/180] Step 800/1249 Loss 5.535 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.876, lat_loss 21.894
09/28 11:41:26 AM | Train: [ 60/180] Step 850/1249 Loss 5.522 Prec@(1,3) (79.6%, 98.5%), ce_loss 0.876, lat_loss 21.894
09/28 11:41:50 AM | Train: [ 60/180] Step 900/1249 Loss 5.518 Prec@(1,3) (79.6%, 98.5%), ce_loss 0.875, lat_loss 21.894
09/28 11:42:15 AM | Train: [ 60/180] Step 950/1249 Loss 5.501 Prec@(1,3) (79.6%, 98.5%), ce_loss 0.875, lat_loss 21.894
09/28 11:42:39 AM | Train: [ 60/180] Step 1000/1249 Loss 5.490 Prec@(1,3) (79.6%, 98.5%), ce_loss 0.875, lat_loss 21.894
09/28 11:42:55 AM | Train: [ 60/180] Step 1050/1249 Loss 5.500 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.875, lat_loss 21.894
09/28 11:43:11 AM | Train: [ 60/180] Step 1100/1249 Loss 5.511 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.875, lat_loss 21.894
09/28 11:43:27 AM | Train: [ 60/180] Step 1150/1249 Loss 5.510 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.875, lat_loss 21.894
09/28 11:43:43 AM | Train: [ 60/180] Step 1200/1249 Loss 5.509 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.875, lat_loss 21.894
09/28 11:43:59 AM | Train: [ 60/180] Step 1249/1249 Loss 5.522 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.875, lat_loss 21.893
09/28 11:43:59 AM | _w_step_train: [ 60/180] Final Prec@1 79.5050% Time 558.91
09/28 11:43:59 AM | Start to train theta for epoch 59
09/28 11:44:21 AM | Train: [ 60/180] Step 050/312 Loss 5.969 Prec@(1,3) (77.5%, 97.9%), ce_loss 0.875, lat_loss 21.893
09/28 11:44:41 AM | Train: [ 60/180] Step 100/312 Loss 5.876 Prec@(1,3) (77.7%, 98.1%), ce_loss 0.874, lat_loss 21.893
09/28 11:44:57 AM | Train: [ 60/180] Step 150/312 Loss 5.970 Prec@(1,3) (77.4%, 98.2%), ce_loss 0.874, lat_loss 21.893
09/28 11:45:14 AM | Train: [ 60/180] Step 200/312 Loss 5.845 Prec@(1,3) (77.8%, 98.3%), ce_loss 0.874, lat_loss 21.893
09/28 11:45:30 AM | Train: [ 60/180] Step 250/312 Loss 5.864 Prec@(1,3) (77.8%, 98.2%), ce_loss 0.874, lat_loss 21.893
09/28 11:45:48 AM | Train: [ 60/180] Step 300/312 Loss 5.938 Prec@(1,3) (77.6%, 98.1%), ce_loss 0.874, lat_loss 21.893
09/28 11:45:53 AM | Train: [ 60/180] Step 312/312 Loss 5.923 Prec@(1,3) (77.7%, 98.2%), ce_loss 0.874, lat_loss 21.893
09/28 11:45:53 AM | _theta_step_train: [ 60/180] Final Prec@1 77.6800% Time 113.63
09/28 11:45:58 AM | Valid: [ 60/180] Step 050/312 Loss 6.104 Prec@(1,3) (78.0%, 98.5%), ce_loss 0.874, lat_loss 21.893
09/28 11:46:03 AM | Valid: [ 60/180] Step 100/312 Loss 6.205 Prec@(1,3) (77.6%, 97.9%), ce_loss 0.874, lat_loss 21.893
09/28 11:46:08 AM | Valid: [ 60/180] Step 150/312 Loss 6.186 Prec@(1,3) (77.0%, 97.7%), ce_loss 0.874, lat_loss 21.893
09/28 11:46:13 AM | Valid: [ 60/180] Step 200/312 Loss 6.104 Prec@(1,3) (77.4%, 97.8%), ce_loss 0.874, lat_loss 21.893
09/28 11:46:17 AM | Valid: [ 60/180] Step 250/312 Loss 6.098 Prec@(1,3) (77.4%, 98.0%), ce_loss 0.874, lat_loss 21.893
09/28 11:46:22 AM | Valid: [ 60/180] Step 300/312 Loss 6.078 Prec@(1,3) (77.2%, 98.0%), ce_loss 0.874, lat_loss 21.893
09/28 11:46:23 AM | Valid: [ 60/180] Step 312/312 Loss 6.059 Prec@(1,3) (77.3%, 98.0%), ce_loss 0.874, lat_loss 21.893
09/28 11:46:23 AM | val: [ 60/180] Final Prec@1 77.3000% Time 30.01
09/28 11:46:23 AM | Start to train weights for epoch 60
09/28 11:46:47 AM | Train: [ 61/180] Step 050/1249 Loss 5.695 Prec@(1,3) (78.9%, 98.0%), ce_loss 0.873, lat_loss 21.893
09/28 11:47:09 AM | Train: [ 61/180] Step 100/1249 Loss 5.481 Prec@(1,3) (79.4%, 98.7%), ce_loss 0.873, lat_loss 21.893
09/28 11:47:33 AM | Train: [ 61/180] Step 150/1249 Loss 5.316 Prec@(1,3) (79.8%, 98.7%), ce_loss 0.873, lat_loss 21.893
09/28 11:47:57 AM | Train: [ 61/180] Step 200/1249 Loss 5.398 Prec@(1,3) (79.9%, 98.6%), ce_loss 0.873, lat_loss 21.893
09/28 11:48:21 AM | Train: [ 61/180] Step 250/1249 Loss 5.449 Prec@(1,3) (79.5%, 98.6%), ce_loss 0.873, lat_loss 21.892
09/28 11:48:46 AM | Train: [ 61/180] Step 300/1249 Loss 5.498 Prec@(1,3) (79.4%, 98.6%), ce_loss 0.873, lat_loss 21.892
09/28 11:49:09 AM | Train: [ 61/180] Step 350/1249 Loss 5.579 Prec@(1,3) (79.1%, 98.5%), ce_loss 0.873, lat_loss 21.892
09/28 11:49:32 AM | Train: [ 61/180] Step 400/1249 Loss 5.560 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.873, lat_loss 21.892
09/28 11:49:54 AM | Train: [ 61/180] Step 450/1249 Loss 5.574 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.873, lat_loss 21.892
09/28 11:50:16 AM | Train: [ 61/180] Step 500/1249 Loss 5.542 Prec@(1,3) (79.3%, 98.5%), ce_loss 0.872, lat_loss 21.892
09/28 11:50:40 AM | Train: [ 61/180] Step 550/1249 Loss 5.551 Prec@(1,3) (79.3%, 98.5%), ce_loss 0.872, lat_loss 21.892
09/28 11:51:04 AM | Train: [ 61/180] Step 600/1249 Loss 5.533 Prec@(1,3) (79.3%, 98.5%), ce_loss 0.872, lat_loss 21.892
09/28 11:51:29 AM | Train: [ 61/180] Step 650/1249 Loss 5.546 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.872, lat_loss 21.892
09/28 11:51:53 AM | Train: [ 61/180] Step 700/1249 Loss 5.549 Prec@(1,3) (79.3%, 98.5%), ce_loss 0.872, lat_loss 21.892
09/28 11:52:16 AM | Train: [ 61/180] Step 750/1249 Loss 5.581 Prec@(1,3) (79.2%, 98.4%), ce_loss 0.872, lat_loss 21.892
09/28 11:52:40 AM | Train: [ 61/180] Step 800/1249 Loss 5.582 Prec@(1,3) (79.2%, 98.4%), ce_loss 0.872, lat_loss 21.892
09/28 11:53:03 AM | Train: [ 61/180] Step 850/1249 Loss 5.577 Prec@(1,3) (79.2%, 98.4%), ce_loss 0.872, lat_loss 21.892
09/28 11:53:24 AM | Train: [ 61/180] Step 900/1249 Loss 5.563 Prec@(1,3) (79.2%, 98.4%), ce_loss 0.872, lat_loss 21.892
09/28 11:53:44 AM | Train: [ 61/180] Step 950/1249 Loss 5.567 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.871, lat_loss 21.892
09/28 11:54:04 AM | Train: [ 61/180] Step 1000/1249 Loss 5.555 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.871, lat_loss 21.892
09/28 11:54:24 AM | Train: [ 61/180] Step 1050/1249 Loss 5.565 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.871, lat_loss 21.891
09/28 11:54:44 AM | Train: [ 61/180] Step 1100/1249 Loss 5.558 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.871, lat_loss 21.891
09/28 11:55:04 AM | Train: [ 61/180] Step 1150/1249 Loss 5.555 Prec@(1,3) (79.2%, 98.5%), ce_loss 0.871, lat_loss 21.891
09/28 11:55:24 AM | Train: [ 61/180] Step 1200/1249 Loss 5.539 Prec@(1,3) (79.3%, 98.5%), ce_loss 0.871, lat_loss 21.891
09/28 11:55:46 AM | Train: [ 61/180] Step 1249/1249 Loss 5.525 Prec@(1,3) (79.3%, 98.5%), ce_loss 0.871, lat_loss 21.891
09/28 11:55:47 AM | _w_step_train: [ 61/180] Final Prec@1 79.3250% Time 563.50
09/28 11:55:47 AM | Start to train theta for epoch 60
09/28 11:56:06 AM | Train: [ 61/180] Step 050/312 Loss 6.123 Prec@(1,3) (78.0%, 98.1%), ce_loss 0.871, lat_loss 21.891
09/28 11:56:27 AM | Train: [ 61/180] Step 100/312 Loss 5.817 Prec@(1,3) (78.8%, 98.1%), ce_loss 0.871, lat_loss 21.891
09/28 11:56:48 AM | Train: [ 61/180] Step 150/312 Loss 5.766 Prec@(1,3) (78.7%, 98.3%), ce_loss 0.870, lat_loss 21.891
09/28 11:57:08 AM | Train: [ 61/180] Step 200/312 Loss 5.692 Prec@(1,3) (78.7%, 98.4%), ce_loss 0.870, lat_loss 21.891
09/28 11:57:28 AM | Train: [ 61/180] Step 250/312 Loss 5.691 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.870, lat_loss 21.891
09/28 11:57:48 AM | Train: [ 61/180] Step 300/312 Loss 5.649 Prec@(1,3) (78.9%, 98.4%), ce_loss 0.870, lat_loss 21.891
09/28 11:57:52 AM | Train: [ 61/180] Step 312/312 Loss 5.622 Prec@(1,3) (79.0%, 98.4%), ce_loss 0.870, lat_loss 21.891
09/28 11:57:52 AM | _theta_step_train: [ 61/180] Final Prec@1 78.9600% Time 125.76
09/28 11:57:59 AM | Valid: [ 61/180] Step 050/312 Loss 5.410 Prec@(1,3) (80.5%, 98.8%), ce_loss 0.870, lat_loss 21.891
09/28 11:58:03 AM | Valid: [ 61/180] Step 100/312 Loss 5.751 Prec@(1,3) (79.0%, 97.9%), ce_loss 0.870, lat_loss 21.891
09/28 11:58:08 AM | Valid: [ 61/180] Step 150/312 Loss 5.825 Prec@(1,3) (78.7%, 97.9%), ce_loss 0.870, lat_loss 21.891
09/28 11:58:12 AM | Valid: [ 61/180] Step 200/312 Loss 5.854 Prec@(1,3) (78.4%, 98.0%), ce_loss 0.870, lat_loss 21.891
09/28 11:58:17 AM | Valid: [ 61/180] Step 250/312 Loss 6.020 Prec@(1,3) (77.9%, 97.8%), ce_loss 0.870, lat_loss 21.890
09/28 11:58:22 AM | Valid: [ 61/180] Step 300/312 Loss 5.954 Prec@(1,3) (78.1%, 98.0%), ce_loss 0.870, lat_loss 21.890
09/28 11:58:23 AM | Valid: [ 61/180] Step 312/312 Loss 5.967 Prec@(1,3) (78.0%, 98.0%), ce_loss 0.870, lat_loss 21.890
09/28 11:58:23 AM | val: [ 61/180] Final Prec@1 78.0200% Time 30.61
09/28 11:58:23 AM | Best top1 acc by now. Save model
09/28 11:58:23 AM | Start to train weights for epoch 61
09/28 11:58:46 AM | Train: [ 62/180] Step 050/1249 Loss 4.915 Prec@(1,3) (81.2%, 98.8%), ce_loss 0.869, lat_loss 21.890
09/28 11:59:07 AM | Train: [ 62/180] Step 100/1249 Loss 5.116 Prec@(1,3) (80.4%, 98.5%), ce_loss 0.869, lat_loss 21.890
09/28 11:59:27 AM | Train: [ 62/180] Step 150/1249 Loss 5.107 Prec@(1,3) (80.5%, 98.7%), ce_loss 0.869, lat_loss 21.890
09/28 11:59:50 AM | Train: [ 62/180] Step 200/1249 Loss 5.204 Prec@(1,3) (79.9%, 98.7%), ce_loss 0.869, lat_loss 21.890
09/28 12:00:12 PM | Train: [ 62/180] Step 250/1249 Loss 5.176 Prec@(1,3) (80.1%, 98.7%), ce_loss 0.869, lat_loss 21.890
09/28 12:00:35 PM | Train: [ 62/180] Step 300/1249 Loss 5.237 Prec@(1,3) (79.9%, 98.8%), ce_loss 0.869, lat_loss 21.890
09/28 12:00:58 PM | Train: [ 62/180] Step 350/1249 Loss 5.283 Prec@(1,3) (79.8%, 98.8%), ce_loss 0.869, lat_loss 21.890
09/28 12:01:21 PM | Train: [ 62/180] Step 400/1249 Loss 5.299 Prec@(1,3) (79.8%, 98.8%), ce_loss 0.869, lat_loss 21.890
09/28 12:01:44 PM | Train: [ 62/180] Step 450/1249 Loss 5.330 Prec@(1,3) (79.7%, 98.7%), ce_loss 0.869, lat_loss 21.890
09/28 12:02:07 PM | Train: [ 62/180] Step 500/1249 Loss 5.332 Prec@(1,3) (79.7%, 98.7%), ce_loss 0.868, lat_loss 21.890
09/28 12:02:29 PM | Train: [ 62/180] Step 550/1249 Loss 5.366 Prec@(1,3) (79.5%, 98.7%), ce_loss 0.868, lat_loss 21.890
09/28 12:02:51 PM | Train: [ 62/180] Step 600/1249 Loss 5.382 Prec@(1,3) (79.5%, 98.7%), ce_loss 0.868, lat_loss 21.890
09/28 12:03:12 PM | Train: [ 62/180] Step 650/1249 Loss 5.391 Prec@(1,3) (79.4%, 98.7%), ce_loss 0.868, lat_loss 21.889
09/28 12:03:33 PM | Train: [ 62/180] Step 700/1249 Loss 5.443 Prec@(1,3) (79.3%, 98.6%), ce_loss 0.868, lat_loss 21.889
09/28 12:03:55 PM | Train: [ 62/180] Step 750/1249 Loss 5.452 Prec@(1,3) (79.3%, 98.6%), ce_loss 0.868, lat_loss 21.889
09/28 12:04:17 PM | Train: [ 62/180] Step 800/1249 Loss 5.470 Prec@(1,3) (79.2%, 98.6%), ce_loss 0.868, lat_loss 21.889
09/28 12:04:40 PM | Train: [ 62/180] Step 850/1249 Loss 5.453 Prec@(1,3) (79.3%, 98.6%), ce_loss 0.868, lat_loss 21.889
09/28 12:05:03 PM | Train: [ 62/180] Step 900/1249 Loss 5.441 Prec@(1,3) (79.3%, 98.6%), ce_loss 0.868, lat_loss 21.889
09/28 12:05:27 PM | Train: [ 62/180] Step 950/1249 Loss 5.422 Prec@(1,3) (79.4%, 98.6%), ce_loss 0.867, lat_loss 21.889
09/28 12:05:50 PM | Train: [ 62/180] Step 1000/1249 Loss 5.404 Prec@(1,3) (79.4%, 98.6%), ce_loss 0.867, lat_loss 21.889
09/28 12:06:14 PM | Train: [ 62/180] Step 1050/1249 Loss 5.387 Prec@(1,3) (79.5%, 98.6%), ce_loss 0.867, lat_loss 21.889
09/28 12:06:37 PM | Train: [ 62/180] Step 1100/1249 Loss 5.396 Prec@(1,3) (79.5%, 98.6%), ce_loss 0.867, lat_loss 21.889
09/28 12:06:59 PM | Train: [ 62/180] Step 1150/1249 Loss 5.404 Prec@(1,3) (79.5%, 98.6%), ce_loss 0.867, lat_loss 21.889
09/28 12:07:23 PM | Train: [ 62/180] Step 1200/1249 Loss 5.404 Prec@(1,3) (79.5%, 98.6%), ce_loss 0.867, lat_loss 21.889
09/28 12:07:48 PM | Train: [ 62/180] Step 1249/1249 Loss 5.413 Prec@(1,3) (79.5%, 98.6%), ce_loss 0.867, lat_loss 21.889
09/28 12:07:48 PM | _w_step_train: [ 62/180] Final Prec@1 79.4725% Time 564.66
09/28 12:07:48 PM | Start to train theta for epoch 61
09/28 12:08:10 PM | Train: [ 62/180] Step 050/312 Loss 5.903 Prec@(1,3) (77.6%, 97.9%), ce_loss 0.867, lat_loss 21.889
09/28 12:08:31 PM | Train: [ 62/180] Step 100/312 Loss 5.732 Prec@(1,3) (77.8%, 98.2%), ce_loss 0.867, lat_loss 21.889
09/28 12:08:52 PM | Train: [ 62/180] Step 150/312 Loss 5.736 Prec@(1,3) (77.8%, 98.2%), ce_loss 0.866, lat_loss 21.889
09/28 12:09:13 PM | Train: [ 62/180] Step 200/312 Loss 5.771 Prec@(1,3) (77.8%, 98.3%), ce_loss 0.866, lat_loss 21.889
09/28 12:09:34 PM | Train: [ 62/180] Step 250/312 Loss 5.842 Prec@(1,3) (77.5%, 98.2%), ce_loss 0.866, lat_loss 21.888
09/28 12:09:55 PM | Train: [ 62/180] Step 300/312 Loss 5.799 Prec@(1,3) (77.7%, 98.3%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:00 PM | Train: [ 62/180] Step 312/312 Loss 5.806 Prec@(1,3) (77.6%, 98.2%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:00 PM | _theta_step_train: [ 62/180] Final Prec@1 77.6100% Time 132.06
09/28 12:10:05 PM | Valid: [ 62/180] Step 050/312 Loss 5.793 Prec@(1,3) (77.8%, 98.7%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:10 PM | Valid: [ 62/180] Step 100/312 Loss 5.944 Prec@(1,3) (77.3%, 98.6%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:15 PM | Valid: [ 62/180] Step 150/312 Loss 5.803 Prec@(1,3) (77.7%, 98.6%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:19 PM | Valid: [ 62/180] Step 200/312 Loss 5.832 Prec@(1,3) (77.4%, 98.5%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:24 PM | Valid: [ 62/180] Step 250/312 Loss 5.939 Prec@(1,3) (77.0%, 98.3%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:29 PM | Valid: [ 62/180] Step 300/312 Loss 5.887 Prec@(1,3) (77.4%, 98.4%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:30 PM | Valid: [ 62/180] Step 312/312 Loss 5.879 Prec@(1,3) (77.4%, 98.4%), ce_loss 0.866, lat_loss 21.888
09/28 12:10:30 PM | val: [ 62/180] Final Prec@1 77.4300% Time 30.20
09/28 12:10:30 PM | Start to train weights for epoch 62
09/28 12:10:55 PM | Train: [ 63/180] Step 050/1249 Loss 5.254 Prec@(1,3) (81.2%, 98.4%), ce_loss 0.866, lat_loss 21.888
09/28 12:11:20 PM | Train: [ 63/180] Step 100/1249 Loss 5.172 Prec@(1,3) (80.3%, 98.6%), ce_loss 0.865, lat_loss 21.888
09/28 12:11:44 PM | Train: [ 63/180] Step 150/1249 Loss 5.191 Prec@(1,3) (80.6%, 98.6%), ce_loss 0.865, lat_loss 21.888
09/28 12:12:08 PM | Train: [ 63/180] Step 200/1249 Loss 5.170 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.865, lat_loss 21.888
09/28 12:12:32 PM | Train: [ 63/180] Step 250/1249 Loss 5.181 Prec@(1,3) (80.6%, 98.7%), ce_loss 0.865, lat_loss 21.888
09/28 12:12:57 PM | Train: [ 63/180] Step 300/1249 Loss 5.242 Prec@(1,3) (80.1%, 98.7%), ce_loss 0.865, lat_loss 21.888
09/28 12:13:20 PM | Train: [ 63/180] Step 350/1249 Loss 5.311 Prec@(1,3) (80.0%, 98.6%), ce_loss 0.865, lat_loss 21.888
09/28 12:13:44 PM | Train: [ 63/180] Step 400/1249 Loss 5.311 Prec@(1,3) (80.0%, 98.6%), ce_loss 0.865, lat_loss 21.888
09/28 12:14:08 PM | Train: [ 63/180] Step 450/1249 Loss 5.266 Prec@(1,3) (80.1%, 98.6%), ce_loss 0.865, lat_loss 21.888
09/28 12:14:33 PM | Train: [ 63/180] Step 500/1249 Loss 5.281 Prec@(1,3) (80.0%, 98.5%), ce_loss 0.864, lat_loss 21.887
09/28 12:14:56 PM | Train: [ 63/180] Step 550/1249 Loss 5.293 Prec@(1,3) (79.9%, 98.6%), ce_loss 0.864, lat_loss 21.887
09/28 12:15:20 PM | Train: [ 63/180] Step 600/1249 Loss 5.312 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.864, lat_loss 21.887
09/28 12:15:44 PM | Train: [ 63/180] Step 650/1249 Loss 5.290 Prec@(1,3) (79.9%, 98.6%), ce_loss 0.864, lat_loss 21.887
09/28 12:16:07 PM | Train: [ 63/180] Step 700/1249 Loss 5.291 Prec@(1,3) (79.9%, 98.6%), ce_loss 0.864, lat_loss 21.887
09/28 12:16:31 PM | Train: [ 63/180] Step 750/1249 Loss 5.333 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.864, lat_loss 21.887
09/28 12:16:54 PM | Train: [ 63/180] Step 800/1249 Loss 5.331 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.864, lat_loss 21.887
09/28 12:17:17 PM | Train: [ 63/180] Step 850/1249 Loss 5.340 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.864, lat_loss 21.887
09/28 12:17:41 PM | Train: [ 63/180] Step 900/1249 Loss 5.338 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.864, lat_loss 21.887
09/28 12:18:04 PM | Train: [ 63/180] Step 950/1249 Loss 5.337 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.863, lat_loss 21.887
09/28 12:18:27 PM | Train: [ 63/180] Step 1000/1249 Loss 5.350 Prec@(1,3) (79.7%, 98.6%), ce_loss 0.863, lat_loss 21.887
09/28 12:18:51 PM | Train: [ 63/180] Step 1050/1249 Loss 5.325 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.863, lat_loss 21.887
09/28 12:19:15 PM | Train: [ 63/180] Step 1100/1249 Loss 5.332 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.863, lat_loss 21.887
09/28 12:19:38 PM | Train: [ 63/180] Step 1150/1249 Loss 5.358 Prec@(1,3) (79.7%, 98.6%), ce_loss 0.863, lat_loss 21.887
09/28 12:20:02 PM | Train: [ 63/180] Step 1200/1249 Loss 5.338 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.863, lat_loss 21.887
09/28 12:20:25 PM | Train: [ 63/180] Step 1249/1249 Loss 5.331 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.863, lat_loss 21.887
09/28 12:20:26 PM | _w_step_train: [ 63/180] Final Prec@1 79.7750% Time 595.44
09/28 12:20:26 PM | Start to train theta for epoch 62
09/28 12:20:44 PM | Train: [ 63/180] Step 050/312 Loss 6.150 Prec@(1,3) (77.6%, 98.5%), ce_loss 0.863, lat_loss 21.886
09/28 12:21:01 PM | Train: [ 63/180] Step 100/312 Loss 6.145 Prec@(1,3) (78.1%, 98.3%), ce_loss 0.863, lat_loss 21.886
09/28 12:21:18 PM | Train: [ 63/180] Step 150/312 Loss 5.991 Prec@(1,3) (78.6%, 98.3%), ce_loss 0.863, lat_loss 21.886
09/28 12:21:33 PM | Train: [ 63/180] Step 200/312 Loss 5.911 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.862, lat_loss 21.886
09/28 12:21:50 PM | Train: [ 63/180] Step 250/312 Loss 5.920 Prec@(1,3) (78.4%, 98.3%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:06 PM | Train: [ 63/180] Step 300/312 Loss 5.873 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:11 PM | Train: [ 63/180] Step 312/312 Loss 5.863 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:11 PM | _theta_step_train: [ 63/180] Final Prec@1 78.6100% Time 105.41
09/28 12:22:16 PM | Valid: [ 63/180] Step 050/312 Loss 6.064 Prec@(1,3) (78.9%, 97.7%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:21 PM | Valid: [ 63/180] Step 100/312 Loss 6.236 Prec@(1,3) (78.0%, 97.8%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:26 PM | Valid: [ 63/180] Step 150/312 Loss 6.415 Prec@(1,3) (77.9%, 98.0%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:30 PM | Valid: [ 63/180] Step 200/312 Loss 6.315 Prec@(1,3) (78.1%, 98.1%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:35 PM | Valid: [ 63/180] Step 250/312 Loss 6.175 Prec@(1,3) (78.5%, 98.3%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:40 PM | Valid: [ 63/180] Step 300/312 Loss 6.080 Prec@(1,3) (78.6%, 98.4%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:41 PM | Valid: [ 63/180] Step 312/312 Loss 6.066 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.862, lat_loss 21.886
09/28 12:22:41 PM | val: [ 63/180] Final Prec@1 78.5500% Time 29.70
09/28 12:22:41 PM | Best top1 acc by now. Save model
09/28 12:22:41 PM | Start to train weights for epoch 63
09/28 12:23:07 PM | Train: [ 64/180] Step 050/1249 Loss 5.022 Prec@(1,3) (81.3%, 99.1%), ce_loss 0.862, lat_loss 21.886
09/28 12:23:30 PM | Train: [ 64/180] Step 100/1249 Loss 5.042 Prec@(1,3) (81.1%, 98.9%), ce_loss 0.862, lat_loss 21.886
09/28 12:23:54 PM | Train: [ 64/180] Step 150/1249 Loss 5.058 Prec@(1,3) (80.9%, 98.8%), ce_loss 0.861, lat_loss 21.886
09/28 12:24:18 PM | Train: [ 64/180] Step 200/1249 Loss 4.993 Prec@(1,3) (81.2%, 98.8%), ce_loss 0.861, lat_loss 21.885
09/28 12:24:39 PM | Train: [ 64/180] Step 250/1249 Loss 5.189 Prec@(1,3) (80.8%, 98.6%), ce_loss 0.861, lat_loss 21.885
09/28 12:25:02 PM | Train: [ 64/180] Step 300/1249 Loss 5.174 Prec@(1,3) (80.8%, 98.6%), ce_loss 0.861, lat_loss 21.885
09/28 12:25:25 PM | Train: [ 64/180] Step 350/1249 Loss 5.156 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.861, lat_loss 21.885
09/28 12:25:49 PM | Train: [ 64/180] Step 400/1249 Loss 5.181 Prec@(1,3) (80.6%, 98.7%), ce_loss 0.861, lat_loss 21.885
09/28 12:26:13 PM | Train: [ 64/180] Step 450/1249 Loss 5.265 Prec@(1,3) (80.3%, 98.6%), ce_loss 0.861, lat_loss 21.885
09/28 12:26:37 PM | Train: [ 64/180] Step 500/1249 Loss 5.253 Prec@(1,3) (80.3%, 98.5%), ce_loss 0.861, lat_loss 21.885
09/28 12:27:00 PM | Train: [ 64/180] Step 550/1249 Loss 5.230 Prec@(1,3) (80.5%, 98.6%), ce_loss 0.861, lat_loss 21.885
09/28 12:27:24 PM | Train: [ 64/180] Step 600/1249 Loss 5.214 Prec@(1,3) (80.5%, 98.6%), ce_loss 0.860, lat_loss 21.885
09/28 12:27:48 PM | Train: [ 64/180] Step 650/1249 Loss 5.192 Prec@(1,3) (80.6%, 98.6%), ce_loss 0.860, lat_loss 21.885
09/28 12:28:11 PM | Train: [ 64/180] Step 700/1249 Loss 5.215 Prec@(1,3) (80.5%, 98.6%), ce_loss 0.860, lat_loss 21.885
09/28 12:28:35 PM | Train: [ 64/180] Step 750/1249 Loss 5.256 Prec@(1,3) (80.3%, 98.6%), ce_loss 0.860, lat_loss 21.885
09/28 12:28:58 PM | Train: [ 64/180] Step 800/1249 Loss 5.273 Prec@(1,3) (80.3%, 98.6%), ce_loss 0.860, lat_loss 21.885
09/28 12:29:22 PM | Train: [ 64/180] Step 850/1249 Loss 5.291 Prec@(1,3) (80.2%, 98.6%), ce_loss 0.860, lat_loss 21.885
09/28 12:29:44 PM | Train: [ 64/180] Step 900/1249 Loss 5.294 Prec@(1,3) (80.2%, 98.6%), ce_loss 0.860, lat_loss 21.885
09/28 12:30:06 PM | Train: [ 64/180] Step 950/1249 Loss 5.292 Prec@(1,3) (80.2%, 98.6%), ce_loss 0.860, lat_loss 21.884
09/28 12:30:30 PM | Train: [ 64/180] Step 1000/1249 Loss 5.303 Prec@(1,3) (80.2%, 98.6%), ce_loss 0.860, lat_loss 21.884
09/28 12:30:54 PM | Train: [ 64/180] Step 1050/1249 Loss 5.333 Prec@(1,3) (80.1%, 98.6%), ce_loss 0.859, lat_loss 21.884
09/28 12:31:18 PM | Train: [ 64/180] Step 1100/1249 Loss 5.320 Prec@(1,3) (80.2%, 98.6%), ce_loss 0.859, lat_loss 21.884
09/28 12:31:42 PM | Train: [ 64/180] Step 1150/1249 Loss 5.304 Prec@(1,3) (80.2%, 98.5%), ce_loss 0.859, lat_loss 21.884
09/28 12:32:05 PM | Train: [ 64/180] Step 1200/1249 Loss 5.296 Prec@(1,3) (80.3%, 98.6%), ce_loss 0.859, lat_loss 21.884
09/28 12:32:29 PM | Train: [ 64/180] Step 1249/1249 Loss 5.335 Prec@(1,3) (80.2%, 98.5%), ce_loss 0.859, lat_loss 21.884
09/28 12:32:30 PM | _w_step_train: [ 64/180] Final Prec@1 80.1775% Time 588.37
09/28 12:32:30 PM | Start to train theta for epoch 63
09/28 12:32:51 PM | Train: [ 64/180] Step 050/312 Loss 5.305 Prec@(1,3) (80.3%, 97.5%), ce_loss 0.859, lat_loss 21.884
09/28 12:33:11 PM | Train: [ 64/180] Step 100/312 Loss 5.511 Prec@(1,3) (79.7%, 98.0%), ce_loss 0.859, lat_loss 21.884
09/28 12:33:31 PM | Train: [ 64/180] Step 150/312 Loss 5.492 Prec@(1,3) (79.8%, 98.2%), ce_loss 0.859, lat_loss 21.884
09/28 12:33:51 PM | Train: [ 64/180] Step 200/312 Loss 5.562 Prec@(1,3) (79.5%, 98.2%), ce_loss 0.859, lat_loss 21.884
09/28 12:34:11 PM | Train: [ 64/180] Step 250/312 Loss 5.656 Prec@(1,3) (79.2%, 98.1%), ce_loss 0.859, lat_loss 21.884
09/28 12:34:32 PM | Train: [ 64/180] Step 300/312 Loss 5.677 Prec@(1,3) (79.4%, 98.0%), ce_loss 0.859, lat_loss 21.884
09/28 12:34:37 PM | Train: [ 64/180] Step 312/312 Loss 5.668 Prec@(1,3) (79.4%, 98.0%), ce_loss 0.858, lat_loss 21.884
09/28 12:34:37 PM | _theta_step_train: [ 64/180] Final Prec@1 79.4000% Time 127.16
09/28 12:34:42 PM | Valid: [ 64/180] Step 050/312 Loss 5.162 Prec@(1,3) (80.1%, 98.2%), ce_loss 0.858, lat_loss 21.884
09/28 12:34:47 PM | Valid: [ 64/180] Step 100/312 Loss 5.318 Prec@(1,3) (79.8%, 98.1%), ce_loss 0.858, lat_loss 21.884
09/28 12:34:52 PM | Valid: [ 64/180] Step 150/312 Loss 5.473 Prec@(1,3) (79.2%, 98.1%), ce_loss 0.858, lat_loss 21.884
09/28 12:34:56 PM | Valid: [ 64/180] Step 200/312 Loss 5.455 Prec@(1,3) (79.3%, 98.2%), ce_loss 0.858, lat_loss 21.883
09/28 12:35:01 PM | Valid: [ 64/180] Step 250/312 Loss 5.484 Prec@(1,3) (79.3%, 98.1%), ce_loss 0.858, lat_loss 21.883
09/28 12:35:06 PM | Valid: [ 64/180] Step 300/312 Loss 5.472 Prec@(1,3) (79.5%, 98.2%), ce_loss 0.858, lat_loss 21.883
09/28 12:35:07 PM | Valid: [ 64/180] Step 312/312 Loss 5.470 Prec@(1,3) (79.5%, 98.2%), ce_loss 0.858, lat_loss 21.883
09/28 12:35:07 PM | val: [ 64/180] Final Prec@1 79.5500% Time 29.98
09/28 12:35:07 PM | Best top1 acc by now. Save model
09/28 12:35:07 PM | Start to train weights for epoch 64
09/28 12:35:33 PM | Train: [ 65/180] Step 050/1249 Loss 5.063 Prec@(1,3) (80.3%, 98.6%), ce_loss 0.858, lat_loss 21.883
09/28 12:35:56 PM | Train: [ 65/180] Step 100/1249 Loss 5.196 Prec@(1,3) (80.4%, 98.4%), ce_loss 0.858, lat_loss 21.883
09/28 12:36:20 PM | Train: [ 65/180] Step 150/1249 Loss 5.158 Prec@(1,3) (80.4%, 98.4%), ce_loss 0.858, lat_loss 21.883
09/28 12:36:44 PM | Train: [ 65/180] Step 200/1249 Loss 5.135 Prec@(1,3) (80.7%, 98.5%), ce_loss 0.857, lat_loss 21.883
09/28 12:37:08 PM | Train: [ 65/180] Step 250/1249 Loss 5.106 Prec@(1,3) (80.8%, 98.6%), ce_loss 0.857, lat_loss 21.883
09/28 12:37:32 PM | Train: [ 65/180] Step 300/1249 Loss 5.131 Prec@(1,3) (80.8%, 98.6%), ce_loss 0.857, lat_loss 21.883
09/28 12:37:57 PM | Train: [ 65/180] Step 350/1249 Loss 5.169 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.857, lat_loss 21.883
09/28 12:38:20 PM | Train: [ 65/180] Step 400/1249 Loss 5.163 Prec@(1,3) (80.8%, 98.6%), ce_loss 0.857, lat_loss 21.883
09/28 12:38:43 PM | Train: [ 65/180] Step 450/1249 Loss 5.139 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.857, lat_loss 21.883
09/28 12:39:07 PM | Train: [ 65/180] Step 500/1249 Loss 5.141 Prec@(1,3) (80.8%, 98.6%), ce_loss 0.857, lat_loss 21.883
09/28 12:39:30 PM | Train: [ 65/180] Step 550/1249 Loss 5.145 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.857, lat_loss 21.883
09/28 12:39:54 PM | Train: [ 65/180] Step 600/1249 Loss 5.143 Prec@(1,3) (80.8%, 98.6%), ce_loss 0.856, lat_loss 21.883
09/28 12:40:17 PM | Train: [ 65/180] Step 650/1249 Loss 5.126 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.856, lat_loss 21.883
09/28 12:40:41 PM | Train: [ 65/180] Step 700/1249 Loss 5.099 Prec@(1,3) (81.0%, 98.7%), ce_loss 0.856, lat_loss 21.882
09/28 12:41:04 PM | Train: [ 65/180] Step 750/1249 Loss 5.094 Prec@(1,3) (81.0%, 98.7%), ce_loss 0.856, lat_loss 21.882
09/28 12:41:27 PM | Train: [ 65/180] Step 800/1249 Loss 5.123 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.856, lat_loss 21.882
09/28 12:41:49 PM | Train: [ 65/180] Step 850/1249 Loss 5.135 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.856, lat_loss 21.882
09/28 12:42:11 PM | Train: [ 65/180] Step 900/1249 Loss 5.135 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.856, lat_loss 21.882
09/28 12:42:33 PM | Train: [ 65/180] Step 950/1249 Loss 5.120 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.856, lat_loss 21.882
09/28 12:42:57 PM | Train: [ 65/180] Step 1000/1249 Loss 5.113 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.856, lat_loss 21.882
09/28 12:43:18 PM | Train: [ 65/180] Step 1050/1249 Loss 5.109 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.855, lat_loss 21.882
09/28 12:43:40 PM | Train: [ 65/180] Step 1100/1249 Loss 5.141 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.855, lat_loss 21.882
09/28 12:44:03 PM | Train: [ 65/180] Step 1150/1249 Loss 5.146 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.855, lat_loss 21.882
09/28 12:44:27 PM | Train: [ 65/180] Step 1200/1249 Loss 5.162 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.855, lat_loss 21.882
09/28 12:44:51 PM | Train: [ 65/180] Step 1249/1249 Loss 5.151 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.855, lat_loss 21.882
09/28 12:44:51 PM | _w_step_train: [ 65/180] Final Prec@1 80.7875% Time 584.47
09/28 12:44:51 PM | Start to train theta for epoch 64
09/28 12:45:13 PM | Train: [ 65/180] Step 050/312 Loss 5.718 Prec@(1,3) (78.6%, 98.3%), ce_loss 0.855, lat_loss 21.882
09/28 12:45:33 PM | Train: [ 65/180] Step 100/312 Loss 5.696 Prec@(1,3) (78.2%, 98.4%), ce_loss 0.855, lat_loss 21.882
09/28 12:45:54 PM | Train: [ 65/180] Step 150/312 Loss 5.704 Prec@(1,3) (78.3%, 98.3%), ce_loss 0.855, lat_loss 21.882
09/28 12:46:14 PM | Train: [ 65/180] Step 200/312 Loss 5.668 Prec@(1,3) (78.5%, 98.2%), ce_loss 0.855, lat_loss 21.882
09/28 12:46:34 PM | Train: [ 65/180] Step 250/312 Loss 5.678 Prec@(1,3) (78.2%, 98.3%), ce_loss 0.855, lat_loss 21.882
09/28 12:46:54 PM | Train: [ 65/180] Step 300/312 Loss 5.608 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.854, lat_loss 21.881
09/28 12:46:59 PM | Train: [ 65/180] Step 312/312 Loss 5.626 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.854, lat_loss 21.881
09/28 12:47:00 PM | _theta_step_train: [ 65/180] Final Prec@1 78.4700% Time 128.09
09/28 12:47:05 PM | Valid: [ 65/180] Step 050/312 Loss 5.370 Prec@(1,3) (80.2%, 98.8%), ce_loss 0.854, lat_loss 21.881
09/28 12:47:10 PM | Valid: [ 65/180] Step 100/312 Loss 5.426 Prec@(1,3) (79.9%, 98.7%), ce_loss 0.854, lat_loss 21.881
09/28 12:47:14 PM | Valid: [ 65/180] Step 150/312 Loss 5.479 Prec@(1,3) (79.5%, 98.6%), ce_loss 0.854, lat_loss 21.881
09/28 12:47:19 PM | Valid: [ 65/180] Step 200/312 Loss 5.375 Prec@(1,3) (79.9%, 98.6%), ce_loss 0.854, lat_loss 21.881
09/28 12:47:24 PM | Valid: [ 65/180] Step 250/312 Loss 5.412 Prec@(1,3) (79.7%, 98.6%), ce_loss 0.854, lat_loss 21.881
09/28 12:47:28 PM | Valid: [ 65/180] Step 300/312 Loss 5.412 Prec@(1,3) (79.7%, 98.6%), ce_loss 0.854, lat_loss 21.881
09/28 12:47:29 PM | Valid: [ 65/180] Step 312/312 Loss 5.387 Prec@(1,3) (79.8%, 98.6%), ce_loss 0.854, lat_loss 21.881
09/28 12:47:29 PM | val: [ 65/180] Final Prec@1 79.7900% Time 29.96
09/28 12:47:29 PM | Best top1 acc by now. Save model
09/28 12:47:30 PM | Start to train weights for epoch 65
09/28 12:47:57 PM | Train: [ 66/180] Step 050/1249 Loss 4.833 Prec@(1,3) (81.8%, 98.8%), ce_loss 0.854, lat_loss 21.881
09/28 12:48:21 PM | Train: [ 66/180] Step 100/1249 Loss 4.975 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.854, lat_loss 21.881
09/28 12:48:46 PM | Train: [ 66/180] Step 150/1249 Loss 4.979 Prec@(1,3) (80.8%, 98.8%), ce_loss 0.853, lat_loss 21.881
09/28 12:49:10 PM | Train: [ 66/180] Step 200/1249 Loss 4.949 Prec@(1,3) (81.2%, 98.8%), ce_loss 0.853, lat_loss 21.881
09/28 12:49:34 PM | Train: [ 66/180] Step 250/1249 Loss 5.025 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.853, lat_loss 21.881
09/28 12:49:59 PM | Train: [ 66/180] Step 300/1249 Loss 5.036 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.853, lat_loss 21.881
09/28 12:50:23 PM | Train: [ 66/180] Step 350/1249 Loss 5.117 Prec@(1,3) (80.6%, 98.7%), ce_loss 0.853, lat_loss 21.881
09/28 12:50:47 PM | Train: [ 66/180] Step 400/1249 Loss 5.123 Prec@(1,3) (80.6%, 98.7%), ce_loss 0.853, lat_loss 21.881
09/28 12:51:11 PM | Train: [ 66/180] Step 450/1249 Loss 5.109 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.853, lat_loss 21.880
09/28 12:51:36 PM | Train: [ 66/180] Step 500/1249 Loss 5.119 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.853, lat_loss 21.880
09/28 12:51:59 PM | Train: [ 66/180] Step 550/1249 Loss 5.116 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.853, lat_loss 21.880
09/28 12:52:23 PM | Train: [ 66/180] Step 600/1249 Loss 5.106 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:52:48 PM | Train: [ 66/180] Step 650/1249 Loss 5.109 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:53:11 PM | Train: [ 66/180] Step 700/1249 Loss 5.148 Prec@(1,3) (80.6%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:53:35 PM | Train: [ 66/180] Step 750/1249 Loss 5.146 Prec@(1,3) (80.6%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:53:57 PM | Train: [ 66/180] Step 800/1249 Loss 5.128 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:54:19 PM | Train: [ 66/180] Step 850/1249 Loss 5.131 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:54:41 PM | Train: [ 66/180] Step 900/1249 Loss 5.130 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:55:06 PM | Train: [ 66/180] Step 950/1249 Loss 5.143 Prec@(1,3) (80.6%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:55:30 PM | Train: [ 66/180] Step 1000/1249 Loss 5.123 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.852, lat_loss 21.880
09/28 12:55:55 PM | Train: [ 66/180] Step 1050/1249 Loss 5.122 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.851, lat_loss 21.880
09/28 12:56:20 PM | Train: [ 66/180] Step 1100/1249 Loss 5.125 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.851, lat_loss 21.880
09/28 12:56:44 PM | Train: [ 66/180] Step 1150/1249 Loss 5.104 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.851, lat_loss 21.880
09/28 12:57:09 PM | Train: [ 66/180] Step 1200/1249 Loss 5.107 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.851, lat_loss 21.880
09/28 12:57:33 PM | Train: [ 66/180] Step 1249/1249 Loss 5.099 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.851, lat_loss 21.879
09/28 12:57:33 PM | _w_step_train: [ 66/180] Final Prec@1 80.7825% Time 603.18
09/28 12:57:33 PM | Start to train theta for epoch 65
09/28 12:57:55 PM | Train: [ 66/180] Step 050/312 Loss 5.850 Prec@(1,3) (77.7%, 98.0%), ce_loss 0.851, lat_loss 21.879
09/28 12:58:16 PM | Train: [ 66/180] Step 100/312 Loss 5.738 Prec@(1,3) (78.2%, 98.3%), ce_loss 0.851, lat_loss 21.879
09/28 12:58:37 PM | Train: [ 66/180] Step 150/312 Loss 5.717 Prec@(1,3) (78.3%, 98.3%), ce_loss 0.851, lat_loss 21.879
09/28 12:58:57 PM | Train: [ 66/180] Step 200/312 Loss 5.656 Prec@(1,3) (78.5%, 98.4%), ce_loss 0.851, lat_loss 21.879
09/28 12:59:17 PM | Train: [ 66/180] Step 250/312 Loss 5.594 Prec@(1,3) (78.9%, 98.4%), ce_loss 0.850, lat_loss 21.879
09/28 12:59:38 PM | Train: [ 66/180] Step 300/312 Loss 5.550 Prec@(1,3) (78.9%, 98.5%), ce_loss 0.850, lat_loss 21.879
09/28 12:59:44 PM | Train: [ 66/180] Step 312/312 Loss 5.541 Prec@(1,3) (79.0%, 98.5%), ce_loss 0.850, lat_loss 21.879
09/28 12:59:44 PM | _theta_step_train: [ 66/180] Final Prec@1 78.9600% Time 130.46
09/28 12:59:49 PM | Valid: [ 66/180] Step 050/312 Loss 4.992 Prec@(1,3) (80.3%, 99.3%), ce_loss 0.850, lat_loss 21.879
09/28 12:59:53 PM | Valid: [ 66/180] Step 100/312 Loss 5.409 Prec@(1,3) (79.5%, 98.7%), ce_loss 0.850, lat_loss 21.879
09/28 12:59:58 PM | Valid: [ 66/180] Step 150/312 Loss 5.471 Prec@(1,3) (79.4%, 98.6%), ce_loss 0.850, lat_loss 21.879
09/28 01:00:03 PM | Valid: [ 66/180] Step 200/312 Loss 5.724 Prec@(1,3) (78.5%, 98.3%), ce_loss 0.850, lat_loss 21.879
09/28 01:00:07 PM | Valid: [ 66/180] Step 250/312 Loss 5.694 Prec@(1,3) (78.5%, 98.2%), ce_loss 0.850, lat_loss 21.879
09/28 01:00:12 PM | Valid: [ 66/180] Step 300/312 Loss 5.620 Prec@(1,3) (78.7%, 98.3%), ce_loss 0.850, lat_loss 21.879
09/28 01:00:13 PM | Valid: [ 66/180] Step 312/312 Loss 5.604 Prec@(1,3) (78.8%, 98.4%), ce_loss 0.850, lat_loss 21.879
09/28 01:00:13 PM | val: [ 66/180] Final Prec@1 78.8000% Time 29.23
09/28 01:00:13 PM | Start to train weights for epoch 66
09/28 01:00:39 PM | Train: [ 67/180] Step 050/1249 Loss 4.495 Prec@(1,3) (81.9%, 99.0%), ce_loss 0.850, lat_loss 21.879
09/28 01:01:04 PM | Train: [ 67/180] Step 100/1249 Loss 4.639 Prec@(1,3) (82.0%, 98.6%), ce_loss 0.850, lat_loss 21.879
09/28 01:01:27 PM | Train: [ 67/180] Step 150/1249 Loss 4.782 Prec@(1,3) (81.6%, 98.7%), ce_loss 0.849, lat_loss 21.879
09/28 01:01:51 PM | Train: [ 67/180] Step 200/1249 Loss 4.711 Prec@(1,3) (81.9%, 98.8%), ce_loss 0.849, lat_loss 21.879
09/28 01:02:15 PM | Train: [ 67/180] Step 250/1249 Loss 4.805 Prec@(1,3) (81.7%, 98.7%), ce_loss 0.849, lat_loss 21.878
09/28 01:02:38 PM | Train: [ 67/180] Step 300/1249 Loss 4.830 Prec@(1,3) (81.5%, 98.6%), ce_loss 0.849, lat_loss 21.878
09/28 01:03:02 PM | Train: [ 67/180] Step 350/1249 Loss 4.859 Prec@(1,3) (81.5%, 98.6%), ce_loss 0.849, lat_loss 21.878
09/28 01:03:26 PM | Train: [ 67/180] Step 400/1249 Loss 4.857 Prec@(1,3) (81.6%, 98.6%), ce_loss 0.849, lat_loss 21.878
09/28 01:03:50 PM | Train: [ 67/180] Step 450/1249 Loss 4.887 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.849, lat_loss 21.878
09/28 01:04:13 PM | Train: [ 67/180] Step 500/1249 Loss 4.885 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.849, lat_loss 21.878
09/28 01:04:38 PM | Train: [ 67/180] Step 550/1249 Loss 4.924 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:05:02 PM | Train: [ 67/180] Step 600/1249 Loss 4.902 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:05:25 PM | Train: [ 67/180] Step 650/1249 Loss 4.918 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:05:48 PM | Train: [ 67/180] Step 700/1249 Loss 4.906 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:06:12 PM | Train: [ 67/180] Step 750/1249 Loss 4.931 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:06:37 PM | Train: [ 67/180] Step 800/1249 Loss 4.969 Prec@(1,3) (81.1%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:07:01 PM | Train: [ 67/180] Step 850/1249 Loss 4.946 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:07:25 PM | Train: [ 67/180] Step 900/1249 Loss 4.939 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:07:49 PM | Train: [ 67/180] Step 950/1249 Loss 4.949 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.848, lat_loss 21.878
09/28 01:08:14 PM | Train: [ 67/180] Step 1000/1249 Loss 4.957 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.847, lat_loss 21.878
09/28 01:08:38 PM | Train: [ 67/180] Step 1050/1249 Loss 4.985 Prec@(1,3) (81.1%, 98.7%), ce_loss 0.847, lat_loss 21.878
09/28 01:09:03 PM | Train: [ 67/180] Step 1100/1249 Loss 5.001 Prec@(1,3) (81.0%, 98.7%), ce_loss 0.847, lat_loss 21.877
09/28 01:09:27 PM | Train: [ 67/180] Step 1150/1249 Loss 5.026 Prec@(1,3) (81.0%, 98.7%), ce_loss 0.847, lat_loss 21.877
09/28 01:09:52 PM | Train: [ 67/180] Step 1200/1249 Loss 5.034 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.847, lat_loss 21.877
09/28 01:10:17 PM | Train: [ 67/180] Step 1249/1249 Loss 5.044 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.847, lat_loss 21.877
09/28 01:10:17 PM | _w_step_train: [ 67/180] Final Prec@1 80.9100% Time 604.11
09/28 01:10:17 PM | Start to train theta for epoch 66
09/28 01:10:39 PM | Train: [ 67/180] Step 050/312 Loss 5.977 Prec@(1,3) (78.8%, 98.0%), ce_loss 0.847, lat_loss 21.877
09/28 01:11:00 PM | Train: [ 67/180] Step 100/312 Loss 5.552 Prec@(1,3) (80.1%, 98.2%), ce_loss 0.847, lat_loss 21.877
09/28 01:11:20 PM | Train: [ 67/180] Step 150/312 Loss 5.463 Prec@(1,3) (79.8%, 98.4%), ce_loss 0.847, lat_loss 21.877
09/28 01:11:41 PM | Train: [ 67/180] Step 200/312 Loss 5.539 Prec@(1,3) (79.3%, 98.4%), ce_loss 0.847, lat_loss 21.877
09/28 01:12:02 PM | Train: [ 67/180] Step 250/312 Loss 5.460 Prec@(1,3) (79.6%, 98.5%), ce_loss 0.847, lat_loss 21.877
09/28 01:12:23 PM | Train: [ 67/180] Step 300/312 Loss 5.493 Prec@(1,3) (79.4%, 98.5%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:28 PM | Train: [ 67/180] Step 312/312 Loss 5.449 Prec@(1,3) (79.6%, 98.5%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:28 PM | _theta_step_train: [ 67/180] Final Prec@1 79.5700% Time 131.23
09/28 01:12:34 PM | Valid: [ 67/180] Step 050/312 Loss 5.104 Prec@(1,3) (80.6%, 99.1%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:38 PM | Valid: [ 67/180] Step 100/312 Loss 5.235 Prec@(1,3) (80.8%, 98.8%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:43 PM | Valid: [ 67/180] Step 150/312 Loss 5.511 Prec@(1,3) (80.4%, 98.4%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:47 PM | Valid: [ 67/180] Step 200/312 Loss 5.483 Prec@(1,3) (80.2%, 98.4%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:52 PM | Valid: [ 67/180] Step 250/312 Loss 5.329 Prec@(1,3) (80.6%, 98.5%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:57 PM | Valid: [ 67/180] Step 300/312 Loss 5.390 Prec@(1,3) (80.5%, 98.4%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:58 PM | Valid: [ 67/180] Step 312/312 Loss 5.373 Prec@(1,3) (80.5%, 98.4%), ce_loss 0.846, lat_loss 21.877
09/28 01:12:58 PM | val: [ 67/180] Final Prec@1 80.4900% Time 30.18
09/28 01:12:58 PM | Best top1 acc by now. Save model
09/28 01:12:59 PM | Start to train weights for epoch 67
09/28 01:13:21 PM | Train: [ 68/180] Step 050/1249 Loss 4.821 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.846, lat_loss 21.877
09/28 01:13:41 PM | Train: [ 68/180] Step 100/1249 Loss 4.871 Prec@(1,3) (81.7%, 98.6%), ce_loss 0.846, lat_loss 21.876
09/28 01:14:01 PM | Train: [ 68/180] Step 150/1249 Loss 4.720 Prec@(1,3) (82.1%, 98.7%), ce_loss 0.845, lat_loss 21.876
09/28 01:14:25 PM | Train: [ 68/180] Step 200/1249 Loss 4.803 Prec@(1,3) (81.8%, 98.7%), ce_loss 0.845, lat_loss 21.876
09/28 01:14:49 PM | Train: [ 68/180] Step 250/1249 Loss 4.811 Prec@(1,3) (81.9%, 98.8%), ce_loss 0.845, lat_loss 21.876
09/28 01:15:10 PM | Train: [ 68/180] Step 300/1249 Loss 4.855 Prec@(1,3) (81.6%, 98.8%), ce_loss 0.845, lat_loss 21.876
09/28 01:15:35 PM | Train: [ 68/180] Step 350/1249 Loss 4.855 Prec@(1,3) (81.7%, 98.8%), ce_loss 0.845, lat_loss 21.876
09/28 01:16:00 PM | Train: [ 68/180] Step 400/1249 Loss 4.854 Prec@(1,3) (81.8%, 98.8%), ce_loss 0.845, lat_loss 21.876
09/28 01:16:25 PM | Train: [ 68/180] Step 450/1249 Loss 4.880 Prec@(1,3) (81.6%, 98.8%), ce_loss 0.845, lat_loss 21.876
09/28 01:16:50 PM | Train: [ 68/180] Step 500/1249 Loss 4.833 Prec@(1,3) (81.7%, 98.8%), ce_loss 0.845, lat_loss 21.876
09/28 01:17:15 PM | Train: [ 68/180] Step 550/1249 Loss 4.857 Prec@(1,3) (81.6%, 98.8%), ce_loss 0.844, lat_loss 21.876
09/28 01:17:35 PM | Train: [ 68/180] Step 600/1249 Loss 4.882 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.844, lat_loss 21.876
09/28 01:17:51 PM | Train: [ 68/180] Step 650/1249 Loss 4.883 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.844, lat_loss 21.876
09/28 01:18:07 PM | Train: [ 68/180] Step 700/1249 Loss 4.934 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.844, lat_loss 21.876
09/28 01:18:22 PM | Train: [ 68/180] Step 750/1249 Loss 4.931 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.844, lat_loss 21.876
09/28 01:18:38 PM | Train: [ 68/180] Step 800/1249 Loss 4.899 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.844, lat_loss 21.876
09/28 01:18:54 PM | Train: [ 68/180] Step 850/1249 Loss 4.914 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.844, lat_loss 21.876
09/28 01:19:19 PM | Train: [ 68/180] Step 900/1249 Loss 4.921 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.844, lat_loss 21.876
09/28 01:19:42 PM | Train: [ 68/180] Step 950/1249 Loss 4.903 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.844, lat_loss 21.875
09/28 01:20:05 PM | Train: [ 68/180] Step 1000/1249 Loss 4.899 Prec@(1,3) (81.5%, 98.7%), ce_loss 0.843, lat_loss 21.875
09/28 01:20:29 PM | Train: [ 68/180] Step 1050/1249 Loss 4.904 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.843, lat_loss 21.875
09/28 01:20:54 PM | Train: [ 68/180] Step 1100/1249 Loss 4.912 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.843, lat_loss 21.875
09/28 01:21:19 PM | Train: [ 68/180] Step 1150/1249 Loss 4.918 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.843, lat_loss 21.875
09/28 01:21:42 PM | Train: [ 68/180] Step 1200/1249 Loss 4.940 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.843, lat_loss 21.875
09/28 01:22:06 PM | Train: [ 68/180] Step 1249/1249 Loss 4.941 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.843, lat_loss 21.875
09/28 01:22:06 PM | _w_step_train: [ 68/180] Final Prec@1 81.3200% Time 547.76
09/28 01:22:06 PM | Start to train theta for epoch 67
09/28 01:22:27 PM | Train: [ 68/180] Step 050/312 Loss 6.260 Prec@(1,3) (77.8%, 98.0%), ce_loss 0.843, lat_loss 21.875
09/28 01:22:47 PM | Train: [ 68/180] Step 100/312 Loss 5.634 Prec@(1,3) (79.5%, 98.2%), ce_loss 0.843, lat_loss 21.875
09/28 01:23:08 PM | Train: [ 68/180] Step 150/312 Loss 5.574 Prec@(1,3) (79.4%, 98.4%), ce_loss 0.843, lat_loss 21.875
09/28 01:23:29 PM | Train: [ 68/180] Step 200/312 Loss 5.558 Prec@(1,3) (79.5%, 98.4%), ce_loss 0.843, lat_loss 21.875
09/28 01:23:50 PM | Train: [ 68/180] Step 250/312 Loss 5.587 Prec@(1,3) (79.3%, 98.4%), ce_loss 0.842, lat_loss 21.875
09/28 01:24:10 PM | Train: [ 68/180] Step 300/312 Loss 5.463 Prec@(1,3) (79.5%, 98.5%), ce_loss 0.842, lat_loss 21.875
09/28 01:24:15 PM | Train: [ 68/180] Step 312/312 Loss 5.426 Prec@(1,3) (79.6%, 98.5%), ce_loss 0.842, lat_loss 21.875
09/28 01:24:15 PM | _theta_step_train: [ 68/180] Final Prec@1 79.6000% Time 128.29
09/28 01:24:20 PM | Valid: [ 68/180] Step 050/312 Loss 4.550 Prec@(1,3) (82.7%, 99.2%), ce_loss 0.842, lat_loss 21.875
09/28 01:24:25 PM | Valid: [ 68/180] Step 100/312 Loss 5.090 Prec@(1,3) (80.4%, 98.6%), ce_loss 0.842, lat_loss 21.875
09/28 01:24:29 PM | Valid: [ 68/180] Step 150/312 Loss 5.189 Prec@(1,3) (80.5%, 98.4%), ce_loss 0.842, lat_loss 21.875
09/28 01:24:34 PM | Valid: [ 68/180] Step 200/312 Loss 5.118 Prec@(1,3) (80.7%, 98.6%), ce_loss 0.842, lat_loss 21.874
09/28 01:24:38 PM | Valid: [ 68/180] Step 250/312 Loss 5.133 Prec@(1,3) (80.5%, 98.7%), ce_loss 0.842, lat_loss 21.874
09/28 01:24:43 PM | Valid: [ 68/180] Step 300/312 Loss 5.111 Prec@(1,3) (80.6%, 98.7%), ce_loss 0.842, lat_loss 21.874
09/28 01:24:44 PM | Valid: [ 68/180] Step 312/312 Loss 5.177 Prec@(1,3) (80.3%, 98.6%), ce_loss 0.842, lat_loss 21.874
09/28 01:24:44 PM | val: [ 68/180] Final Prec@1 80.3400% Time 29.36
09/28 01:24:44 PM | Start to train weights for epoch 68
09/28 01:25:09 PM | Train: [ 69/180] Step 050/1249 Loss 5.087 Prec@(1,3) (80.8%, 98.7%), ce_loss 0.842, lat_loss 21.874
09/28 01:25:34 PM | Train: [ 69/180] Step 100/1249 Loss 5.202 Prec@(1,3) (79.6%, 98.6%), ce_loss 0.842, lat_loss 21.874
09/28 01:25:58 PM | Train: [ 69/180] Step 150/1249 Loss 4.886 Prec@(1,3) (80.5%, 98.9%), ce_loss 0.841, lat_loss 21.874
09/28 01:26:22 PM | Train: [ 69/180] Step 200/1249 Loss 4.902 Prec@(1,3) (80.4%, 98.9%), ce_loss 0.841, lat_loss 21.874
09/28 01:26:47 PM | Train: [ 69/180] Step 250/1249 Loss 4.843 Prec@(1,3) (80.8%, 98.9%), ce_loss 0.841, lat_loss 21.874
09/28 01:27:10 PM | Train: [ 69/180] Step 300/1249 Loss 4.923 Prec@(1,3) (80.8%, 98.8%), ce_loss 0.841, lat_loss 21.874
09/28 01:27:33 PM | Train: [ 69/180] Step 350/1249 Loss 4.940 Prec@(1,3) (80.8%, 98.8%), ce_loss 0.841, lat_loss 21.874
09/28 01:27:56 PM | Train: [ 69/180] Step 400/1249 Loss 4.886 Prec@(1,3) (81.1%, 98.8%), ce_loss 0.841, lat_loss 21.874
09/28 01:28:18 PM | Train: [ 69/180] Step 450/1249 Loss 4.844 Prec@(1,3) (81.3%, 98.8%), ce_loss 0.841, lat_loss 21.874
09/28 01:28:40 PM | Train: [ 69/180] Step 500/1249 Loss 4.863 Prec@(1,3) (81.3%, 98.8%), ce_loss 0.841, lat_loss 21.874
09/28 01:29:04 PM | Train: [ 69/180] Step 550/1249 Loss 4.872 Prec@(1,3) (81.3%, 98.8%), ce_loss 0.840, lat_loss 21.874
09/28 01:29:28 PM | Train: [ 69/180] Step 600/1249 Loss 4.855 Prec@(1,3) (81.4%, 98.9%), ce_loss 0.840, lat_loss 21.874
09/28 01:29:50 PM | Train: [ 69/180] Step 650/1249 Loss 4.827 Prec@(1,3) (81.5%, 98.9%), ce_loss 0.840, lat_loss 21.873
09/28 01:30:10 PM | Train: [ 69/180] Step 700/1249 Loss 4.842 Prec@(1,3) (81.5%, 98.9%), ce_loss 0.840, lat_loss 21.873
09/28 01:30:31 PM | Train: [ 69/180] Step 750/1249 Loss 4.812 Prec@(1,3) (81.5%, 98.9%), ce_loss 0.840, lat_loss 21.873
09/28 01:30:53 PM | Train: [ 69/180] Step 800/1249 Loss 4.799 Prec@(1,3) (81.6%, 98.9%), ce_loss 0.840, lat_loss 21.873
09/28 01:31:17 PM | Train: [ 69/180] Step 850/1249 Loss 4.812 Prec@(1,3) (81.6%, 98.9%), ce_loss 0.840, lat_loss 21.873
09/28 01:31:42 PM | Train: [ 69/180] Step 900/1249 Loss 4.841 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.840, lat_loss 21.873
09/28 01:32:07 PM | Train: [ 69/180] Step 950/1249 Loss 4.841 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.840, lat_loss 21.873
09/28 01:32:31 PM | Train: [ 69/180] Step 1000/1249 Loss 4.848 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.839, lat_loss 21.873
09/28 01:32:57 PM | Train: [ 69/180] Step 1050/1249 Loss 4.840 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.839, lat_loss 21.873
09/28 01:33:21 PM | Train: [ 69/180] Step 1100/1249 Loss 4.858 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.839, lat_loss 21.873
09/28 01:33:47 PM | Train: [ 69/180] Step 1150/1249 Loss 4.857 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.839, lat_loss 21.873
09/28 01:34:06 PM | Train: [ 69/180] Step 1200/1249 Loss 4.862 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.839, lat_loss 21.873
09/28 01:34:22 PM | Train: [ 69/180] Step 1249/1249 Loss 4.873 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.839, lat_loss 21.873
09/28 01:34:22 PM | _w_step_train: [ 69/180] Final Prec@1 81.5325% Time 577.73
09/28 01:34:22 PM | Start to train theta for epoch 68
09/28 01:34:35 PM | Train: [ 69/180] Step 050/312 Loss 5.555 Prec@(1,3) (79.2%, 97.9%), ce_loss 0.839, lat_loss 21.873
09/28 01:34:47 PM | Train: [ 69/180] Step 100/312 Loss 5.395 Prec@(1,3) (79.4%, 98.4%), ce_loss 0.839, lat_loss 21.873
09/28 01:35:05 PM | Train: [ 69/180] Step 150/312 Loss 5.520 Prec@(1,3) (79.2%, 98.3%), ce_loss 0.839, lat_loss 21.872
09/28 01:35:24 PM | Train: [ 69/180] Step 200/312 Loss 5.426 Prec@(1,3) (79.7%, 98.3%), ce_loss 0.838, lat_loss 21.872
09/28 01:35:42 PM | Train: [ 69/180] Step 250/312 Loss 5.356 Prec@(1,3) (79.8%, 98.4%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:01 PM | Train: [ 69/180] Step 300/312 Loss 5.356 Prec@(1,3) (79.9%, 98.3%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:06 PM | Train: [ 69/180] Step 312/312 Loss 5.373 Prec@(1,3) (79.8%, 98.3%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:06 PM | _theta_step_train: [ 69/180] Final Prec@1 79.8000% Time 104.37
09/28 01:36:12 PM | Valid: [ 69/180] Step 050/312 Loss 4.964 Prec@(1,3) (81.9%, 99.0%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:16 PM | Valid: [ 69/180] Step 100/312 Loss 5.050 Prec@(1,3) (81.6%, 98.9%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:21 PM | Valid: [ 69/180] Step 150/312 Loss 5.497 Prec@(1,3) (80.4%, 98.4%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:25 PM | Valid: [ 69/180] Step 200/312 Loss 5.540 Prec@(1,3) (80.4%, 98.2%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:30 PM | Valid: [ 69/180] Step 250/312 Loss 5.508 Prec@(1,3) (80.4%, 98.3%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:34 PM | Valid: [ 69/180] Step 300/312 Loss 5.470 Prec@(1,3) (80.6%, 98.4%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:35 PM | Valid: [ 69/180] Step 312/312 Loss 5.493 Prec@(1,3) (80.5%, 98.4%), ce_loss 0.838, lat_loss 21.872
09/28 01:36:36 PM | val: [ 69/180] Final Prec@1 80.5100% Time 29.25
09/28 01:36:36 PM | Best top1 acc by now. Save model
09/28 01:36:36 PM | Start to train weights for epoch 69
09/28 01:37:01 PM | Train: [ 70/180] Step 050/1249 Loss 4.602 Prec@(1,3) (82.1%, 99.0%), ce_loss 0.838, lat_loss 21.872
09/28 01:37:24 PM | Train: [ 70/180] Step 100/1249 Loss 4.768 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.838, lat_loss 21.872
09/28 01:37:47 PM | Train: [ 70/180] Step 150/1249 Loss 4.653 Prec@(1,3) (82.6%, 98.9%), ce_loss 0.837, lat_loss 21.872
09/28 01:38:11 PM | Train: [ 70/180] Step 200/1249 Loss 4.691 Prec@(1,3) (82.4%, 98.9%), ce_loss 0.837, lat_loss 21.872
09/28 01:38:34 PM | Train: [ 70/180] Step 250/1249 Loss 4.729 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.837, lat_loss 21.871
09/28 01:38:57 PM | Train: [ 70/180] Step 300/1249 Loss 4.793 Prec@(1,3) (81.9%, 98.9%), ce_loss 0.837, lat_loss 21.871
09/28 01:39:19 PM | Train: [ 70/180] Step 350/1249 Loss 4.776 Prec@(1,3) (82.0%, 98.8%), ce_loss 0.837, lat_loss 21.871
09/28 01:39:43 PM | Train: [ 70/180] Step 400/1249 Loss 4.728 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.837, lat_loss 21.871
09/28 01:40:06 PM | Train: [ 70/180] Step 450/1249 Loss 4.695 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.837, lat_loss 21.871
09/28 01:40:23 PM | Train: [ 70/180] Step 500/1249 Loss 4.685 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.837, lat_loss 21.871
09/28 01:40:38 PM | Train: [ 70/180] Step 550/1249 Loss 4.709 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:40:54 PM | Train: [ 70/180] Step 600/1249 Loss 4.692 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:41:10 PM | Train: [ 70/180] Step 650/1249 Loss 4.713 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:41:26 PM | Train: [ 70/180] Step 700/1249 Loss 4.708 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:41:42 PM | Train: [ 70/180] Step 750/1249 Loss 4.726 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:41:58 PM | Train: [ 70/180] Step 800/1249 Loss 4.697 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:42:14 PM | Train: [ 70/180] Step 850/1249 Loss 4.727 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:42:30 PM | Train: [ 70/180] Step 900/1249 Loss 4.717 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:42:55 PM | Train: [ 70/180] Step 950/1249 Loss 4.720 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.836, lat_loss 21.871
09/28 01:43:19 PM | Train: [ 70/180] Step 1000/1249 Loss 4.715 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.835, lat_loss 21.870
09/28 01:43:43 PM | Train: [ 70/180] Step 1050/1249 Loss 4.723 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.835, lat_loss 21.870
09/28 01:44:05 PM | Train: [ 70/180] Step 1100/1249 Loss 4.733 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.835, lat_loss 21.870
09/28 01:44:29 PM | Train: [ 70/180] Step 1150/1249 Loss 4.727 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.835, lat_loss 21.870
09/28 01:44:53 PM | Train: [ 70/180] Step 1200/1249 Loss 4.741 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.835, lat_loss 21.870
09/28 01:45:18 PM | Train: [ 70/180] Step 1249/1249 Loss 4.730 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.835, lat_loss 21.870
09/28 01:45:18 PM | _w_step_train: [ 70/180] Final Prec@1 82.0500% Time 522.08
09/28 01:45:18 PM | Start to train theta for epoch 69
09/28 01:45:40 PM | Train: [ 70/180] Step 050/312 Loss 5.265 Prec@(1,3) (79.5%, 99.1%), ce_loss 0.835, lat_loss 21.870
09/28 01:46:01 PM | Train: [ 70/180] Step 100/312 Loss 5.175 Prec@(1,3) (80.6%, 98.9%), ce_loss 0.835, lat_loss 21.870
09/28 01:46:21 PM | Train: [ 70/180] Step 150/312 Loss 5.170 Prec@(1,3) (80.5%, 98.6%), ce_loss 0.835, lat_loss 21.870
09/28 01:46:42 PM | Train: [ 70/180] Step 200/312 Loss 5.167 Prec@(1,3) (80.6%, 98.6%), ce_loss 0.834, lat_loss 21.870
09/28 01:47:03 PM | Train: [ 70/180] Step 250/312 Loss 5.133 Prec@(1,3) (80.5%, 98.6%), ce_loss 0.834, lat_loss 21.870
09/28 01:47:24 PM | Train: [ 70/180] Step 300/312 Loss 5.161 Prec@(1,3) (80.5%, 98.6%), ce_loss 0.834, lat_loss 21.870
09/28 01:47:29 PM | Train: [ 70/180] Step 312/312 Loss 5.185 Prec@(1,3) (80.4%, 98.6%), ce_loss 0.834, lat_loss 21.870
09/28 01:47:29 PM | _theta_step_train: [ 70/180] Final Prec@1 80.4200% Time 131.30
09/28 01:47:34 PM | Valid: [ 70/180] Step 050/312 Loss 4.701 Prec@(1,3) (81.5%, 99.3%), ce_loss 0.834, lat_loss 21.870
09/28 01:47:39 PM | Valid: [ 70/180] Step 100/312 Loss 5.086 Prec@(1,3) (80.6%, 99.0%), ce_loss 0.834, lat_loss 21.870
09/28 01:47:44 PM | Valid: [ 70/180] Step 150/312 Loss 5.176 Prec@(1,3) (80.6%, 98.9%), ce_loss 0.834, lat_loss 21.870
09/28 01:47:48 PM | Valid: [ 70/180] Step 200/312 Loss 5.184 Prec@(1,3) (80.4%, 98.9%), ce_loss 0.834, lat_loss 21.870
09/28 01:47:53 PM | Valid: [ 70/180] Step 250/312 Loss 5.134 Prec@(1,3) (80.6%, 98.9%), ce_loss 0.834, lat_loss 21.869
09/28 01:47:58 PM | Valid: [ 70/180] Step 300/312 Loss 5.113 Prec@(1,3) (80.8%, 99.0%), ce_loss 0.834, lat_loss 21.869
09/28 01:47:59 PM | Valid: [ 70/180] Step 312/312 Loss 5.098 Prec@(1,3) (80.8%, 99.0%), ce_loss 0.834, lat_loss 21.869
09/28 01:47:59 PM | val: [ 70/180] Final Prec@1 80.8000% Time 29.73
09/28 01:47:59 PM | Best top1 acc by now. Save model
09/28 01:47:59 PM | Start to train weights for epoch 70
09/28 01:48:26 PM | Train: [ 71/180] Step 050/1249 Loss 4.639 Prec@(1,3) (82.2%, 98.7%), ce_loss 0.833, lat_loss 21.869
09/28 01:48:51 PM | Train: [ 71/180] Step 100/1249 Loss 4.723 Prec@(1,3) (82.1%, 98.7%), ce_loss 0.833, lat_loss 21.869
09/28 01:49:13 PM | Train: [ 71/180] Step 150/1249 Loss 4.699 Prec@(1,3) (82.1%, 98.7%), ce_loss 0.833, lat_loss 21.869
09/28 01:49:34 PM | Train: [ 71/180] Step 200/1249 Loss 4.632 Prec@(1,3) (82.2%, 98.8%), ce_loss 0.833, lat_loss 21.869
09/28 01:49:54 PM | Train: [ 71/180] Step 250/1249 Loss 4.709 Prec@(1,3) (82.0%, 98.8%), ce_loss 0.833, lat_loss 21.869
09/28 01:50:14 PM | Train: [ 71/180] Step 300/1249 Loss 4.720 Prec@(1,3) (82.0%, 98.8%), ce_loss 0.833, lat_loss 21.869
09/28 01:50:35 PM | Train: [ 71/180] Step 350/1249 Loss 4.709 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.833, lat_loss 21.869
09/28 01:50:57 PM | Train: [ 71/180] Step 400/1249 Loss 4.731 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.833, lat_loss 21.869
09/28 01:51:22 PM | Train: [ 71/180] Step 450/1249 Loss 4.751 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.833, lat_loss 21.869
09/28 01:51:47 PM | Train: [ 71/180] Step 500/1249 Loss 4.730 Prec@(1,3) (82.0%, 98.7%), ce_loss 0.832, lat_loss 21.869
09/28 01:52:12 PM | Train: [ 71/180] Step 550/1249 Loss 4.738 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.832, lat_loss 21.869
09/28 01:52:37 PM | Train: [ 71/180] Step 600/1249 Loss 4.758 Prec@(1,3) (81.8%, 98.7%), ce_loss 0.832, lat_loss 21.869
09/28 01:53:02 PM | Train: [ 71/180] Step 650/1249 Loss 4.724 Prec@(1,3) (81.9%, 98.8%), ce_loss 0.832, lat_loss 21.869
09/28 01:53:27 PM | Train: [ 71/180] Step 700/1249 Loss 4.753 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.832, lat_loss 21.868
09/28 01:53:51 PM | Train: [ 71/180] Step 750/1249 Loss 4.774 Prec@(1,3) (81.8%, 98.7%), ce_loss 0.832, lat_loss 21.868
09/28 01:54:15 PM | Train: [ 71/180] Step 800/1249 Loss 4.769 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.832, lat_loss 21.868
09/28 01:54:40 PM | Train: [ 71/180] Step 850/1249 Loss 4.758 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.832, lat_loss 21.868
09/28 01:55:05 PM | Train: [ 71/180] Step 900/1249 Loss 4.770 Prec@(1,3) (81.8%, 98.7%), ce_loss 0.832, lat_loss 21.868
09/28 01:55:29 PM | Train: [ 71/180] Step 950/1249 Loss 4.785 Prec@(1,3) (81.8%, 98.8%), ce_loss 0.831, lat_loss 21.868
09/28 01:55:53 PM | Train: [ 71/180] Step 1000/1249 Loss 4.796 Prec@(1,3) (81.8%, 98.8%), ce_loss 0.831, lat_loss 21.868
09/28 01:56:18 PM | Train: [ 71/180] Step 1050/1249 Loss 4.801 Prec@(1,3) (81.7%, 98.8%), ce_loss 0.831, lat_loss 21.868
09/28 01:56:42 PM | Train: [ 71/180] Step 1100/1249 Loss 4.783 Prec@(1,3) (81.8%, 98.8%), ce_loss 0.831, lat_loss 21.868
09/28 01:57:07 PM | Train: [ 71/180] Step 1150/1249 Loss 4.784 Prec@(1,3) (81.8%, 98.8%), ce_loss 0.831, lat_loss 21.868
09/28 01:57:31 PM | Train: [ 71/180] Step 1200/1249 Loss 4.775 Prec@(1,3) (81.9%, 98.8%), ce_loss 0.831, lat_loss 21.868
09/28 01:57:56 PM | Train: [ 71/180] Step 1249/1249 Loss 4.756 Prec@(1,3) (82.0%, 98.8%), ce_loss 0.831, lat_loss 21.868
09/28 01:57:56 PM | _w_step_train: [ 71/180] Final Prec@1 81.9675% Time 596.70
09/28 01:57:56 PM | Start to train theta for epoch 70
09/28 01:58:15 PM | Train: [ 71/180] Step 050/312 Loss 5.557 Prec@(1,3) (79.4%, 98.4%), ce_loss 0.831, lat_loss 21.868
09/28 01:58:33 PM | Train: [ 71/180] Step 100/312 Loss 5.553 Prec@(1,3) (79.9%, 98.3%), ce_loss 0.831, lat_loss 21.868
09/28 01:58:51 PM | Train: [ 71/180] Step 150/312 Loss 5.571 Prec@(1,3) (80.1%, 98.1%), ce_loss 0.831, lat_loss 21.868
09/28 01:59:08 PM | Train: [ 71/180] Step 200/312 Loss 5.533 Prec@(1,3) (80.0%, 98.1%), ce_loss 0.830, lat_loss 21.868
09/28 01:59:25 PM | Train: [ 71/180] Step 250/312 Loss 5.632 Prec@(1,3) (79.8%, 98.1%), ce_loss 0.830, lat_loss 21.867
09/28 01:59:44 PM | Train: [ 71/180] Step 300/312 Loss 5.572 Prec@(1,3) (80.0%, 98.0%), ce_loss 0.830, lat_loss 21.867
09/28 01:59:49 PM | Train: [ 71/180] Step 312/312 Loss 5.565 Prec@(1,3) (79.8%, 98.0%), ce_loss 0.830, lat_loss 21.867
09/28 01:59:49 PM | _theta_step_train: [ 71/180] Final Prec@1 79.8400% Time 112.96
09/28 01:59:54 PM | Valid: [ 71/180] Step 050/312 Loss 5.286 Prec@(1,3) (81.1%, 97.9%), ce_loss 0.830, lat_loss 21.867
09/28 01:59:59 PM | Valid: [ 71/180] Step 100/312 Loss 5.261 Prec@(1,3) (80.6%, 98.1%), ce_loss 0.830, lat_loss 21.867
09/28 02:00:04 PM | Valid: [ 71/180] Step 150/312 Loss 5.200 Prec@(1,3) (80.9%, 98.2%), ce_loss 0.830, lat_loss 21.867
09/28 02:00:08 PM | Valid: [ 71/180] Step 200/312 Loss 5.326 Prec@(1,3) (80.6%, 98.2%), ce_loss 0.830, lat_loss 21.867
09/28 02:00:13 PM | Valid: [ 71/180] Step 250/312 Loss 5.261 Prec@(1,3) (80.8%, 98.3%), ce_loss 0.830, lat_loss 21.867
09/28 02:00:17 PM | Valid: [ 71/180] Step 300/312 Loss 5.219 Prec@(1,3) (80.8%, 98.3%), ce_loss 0.830, lat_loss 21.867
09/28 02:00:18 PM | Valid: [ 71/180] Step 312/312 Loss 5.202 Prec@(1,3) (80.7%, 98.4%), ce_loss 0.830, lat_loss 21.867
09/28 02:00:18 PM | val: [ 71/180] Final Prec@1 80.7400% Time 29.31
09/28 02:00:18 PM | Start to train weights for epoch 71
09/28 02:00:44 PM | Train: [ 72/180] Step 050/1249 Loss 4.576 Prec@(1,3) (82.8%, 99.3%), ce_loss 0.830, lat_loss 21.867
09/28 02:01:08 PM | Train: [ 72/180] Step 100/1249 Loss 4.590 Prec@(1,3) (83.0%, 99.1%), ce_loss 0.830, lat_loss 21.867
09/28 02:01:32 PM | Train: [ 72/180] Step 150/1249 Loss 4.611 Prec@(1,3) (82.7%, 99.1%), ce_loss 0.829, lat_loss 21.867
09/28 02:01:57 PM | Train: [ 72/180] Step 200/1249 Loss 4.609 Prec@(1,3) (82.6%, 99.1%), ce_loss 0.829, lat_loss 21.867
09/28 02:02:21 PM | Train: [ 72/180] Step 250/1249 Loss 4.677 Prec@(1,3) (82.5%, 99.0%), ce_loss 0.829, lat_loss 21.867
09/28 02:02:45 PM | Train: [ 72/180] Step 300/1249 Loss 4.624 Prec@(1,3) (82.6%, 99.0%), ce_loss 0.829, lat_loss 21.867
09/28 02:03:10 PM | Train: [ 72/180] Step 350/1249 Loss 4.685 Prec@(1,3) (82.4%, 98.9%), ce_loss 0.829, lat_loss 21.866
09/28 02:03:35 PM | Train: [ 72/180] Step 400/1249 Loss 4.663 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.829, lat_loss 21.866
09/28 02:04:00 PM | Train: [ 72/180] Step 450/1249 Loss 4.653 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.829, lat_loss 21.866
09/28 02:04:24 PM | Train: [ 72/180] Step 500/1249 Loss 4.713 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.829, lat_loss 21.866
09/28 02:04:49 PM | Train: [ 72/180] Step 550/1249 Loss 4.707 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.828, lat_loss 21.866
09/28 02:05:13 PM | Train: [ 72/180] Step 600/1249 Loss 4.688 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.828, lat_loss 21.866
09/28 02:05:38 PM | Train: [ 72/180] Step 650/1249 Loss 4.713 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.828, lat_loss 21.866
09/28 02:06:02 PM | Train: [ 72/180] Step 700/1249 Loss 4.712 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.828, lat_loss 21.866
09/28 02:06:27 PM | Train: [ 72/180] Step 750/1249 Loss 4.724 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.828, lat_loss 21.866
09/28 02:06:51 PM | Train: [ 72/180] Step 800/1249 Loss 4.713 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.828, lat_loss 21.866
09/28 02:07:15 PM | Train: [ 72/180] Step 850/1249 Loss 4.694 Prec@(1,3) (82.2%, 98.8%), ce_loss 0.828, lat_loss 21.866
09/28 02:07:31 PM | Train: [ 72/180] Step 900/1249 Loss 4.730 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.828, lat_loss 21.866
09/28 02:07:47 PM | Train: [ 72/180] Step 950/1249 Loss 4.702 Prec@(1,3) (82.3%, 98.8%), ce_loss 0.828, lat_loss 21.866
09/28 02:08:02 PM | Train: [ 72/180] Step 1000/1249 Loss 4.675 Prec@(1,3) (82.3%, 98.8%), ce_loss 0.827, lat_loss 21.866
09/28 02:08:18 PM | Train: [ 72/180] Step 1050/1249 Loss 4.698 Prec@(1,3) (82.3%, 98.8%), ce_loss 0.827, lat_loss 21.866
09/28 02:08:34 PM | Train: [ 72/180] Step 1100/1249 Loss 4.690 Prec@(1,3) (82.3%, 98.8%), ce_loss 0.827, lat_loss 21.866
09/28 02:08:50 PM | Train: [ 72/180] Step 1150/1249 Loss 4.687 Prec@(1,3) (82.3%, 98.8%), ce_loss 0.827, lat_loss 21.865
09/28 02:09:06 PM | Train: [ 72/180] Step 1200/1249 Loss 4.710 Prec@(1,3) (82.2%, 98.8%), ce_loss 0.827, lat_loss 21.865
09/28 02:09:21 PM | Train: [ 72/180] Step 1249/1249 Loss 4.704 Prec@(1,3) (82.2%, 98.8%), ce_loss 0.827, lat_loss 21.865
09/28 02:09:22 PM | _w_step_train: [ 72/180] Final Prec@1 82.2375% Time 543.17
09/28 02:09:22 PM | Start to train theta for epoch 71
09/28 02:09:42 PM | Train: [ 72/180] Step 050/312 Loss 5.263 Prec@(1,3) (79.1%, 98.8%), ce_loss 0.827, lat_loss 21.865
09/28 02:09:59 PM | Train: [ 72/180] Step 100/312 Loss 5.115 Prec@(1,3) (80.3%, 98.9%), ce_loss 0.827, lat_loss 21.865
09/28 02:10:11 PM | Train: [ 72/180] Step 150/312 Loss 5.214 Prec@(1,3) (80.0%, 98.7%), ce_loss 0.827, lat_loss 21.865
09/28 02:10:24 PM | Train: [ 72/180] Step 200/312 Loss 5.231 Prec@(1,3) (80.0%, 98.6%), ce_loss 0.827, lat_loss 21.865
09/28 02:10:36 PM | Train: [ 72/180] Step 250/312 Loss 5.392 Prec@(1,3) (79.6%, 98.6%), ce_loss 0.827, lat_loss 21.865
09/28 02:10:48 PM | Train: [ 72/180] Step 300/312 Loss 5.313 Prec@(1,3) (79.8%, 98.7%), ce_loss 0.826, lat_loss 21.865
09/28 02:10:51 PM | Train: [ 72/180] Step 312/312 Loss 5.315 Prec@(1,3) (79.8%, 98.7%), ce_loss 0.826, lat_loss 21.865
09/28 02:10:51 PM | _theta_step_train: [ 72/180] Final Prec@1 79.7800% Time 89.66
09/28 02:10:57 PM | Valid: [ 72/180] Step 050/312 Loss 4.701 Prec@(1,3) (81.9%, 99.1%), ce_loss 0.826, lat_loss 21.865
09/28 02:11:01 PM | Valid: [ 72/180] Step 100/312 Loss 4.970 Prec@(1,3) (80.8%, 98.9%), ce_loss 0.826, lat_loss 21.865
09/28 02:11:06 PM | Valid: [ 72/180] Step 150/312 Loss 4.919 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.826, lat_loss 21.865
09/28 02:11:10 PM | Valid: [ 72/180] Step 200/312 Loss 4.955 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.826, lat_loss 21.865
09/28 02:11:15 PM | Valid: [ 72/180] Step 250/312 Loss 4.885 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.826, lat_loss 21.865
09/28 02:11:19 PM | Valid: [ 72/180] Step 300/312 Loss 4.843 Prec@(1,3) (81.6%, 98.8%), ce_loss 0.826, lat_loss 21.865
09/28 02:11:21 PM | Valid: [ 72/180] Step 312/312 Loss 4.812 Prec@(1,3) (81.7%, 98.9%), ce_loss 0.826, lat_loss 21.865
09/28 02:11:21 PM | val: [ 72/180] Final Prec@1 81.7000% Time 29.40
09/28 02:11:21 PM | Best top1 acc by now. Save model
09/28 02:11:21 PM | Start to train weights for epoch 72
09/28 02:11:47 PM | Train: [ 73/180] Step 050/1249 Loss 4.479 Prec@(1,3) (83.3%, 99.1%), ce_loss 0.826, lat_loss 21.864
09/28 02:12:09 PM | Train: [ 73/180] Step 100/1249 Loss 4.658 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.826, lat_loss 21.864
09/28 02:12:30 PM | Train: [ 73/180] Step 150/1249 Loss 4.754 Prec@(1,3) (81.7%, 98.9%), ce_loss 0.825, lat_loss 21.864
09/28 02:12:54 PM | Train: [ 73/180] Step 200/1249 Loss 4.713 Prec@(1,3) (82.0%, 99.0%), ce_loss 0.825, lat_loss 21.864
09/28 02:13:19 PM | Train: [ 73/180] Step 250/1249 Loss 4.756 Prec@(1,3) (81.8%, 99.0%), ce_loss 0.825, lat_loss 21.864
09/28 02:13:43 PM | Train: [ 73/180] Step 300/1249 Loss 4.762 Prec@(1,3) (81.8%, 99.0%), ce_loss 0.825, lat_loss 21.864
09/28 02:14:06 PM | Train: [ 73/180] Step 350/1249 Loss 4.789 Prec@(1,3) (81.7%, 99.0%), ce_loss 0.825, lat_loss 21.864
09/28 02:14:31 PM | Train: [ 73/180] Step 400/1249 Loss 4.781 Prec@(1,3) (81.8%, 98.9%), ce_loss 0.825, lat_loss 21.864
09/28 02:14:55 PM | Train: [ 73/180] Step 450/1249 Loss 4.760 Prec@(1,3) (81.9%, 99.0%), ce_loss 0.825, lat_loss 21.864
09/28 02:15:20 PM | Train: [ 73/180] Step 500/1249 Loss 4.742 Prec@(1,3) (81.9%, 99.0%), ce_loss 0.825, lat_loss 21.864
09/28 02:15:44 PM | Train: [ 73/180] Step 550/1249 Loss 4.746 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.825, lat_loss 21.864
09/28 02:16:09 PM | Train: [ 73/180] Step 600/1249 Loss 4.714 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.824, lat_loss 21.864
09/28 02:16:33 PM | Train: [ 73/180] Step 650/1249 Loss 4.734 Prec@(1,3) (82.0%, 99.0%), ce_loss 0.824, lat_loss 21.864
09/28 02:16:58 PM | Train: [ 73/180] Step 700/1249 Loss 4.705 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.824, lat_loss 21.864
09/28 02:17:22 PM | Train: [ 73/180] Step 750/1249 Loss 4.704 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.824, lat_loss 21.864
09/28 02:17:47 PM | Train: [ 73/180] Step 800/1249 Loss 4.690 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.824, lat_loss 21.864
09/28 02:18:11 PM | Train: [ 73/180] Step 850/1249 Loss 4.651 Prec@(1,3) (82.4%, 99.0%), ce_loss 0.824, lat_loss 21.863
09/28 02:18:35 PM | Train: [ 73/180] Step 900/1249 Loss 4.652 Prec@(1,3) (82.4%, 99.0%), ce_loss 0.824, lat_loss 21.863
09/28 02:19:00 PM | Train: [ 73/180] Step 950/1249 Loss 4.675 Prec@(1,3) (82.2%, 99.0%), ce_loss 0.824, lat_loss 21.863
09/28 02:19:25 PM | Train: [ 73/180] Step 1000/1249 Loss 4.672 Prec@(1,3) (82.2%, 99.0%), ce_loss 0.824, lat_loss 21.863
09/28 02:19:50 PM | Train: [ 73/180] Step 1050/1249 Loss 4.726 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.823, lat_loss 21.863
09/28 02:20:15 PM | Train: [ 73/180] Step 1100/1249 Loss 4.731 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.823, lat_loss 21.863
09/28 02:20:40 PM | Train: [ 73/180] Step 1150/1249 Loss 4.715 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.823, lat_loss 21.863
09/28 02:21:05 PM | Train: [ 73/180] Step 1200/1249 Loss 4.726 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.823, lat_loss 21.863
09/28 02:21:29 PM | Train: [ 73/180] Step 1249/1249 Loss 4.725 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.823, lat_loss 21.863
09/28 02:21:29 PM | _w_step_train: [ 73/180] Final Prec@1 82.1275% Time 608.54
09/28 02:21:29 PM | Start to train theta for epoch 72
09/28 02:21:51 PM | Train: [ 73/180] Step 050/312 Loss 5.234 Prec@(1,3) (81.0%, 98.7%), ce_loss 0.823, lat_loss 21.863
09/28 02:22:12 PM | Train: [ 73/180] Step 100/312 Loss 5.047 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.823, lat_loss 21.863
09/28 02:22:32 PM | Train: [ 73/180] Step 150/312 Loss 5.066 Prec@(1,3) (81.1%, 98.7%), ce_loss 0.823, lat_loss 21.863
09/28 02:22:52 PM | Train: [ 73/180] Step 200/312 Loss 5.069 Prec@(1,3) (81.0%, 98.7%), ce_loss 0.823, lat_loss 21.863
09/28 02:23:12 PM | Train: [ 73/180] Step 250/312 Loss 5.052 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.823, lat_loss 21.863
09/28 02:23:31 PM | Train: [ 73/180] Step 300/312 Loss 5.031 Prec@(1,3) (81.1%, 98.8%), ce_loss 0.822, lat_loss 21.863
09/28 02:23:36 PM | Train: [ 73/180] Step 312/312 Loss 5.031 Prec@(1,3) (81.1%, 98.8%), ce_loss 0.822, lat_loss 21.863
09/28 02:23:36 PM | _theta_step_train: [ 73/180] Final Prec@1 81.1200% Time 126.52
09/28 02:23:41 PM | Valid: [ 73/180] Step 050/312 Loss 4.283 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.822, lat_loss 21.863
09/28 02:23:46 PM | Valid: [ 73/180] Step 100/312 Loss 4.648 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.822, lat_loss 21.862
09/28 02:23:50 PM | Valid: [ 73/180] Step 150/312 Loss 4.789 Prec@(1,3) (82.6%, 98.6%), ce_loss 0.822, lat_loss 21.862
09/28 02:23:55 PM | Valid: [ 73/180] Step 200/312 Loss 4.715 Prec@(1,3) (82.6%, 98.7%), ce_loss 0.822, lat_loss 21.862
09/28 02:24:00 PM | Valid: [ 73/180] Step 250/312 Loss 4.771 Prec@(1,3) (82.4%, 98.6%), ce_loss 0.822, lat_loss 21.862
09/28 02:24:05 PM | Valid: [ 73/180] Step 300/312 Loss 4.735 Prec@(1,3) (82.4%, 98.7%), ce_loss 0.822, lat_loss 21.862
09/28 02:24:06 PM | Valid: [ 73/180] Step 312/312 Loss 4.746 Prec@(1,3) (82.3%, 98.7%), ce_loss 0.822, lat_loss 21.862
09/28 02:24:06 PM | val: [ 73/180] Final Prec@1 82.2700% Time 29.77
09/28 02:24:06 PM | Best top1 acc by now. Save model
09/28 02:24:06 PM | Start to train weights for epoch 73
09/28 02:24:31 PM | Train: [ 74/180] Step 050/1249 Loss 4.653 Prec@(1,3) (82.6%, 98.9%), ce_loss 0.822, lat_loss 21.862
09/28 02:24:52 PM | Train: [ 74/180] Step 100/1249 Loss 4.506 Prec@(1,3) (82.8%, 99.1%), ce_loss 0.822, lat_loss 21.862
09/28 02:25:14 PM | Train: [ 74/180] Step 150/1249 Loss 4.528 Prec@(1,3) (82.7%, 99.0%), ce_loss 0.821, lat_loss 21.862
09/28 02:25:36 PM | Train: [ 74/180] Step 200/1249 Loss 4.544 Prec@(1,3) (82.8%, 99.0%), ce_loss 0.821, lat_loss 21.862
09/28 02:25:58 PM | Train: [ 74/180] Step 250/1249 Loss 4.434 Prec@(1,3) (83.3%, 99.1%), ce_loss 0.821, lat_loss 21.862
09/28 02:26:20 PM | Train: [ 74/180] Step 300/1249 Loss 4.526 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.821, lat_loss 21.862
09/28 02:26:42 PM | Train: [ 74/180] Step 350/1249 Loss 4.512 Prec@(1,3) (83.1%, 98.9%), ce_loss 0.821, lat_loss 21.862
09/28 02:27:03 PM | Train: [ 74/180] Step 400/1249 Loss 4.543 Prec@(1,3) (82.8%, 98.9%), ce_loss 0.821, lat_loss 21.862
09/28 02:27:25 PM | Train: [ 74/180] Step 450/1249 Loss 4.545 Prec@(1,3) (82.8%, 98.9%), ce_loss 0.821, lat_loss 21.862
09/28 02:27:48 PM | Train: [ 74/180] Step 500/1249 Loss 4.578 Prec@(1,3) (82.6%, 98.9%), ce_loss 0.821, lat_loss 21.862
09/28 02:28:11 PM | Train: [ 74/180] Step 550/1249 Loss 4.595 Prec@(1,3) (82.6%, 98.9%), ce_loss 0.821, lat_loss 21.862
09/28 02:28:34 PM | Train: [ 74/180] Step 600/1249 Loss 4.550 Prec@(1,3) (82.8%, 98.9%), ce_loss 0.820, lat_loss 21.861
09/28 02:28:49 PM | Train: [ 74/180] Step 650/1249 Loss 4.588 Prec@(1,3) (82.6%, 98.9%), ce_loss 0.820, lat_loss 21.861
09/28 02:29:05 PM | Train: [ 74/180] Step 700/1249 Loss 4.600 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.820, lat_loss 21.861
09/28 02:29:21 PM | Train: [ 74/180] Step 750/1249 Loss 4.612 Prec@(1,3) (82.4%, 98.9%), ce_loss 0.820, lat_loss 21.861
09/28 02:29:37 PM | Train: [ 74/180] Step 800/1249 Loss 4.607 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.820, lat_loss 21.861
09/28 02:29:53 PM | Train: [ 74/180] Step 850/1249 Loss 4.605 Prec@(1,3) (82.6%, 98.8%), ce_loss 0.820, lat_loss 21.861
09/28 02:30:09 PM | Train: [ 74/180] Step 900/1249 Loss 4.599 Prec@(1,3) (82.6%, 98.8%), ce_loss 0.820, lat_loss 21.861
09/28 02:30:25 PM | Train: [ 74/180] Step 950/1249 Loss 4.584 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.820, lat_loss 21.861
09/28 02:30:41 PM | Train: [ 74/180] Step 1000/1249 Loss 4.575 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.820, lat_loss 21.861
09/28 02:30:57 PM | Train: [ 74/180] Step 1050/1249 Loss 4.574 Prec@(1,3) (82.6%, 98.9%), ce_loss 0.819, lat_loss 21.861
09/28 02:31:12 PM | Train: [ 74/180] Step 1100/1249 Loss 4.557 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.819, lat_loss 21.861
09/28 02:31:28 PM | Train: [ 74/180] Step 1150/1249 Loss 4.551 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.819, lat_loss 21.861
09/28 02:31:44 PM | Train: [ 74/180] Step 1200/1249 Loss 4.557 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.819, lat_loss 21.861
09/28 02:32:00 PM | Train: [ 74/180] Step 1249/1249 Loss 4.581 Prec@(1,3) (82.6%, 98.9%), ce_loss 0.819, lat_loss 21.861
09/28 02:32:00 PM | _w_step_train: [ 74/180] Final Prec@1 82.5550% Time 473.85
09/28 02:32:00 PM | Start to train theta for epoch 73
09/28 02:32:21 PM | Train: [ 74/180] Step 050/312 Loss 5.213 Prec@(1,3) (80.5%, 98.5%), ce_loss 0.819, lat_loss 21.861
09/28 02:32:42 PM | Train: [ 74/180] Step 100/312 Loss 5.239 Prec@(1,3) (80.4%, 98.5%), ce_loss 0.819, lat_loss 21.861
09/28 02:33:02 PM | Train: [ 74/180] Step 150/312 Loss 5.230 Prec@(1,3) (80.3%, 98.6%), ce_loss 0.819, lat_loss 21.861
09/28 02:33:23 PM | Train: [ 74/180] Step 200/312 Loss 5.116 Prec@(1,3) (80.9%, 98.6%), ce_loss 0.819, lat_loss 21.860
09/28 02:33:43 PM | Train: [ 74/180] Step 250/312 Loss 5.139 Prec@(1,3) (80.7%, 98.6%), ce_loss 0.819, lat_loss 21.860
09/28 02:34:04 PM | Train: [ 74/180] Step 300/312 Loss 5.182 Prec@(1,3) (80.3%, 98.7%), ce_loss 0.819, lat_loss 21.860
09/28 02:34:09 PM | Train: [ 74/180] Step 312/312 Loss 5.194 Prec@(1,3) (80.2%, 98.7%), ce_loss 0.818, lat_loss 21.860
09/28 02:34:09 PM | _theta_step_train: [ 74/180] Final Prec@1 80.2500% Time 128.86
09/28 02:34:14 PM | Valid: [ 74/180] Step 050/312 Loss 4.878 Prec@(1,3) (81.6%, 99.1%), ce_loss 0.818, lat_loss 21.860
09/28 02:34:19 PM | Valid: [ 74/180] Step 100/312 Loss 5.333 Prec@(1,3) (80.2%, 98.7%), ce_loss 0.818, lat_loss 21.860
09/28 02:34:23 PM | Valid: [ 74/180] Step 150/312 Loss 5.374 Prec@(1,3) (80.2%, 98.7%), ce_loss 0.818, lat_loss 21.860
09/28 02:34:28 PM | Valid: [ 74/180] Step 200/312 Loss 5.300 Prec@(1,3) (80.8%, 98.8%), ce_loss 0.818, lat_loss 21.860
09/28 02:34:32 PM | Valid: [ 74/180] Step 250/312 Loss 5.228 Prec@(1,3) (80.8%, 98.8%), ce_loss 0.818, lat_loss 21.860
09/28 02:34:38 PM | Valid: [ 74/180] Step 300/312 Loss 5.164 Prec@(1,3) (80.8%, 98.9%), ce_loss 0.818, lat_loss 21.860
09/28 02:34:39 PM | Valid: [ 74/180] Step 312/312 Loss 5.149 Prec@(1,3) (80.9%, 98.9%), ce_loss 0.818, lat_loss 21.860
09/28 02:34:39 PM | val: [ 74/180] Final Prec@1 80.9000% Time 30.06
09/28 02:34:39 PM | Start to train weights for epoch 74
09/28 02:35:05 PM | Train: [ 75/180] Step 050/1249 Loss 4.826 Prec@(1,3) (82.2%, 98.7%), ce_loss 0.818, lat_loss 21.860
09/28 02:35:30 PM | Train: [ 75/180] Step 100/1249 Loss 4.775 Prec@(1,3) (81.8%, 98.5%), ce_loss 0.818, lat_loss 21.860
09/28 02:35:55 PM | Train: [ 75/180] Step 150/1249 Loss 4.555 Prec@(1,3) (82.7%, 98.7%), ce_loss 0.818, lat_loss 21.860
09/28 02:36:20 PM | Train: [ 75/180] Step 200/1249 Loss 4.527 Prec@(1,3) (83.0%, 98.8%), ce_loss 0.818, lat_loss 21.860
09/28 02:36:45 PM | Train: [ 75/180] Step 250/1249 Loss 4.464 Prec@(1,3) (83.3%, 98.9%), ce_loss 0.817, lat_loss 21.860
09/28 02:37:09 PM | Train: [ 75/180] Step 300/1249 Loss 4.484 Prec@(1,3) (83.0%, 98.9%), ce_loss 0.817, lat_loss 21.860
09/28 02:37:32 PM | Train: [ 75/180] Step 350/1249 Loss 4.501 Prec@(1,3) (82.9%, 98.9%), ce_loss 0.817, lat_loss 21.860
09/28 02:37:57 PM | Train: [ 75/180] Step 400/1249 Loss 4.485 Prec@(1,3) (83.0%, 98.9%), ce_loss 0.817, lat_loss 21.859
09/28 02:38:22 PM | Train: [ 75/180] Step 450/1249 Loss 4.475 Prec@(1,3) (83.0%, 98.9%), ce_loss 0.817, lat_loss 21.859
09/28 02:38:47 PM | Train: [ 75/180] Step 500/1249 Loss 4.470 Prec@(1,3) (83.1%, 98.9%), ce_loss 0.817, lat_loss 21.859
09/28 02:39:12 PM | Train: [ 75/180] Step 550/1249 Loss 4.468 Prec@(1,3) (83.1%, 98.9%), ce_loss 0.817, lat_loss 21.859
09/28 02:39:36 PM | Train: [ 75/180] Step 600/1249 Loss 4.482 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.817, lat_loss 21.859
09/28 02:40:01 PM | Train: [ 75/180] Step 650/1249 Loss 4.465 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:40:26 PM | Train: [ 75/180] Step 700/1249 Loss 4.479 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:40:50 PM | Train: [ 75/180] Step 750/1249 Loss 4.448 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:41:15 PM | Train: [ 75/180] Step 800/1249 Loss 4.466 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:41:30 PM | Train: [ 75/180] Step 850/1249 Loss 4.469 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:41:46 PM | Train: [ 75/180] Step 900/1249 Loss 4.454 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:42:02 PM | Train: [ 75/180] Step 950/1249 Loss 4.462 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:42:18 PM | Train: [ 75/180] Step 1000/1249 Loss 4.479 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:42:34 PM | Train: [ 75/180] Step 1050/1249 Loss 4.468 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.816, lat_loss 21.859
09/28 02:42:50 PM | Train: [ 75/180] Step 1100/1249 Loss 4.499 Prec@(1,3) (82.9%, 99.0%), ce_loss 0.815, lat_loss 21.859
09/28 02:43:14 PM | Train: [ 75/180] Step 1150/1249 Loss 4.509 Prec@(1,3) (82.8%, 99.0%), ce_loss 0.815, lat_loss 21.859
09/28 02:43:39 PM | Train: [ 75/180] Step 1200/1249 Loss 4.513 Prec@(1,3) (82.8%, 99.0%), ce_loss 0.815, lat_loss 21.859
09/28 02:44:03 PM | Train: [ 75/180] Step 1249/1249 Loss 4.504 Prec@(1,3) (82.8%, 99.0%), ce_loss 0.815, lat_loss 21.859
09/28 02:44:03 PM | _w_step_train: [ 75/180] Final Prec@1 82.8050% Time 564.40
09/28 02:44:03 PM | Start to train theta for epoch 74
09/28 02:44:17 PM | Train: [ 75/180] Step 050/312 Loss 5.237 Prec@(1,3) (80.0%, 98.3%), ce_loss 0.815, lat_loss 21.859
09/28 02:44:37 PM | Train: [ 75/180] Step 100/312 Loss 4.899 Prec@(1,3) (81.1%, 98.6%), ce_loss 0.815, lat_loss 21.858
09/28 02:44:58 PM | Train: [ 75/180] Step 150/312 Loss 4.956 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.815, lat_loss 21.858
09/28 02:45:19 PM | Train: [ 75/180] Step 200/312 Loss 4.962 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.815, lat_loss 21.858
09/28 02:45:40 PM | Train: [ 75/180] Step 250/312 Loss 5.038 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.815, lat_loss 21.858
09/28 02:46:01 PM | Train: [ 75/180] Step 300/312 Loss 5.053 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.815, lat_loss 21.858
09/28 02:46:06 PM | Train: [ 75/180] Step 312/312 Loss 5.047 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.815, lat_loss 21.858
09/28 02:46:06 PM | _theta_step_train: [ 75/180] Final Prec@1 81.3800% Time 122.48
09/28 02:46:11 PM | Valid: [ 75/180] Step 050/312 Loss 4.627 Prec@(1,3) (83.2%, 99.0%), ce_loss 0.815, lat_loss 21.858
09/28 02:46:16 PM | Valid: [ 75/180] Step 100/312 Loss 4.647 Prec@(1,3) (82.8%, 98.9%), ce_loss 0.814, lat_loss 21.858
09/28 02:46:20 PM | Valid: [ 75/180] Step 150/312 Loss 4.701 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.814, lat_loss 21.858
09/28 02:46:25 PM | Valid: [ 75/180] Step 200/312 Loss 4.729 Prec@(1,3) (82.2%, 98.9%), ce_loss 0.814, lat_loss 21.858
09/28 02:46:29 PM | Valid: [ 75/180] Step 250/312 Loss 4.763 Prec@(1,3) (82.1%, 99.0%), ce_loss 0.814, lat_loss 21.858
09/28 02:46:34 PM | Valid: [ 75/180] Step 300/312 Loss 4.819 Prec@(1,3) (81.9%, 98.8%), ce_loss 0.814, lat_loss 21.858
09/28 02:46:35 PM | Valid: [ 75/180] Step 312/312 Loss 4.813 Prec@(1,3) (81.9%, 98.9%), ce_loss 0.814, lat_loss 21.858
09/28 02:46:35 PM | val: [ 75/180] Final Prec@1 81.8800% Time 29.57
09/28 02:46:35 PM | Start to train weights for epoch 75
09/28 02:47:02 PM | Train: [ 76/180] Step 050/1249 Loss 4.386 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.814, lat_loss 21.858
09/28 02:47:24 PM | Train: [ 76/180] Step 100/1249 Loss 4.330 Prec@(1,3) (84.0%, 99.0%), ce_loss 0.814, lat_loss 21.858
09/28 02:47:49 PM | Train: [ 76/180] Step 150/1249 Loss 4.232 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.814, lat_loss 21.858
09/28 02:48:13 PM | Train: [ 76/180] Step 200/1249 Loss 4.267 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.814, lat_loss 21.858
09/28 02:48:38 PM | Train: [ 76/180] Step 250/1249 Loss 4.293 Prec@(1,3) (84.0%, 99.1%), ce_loss 0.813, lat_loss 21.858
09/28 02:49:03 PM | Train: [ 76/180] Step 300/1249 Loss 4.286 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.813, lat_loss 21.858
09/28 02:49:28 PM | Train: [ 76/180] Step 350/1249 Loss 4.274 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.813, lat_loss 21.857
09/28 02:49:53 PM | Train: [ 76/180] Step 400/1249 Loss 4.282 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.813, lat_loss 21.857
09/28 02:50:18 PM | Train: [ 76/180] Step 450/1249 Loss 4.260 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.813, lat_loss 21.857
09/28 02:50:43 PM | Train: [ 76/180] Step 500/1249 Loss 4.295 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.813, lat_loss 21.857
09/28 02:51:08 PM | Train: [ 76/180] Step 550/1249 Loss 4.350 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.813, lat_loss 21.857
09/28 02:51:33 PM | Train: [ 76/180] Step 600/1249 Loss 4.370 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.813, lat_loss 21.857
09/28 02:51:58 PM | Train: [ 76/180] Step 650/1249 Loss 4.432 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.813, lat_loss 21.857
09/28 02:52:23 PM | Train: [ 76/180] Step 700/1249 Loss 4.436 Prec@(1,3) (83.5%, 98.9%), ce_loss 0.812, lat_loss 21.857
09/28 02:52:48 PM | Train: [ 76/180] Step 750/1249 Loss 4.438 Prec@(1,3) (83.5%, 98.9%), ce_loss 0.812, lat_loss 21.857
09/28 02:53:12 PM | Train: [ 76/180] Step 800/1249 Loss 4.452 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.812, lat_loss 21.857
09/28 02:53:36 PM | Train: [ 76/180] Step 850/1249 Loss 4.458 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.812, lat_loss 21.857
09/28 02:54:00 PM | Train: [ 76/180] Step 900/1249 Loss 4.462 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.812, lat_loss 21.857
09/28 02:54:25 PM | Train: [ 76/180] Step 950/1249 Loss 4.466 Prec@(1,3) (83.2%, 99.0%), ce_loss 0.812, lat_loss 21.857
09/28 02:54:48 PM | Train: [ 76/180] Step 1000/1249 Loss 4.447 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.812, lat_loss 21.857
09/28 02:55:12 PM | Train: [ 76/180] Step 1050/1249 Loss 4.468 Prec@(1,3) (83.2%, 99.0%), ce_loss 0.812, lat_loss 21.857
09/28 02:55:36 PM | Train: [ 76/180] Step 1100/1249 Loss 4.466 Prec@(1,3) (83.2%, 99.0%), ce_loss 0.812, lat_loss 21.857
09/28 02:55:59 PM | Train: [ 76/180] Step 1150/1249 Loss 4.471 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.811, lat_loss 21.857
09/28 02:56:23 PM | Train: [ 76/180] Step 1200/1249 Loss 4.467 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.811, lat_loss 21.857
09/28 02:56:47 PM | Train: [ 76/180] Step 1249/1249 Loss 4.465 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.811, lat_loss 21.857
09/28 02:56:47 PM | _w_step_train: [ 76/180] Final Prec@1 83.1325% Time 611.72
09/28 02:56:47 PM | Start to train theta for epoch 75
09/28 02:57:00 PM | Train: [ 76/180] Step 050/312 Loss 5.271 Prec@(1,3) (79.7%, 98.5%), ce_loss 0.811, lat_loss 21.856
09/28 02:57:12 PM | Train: [ 76/180] Step 100/312 Loss 4.999 Prec@(1,3) (80.7%, 98.7%), ce_loss 0.811, lat_loss 21.856
09/28 02:57:25 PM | Train: [ 76/180] Step 150/312 Loss 4.946 Prec@(1,3) (80.9%, 98.8%), ce_loss 0.811, lat_loss 21.856
09/28 02:57:37 PM | Train: [ 76/180] Step 200/312 Loss 4.985 Prec@(1,3) (80.9%, 98.8%), ce_loss 0.811, lat_loss 21.856
09/28 02:57:49 PM | Train: [ 76/180] Step 250/312 Loss 4.944 Prec@(1,3) (81.2%, 98.9%), ce_loss 0.811, lat_loss 21.856
09/28 02:58:01 PM | Train: [ 76/180] Step 300/312 Loss 4.852 Prec@(1,3) (81.6%, 98.9%), ce_loss 0.811, lat_loss 21.856
09/28 02:58:04 PM | Train: [ 76/180] Step 312/312 Loss 4.846 Prec@(1,3) (81.6%, 98.9%), ce_loss 0.811, lat_loss 21.856
09/28 02:58:04 PM | _theta_step_train: [ 76/180] Final Prec@1 81.6200% Time 77.42
09/28 02:58:10 PM | Valid: [ 76/180] Step 050/312 Loss 4.183 Prec@(1,3) (83.8%, 99.4%), ce_loss 0.811, lat_loss 21.856
09/28 02:58:14 PM | Valid: [ 76/180] Step 100/312 Loss 4.753 Prec@(1,3) (82.3%, 98.3%), ce_loss 0.810, lat_loss 21.856
09/28 02:58:19 PM | Valid: [ 76/180] Step 150/312 Loss 4.897 Prec@(1,3) (81.6%, 98.2%), ce_loss 0.810, lat_loss 21.856
09/28 02:58:23 PM | Valid: [ 76/180] Step 200/312 Loss 4.947 Prec@(1,3) (81.5%, 98.3%), ce_loss 0.810, lat_loss 21.856
09/28 02:58:28 PM | Valid: [ 76/180] Step 250/312 Loss 4.831 Prec@(1,3) (81.8%, 98.5%), ce_loss 0.810, lat_loss 21.856
09/28 02:58:33 PM | Valid: [ 76/180] Step 300/312 Loss 4.762 Prec@(1,3) (82.0%, 98.6%), ce_loss 0.810, lat_loss 21.856
09/28 02:58:34 PM | Valid: [ 76/180] Step 312/312 Loss 4.882 Prec@(1,3) (81.7%, 98.5%), ce_loss 0.810, lat_loss 21.856
09/28 02:58:34 PM | val: [ 76/180] Final Prec@1 81.7300% Time 29.86
09/28 02:58:34 PM | Start to train weights for epoch 76
09/28 02:59:00 PM | Train: [ 77/180] Step 050/1249 Loss 4.700 Prec@(1,3) (81.6%, 99.2%), ce_loss 0.810, lat_loss 21.856
09/28 02:59:22 PM | Train: [ 77/180] Step 100/1249 Loss 4.391 Prec@(1,3) (82.9%, 99.1%), ce_loss 0.810, lat_loss 21.856
09/28 02:59:42 PM | Train: [ 77/180] Step 150/1249 Loss 4.415 Prec@(1,3) (83.0%, 99.3%), ce_loss 0.810, lat_loss 21.856
09/28 03:00:03 PM | Train: [ 77/180] Step 200/1249 Loss 4.295 Prec@(1,3) (83.2%, 99.3%), ce_loss 0.810, lat_loss 21.856
09/28 03:00:24 PM | Train: [ 77/180] Step 250/1249 Loss 4.294 Prec@(1,3) (83.3%, 99.2%), ce_loss 0.810, lat_loss 21.856
09/28 03:00:45 PM | Train: [ 77/180] Step 300/1249 Loss 4.231 Prec@(1,3) (83.6%, 99.2%), ce_loss 0.809, lat_loss 21.856
09/28 03:01:06 PM | Train: [ 77/180] Step 350/1249 Loss 4.237 Prec@(1,3) (83.5%, 99.2%), ce_loss 0.809, lat_loss 21.856
09/28 03:01:26 PM | Train: [ 77/180] Step 400/1249 Loss 4.232 Prec@(1,3) (83.6%, 99.2%), ce_loss 0.809, lat_loss 21.855
09/28 03:01:47 PM | Train: [ 77/180] Step 450/1249 Loss 4.256 Prec@(1,3) (83.5%, 99.2%), ce_loss 0.809, lat_loss 21.855
09/28 03:02:08 PM | Train: [ 77/180] Step 500/1249 Loss 4.280 Prec@(1,3) (83.5%, 99.2%), ce_loss 0.809, lat_loss 21.855
09/28 03:02:30 PM | Train: [ 77/180] Step 550/1249 Loss 4.227 Prec@(1,3) (83.7%, 99.2%), ce_loss 0.809, lat_loss 21.855
09/28 03:02:52 PM | Train: [ 77/180] Step 600/1249 Loss 4.211 Prec@(1,3) (83.8%, 99.2%), ce_loss 0.809, lat_loss 21.855
09/28 03:03:15 PM | Train: [ 77/180] Step 650/1249 Loss 4.239 Prec@(1,3) (83.7%, 99.2%), ce_loss 0.809, lat_loss 21.855
09/28 03:03:39 PM | Train: [ 77/180] Step 700/1249 Loss 4.242 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:04:02 PM | Train: [ 77/180] Step 750/1249 Loss 4.239 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:04:23 PM | Train: [ 77/180] Step 800/1249 Loss 4.246 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:04:44 PM | Train: [ 77/180] Step 850/1249 Loss 4.266 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:05:04 PM | Train: [ 77/180] Step 900/1249 Loss 4.269 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:05:26 PM | Train: [ 77/180] Step 950/1249 Loss 4.261 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:05:47 PM | Train: [ 77/180] Step 1000/1249 Loss 4.255 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:06:08 PM | Train: [ 77/180] Step 1050/1249 Loss 4.269 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:06:29 PM | Train: [ 77/180] Step 1100/1249 Loss 4.283 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.808, lat_loss 21.855
09/28 03:06:50 PM | Train: [ 77/180] Step 1150/1249 Loss 4.305 Prec@(1,3) (83.6%, 99.1%), ce_loss 0.807, lat_loss 21.855
09/28 03:07:14 PM | Train: [ 77/180] Step 1200/1249 Loss 4.312 Prec@(1,3) (83.6%, 99.1%), ce_loss 0.807, lat_loss 21.855
09/28 03:07:38 PM | Train: [ 77/180] Step 1249/1249 Loss 4.314 Prec@(1,3) (83.6%, 99.1%), ce_loss 0.807, lat_loss 21.855
09/28 03:07:38 PM | _w_step_train: [ 77/180] Final Prec@1 83.5625% Time 543.97
09/28 03:07:38 PM | Start to train theta for epoch 76
09/28 03:07:59 PM | Train: [ 77/180] Step 050/312 Loss 4.760 Prec@(1,3) (82.0%, 98.6%), ce_loss 0.807, lat_loss 21.855
09/28 03:08:20 PM | Train: [ 77/180] Step 100/312 Loss 4.978 Prec@(1,3) (81.1%, 98.8%), ce_loss 0.807, lat_loss 21.855
09/28 03:08:39 PM | Train: [ 77/180] Step 150/312 Loss 5.021 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.807, lat_loss 21.854
09/28 03:09:00 PM | Train: [ 77/180] Step 200/312 Loss 4.956 Prec@(1,3) (81.5%, 98.7%), ce_loss 0.807, lat_loss 21.854
09/28 03:09:21 PM | Train: [ 77/180] Step 250/312 Loss 5.016 Prec@(1,3) (81.0%, 98.8%), ce_loss 0.807, lat_loss 21.854
09/28 03:09:42 PM | Train: [ 77/180] Step 300/312 Loss 4.998 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.807, lat_loss 21.854
09/28 03:09:47 PM | Train: [ 77/180] Step 312/312 Loss 4.984 Prec@(1,3) (81.3%, 98.7%), ce_loss 0.807, lat_loss 21.854
09/28 03:09:47 PM | _theta_step_train: [ 77/180] Final Prec@1 81.3100% Time 128.74
09/28 03:09:52 PM | Valid: [ 77/180] Step 050/312 Loss 4.425 Prec@(1,3) (83.0%, 99.1%), ce_loss 0.807, lat_loss 21.854
09/28 03:09:57 PM | Valid: [ 77/180] Step 100/312 Loss 4.985 Prec@(1,3) (81.5%, 98.6%), ce_loss 0.807, lat_loss 21.854
09/28 03:10:01 PM | Valid: [ 77/180] Step 150/312 Loss 5.006 Prec@(1,3) (81.3%, 98.4%), ce_loss 0.806, lat_loss 21.854
09/28 03:10:06 PM | Valid: [ 77/180] Step 200/312 Loss 4.959 Prec@(1,3) (81.3%, 98.6%), ce_loss 0.806, lat_loss 21.854
09/28 03:10:10 PM | Valid: [ 77/180] Step 250/312 Loss 4.957 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.806, lat_loss 21.854
09/28 03:10:15 PM | Valid: [ 77/180] Step 300/312 Loss 4.992 Prec@(1,3) (81.4%, 98.6%), ce_loss 0.806, lat_loss 21.854
09/28 03:10:16 PM | Valid: [ 77/180] Step 312/312 Loss 4.980 Prec@(1,3) (81.5%, 98.7%), ce_loss 0.806, lat_loss 21.854
09/28 03:10:16 PM | val: [ 77/180] Final Prec@1 81.4500% Time 29.07
09/28 03:10:16 PM | Start to train weights for epoch 77
09/28 03:10:42 PM | Train: [ 78/180] Step 050/1249 Loss 3.914 Prec@(1,3) (84.8%, 99.3%), ce_loss 0.806, lat_loss 21.854
09/28 03:11:07 PM | Train: [ 78/180] Step 100/1249 Loss 4.165 Prec@(1,3) (84.0%, 99.3%), ce_loss 0.806, lat_loss 21.854
09/28 03:11:31 PM | Train: [ 78/180] Step 150/1249 Loss 4.117 Prec@(1,3) (84.0%, 99.3%), ce_loss 0.806, lat_loss 21.854
09/28 03:11:56 PM | Train: [ 78/180] Step 200/1249 Loss 4.247 Prec@(1,3) (83.6%, 99.3%), ce_loss 0.806, lat_loss 21.854
09/28 03:12:21 PM | Train: [ 78/180] Step 250/1249 Loss 4.227 Prec@(1,3) (83.8%, 99.3%), ce_loss 0.806, lat_loss 21.854
09/28 03:12:45 PM | Train: [ 78/180] Step 300/1249 Loss 4.167 Prec@(1,3) (84.1%, 99.3%), ce_loss 0.805, lat_loss 21.854
09/28 03:13:09 PM | Train: [ 78/180] Step 350/1249 Loss 4.203 Prec@(1,3) (83.9%, 99.3%), ce_loss 0.805, lat_loss 21.854
09/28 03:13:34 PM | Train: [ 78/180] Step 400/1249 Loss 4.264 Prec@(1,3) (83.8%, 99.2%), ce_loss 0.805, lat_loss 21.854
09/28 03:13:59 PM | Train: [ 78/180] Step 450/1249 Loss 4.262 Prec@(1,3) (83.7%, 99.2%), ce_loss 0.805, lat_loss 21.854
09/28 03:14:22 PM | Train: [ 78/180] Step 500/1249 Loss 4.261 Prec@(1,3) (83.6%, 99.2%), ce_loss 0.805, lat_loss 21.853
09/28 03:14:46 PM | Train: [ 78/180] Step 550/1249 Loss 4.258 Prec@(1,3) (83.6%, 99.2%), ce_loss 0.805, lat_loss 21.853
09/28 03:15:11 PM | Train: [ 78/180] Step 600/1249 Loss 4.236 Prec@(1,3) (83.7%, 99.2%), ce_loss 0.805, lat_loss 21.853
09/28 03:15:36 PM | Train: [ 78/180] Step 650/1249 Loss 4.264 Prec@(1,3) (83.6%, 99.2%), ce_loss 0.805, lat_loss 21.853
09/28 03:16:00 PM | Train: [ 78/180] Step 700/1249 Loss 4.248 Prec@(1,3) (83.7%, 99.2%), ce_loss 0.805, lat_loss 21.853
09/28 03:16:25 PM | Train: [ 78/180] Step 750/1249 Loss 4.238 Prec@(1,3) (83.7%, 99.2%), ce_loss 0.804, lat_loss 21.853
09/28 03:16:48 PM | Train: [ 78/180] Step 800/1249 Loss 4.255 Prec@(1,3) (83.6%, 99.1%), ce_loss 0.804, lat_loss 21.853
09/28 03:17:09 PM | Train: [ 78/180] Step 850/1249 Loss 4.259 Prec@(1,3) (83.6%, 99.2%), ce_loss 0.804, lat_loss 21.853
09/28 03:17:32 PM | Train: [ 78/180] Step 900/1249 Loss 4.265 Prec@(1,3) (83.6%, 99.1%), ce_loss 0.804, lat_loss 21.853
09/28 03:17:53 PM | Train: [ 78/180] Step 950/1249 Loss 4.272 Prec@(1,3) (83.6%, 99.1%), ce_loss 0.804, lat_loss 21.853
09/28 03:18:16 PM | Train: [ 78/180] Step 1000/1249 Loss 4.290 Prec@(1,3) (83.5%, 99.1%), ce_loss 0.804, lat_loss 21.853
09/28 03:18:39 PM | Train: [ 78/180] Step 1050/1249 Loss 4.291 Prec@(1,3) (83.5%, 99.1%), ce_loss 0.804, lat_loss 21.853
09/28 03:19:03 PM | Train: [ 78/180] Step 1100/1249 Loss 4.294 Prec@(1,3) (83.5%, 99.1%), ce_loss 0.804, lat_loss 21.853
09/28 03:19:25 PM | Train: [ 78/180] Step 1150/1249 Loss 4.290 Prec@(1,3) (83.5%, 99.1%), ce_loss 0.804, lat_loss 21.853
09/28 03:19:47 PM | Train: [ 78/180] Step 1200/1249 Loss 4.303 Prec@(1,3) (83.4%, 99.1%), ce_loss 0.804, lat_loss 21.853
09/28 03:20:12 PM | Train: [ 78/180] Step 1249/1249 Loss 4.310 Prec@(1,3) (83.4%, 99.1%), ce_loss 0.803, lat_loss 21.853
09/28 03:20:12 PM | _w_step_train: [ 78/180] Final Prec@1 83.4225% Time 595.68
09/28 03:20:12 PM | Start to train theta for epoch 77
09/28 03:20:33 PM | Train: [ 78/180] Step 050/312 Loss 5.035 Prec@(1,3) (79.3%, 98.6%), ce_loss 0.803, lat_loss 21.853
09/28 03:20:51 PM | Train: [ 78/180] Step 100/312 Loss 4.847 Prec@(1,3) (80.4%, 98.8%), ce_loss 0.803, lat_loss 21.853
09/28 03:21:07 PM | Train: [ 78/180] Step 150/312 Loss 4.882 Prec@(1,3) (80.4%, 98.7%), ce_loss 0.803, lat_loss 21.853
09/28 03:21:24 PM | Train: [ 78/180] Step 200/312 Loss 4.912 Prec@(1,3) (80.3%, 98.7%), ce_loss 0.803, lat_loss 21.852
09/28 03:21:42 PM | Train: [ 78/180] Step 250/312 Loss 4.906 Prec@(1,3) (80.3%, 98.7%), ce_loss 0.803, lat_loss 21.852
09/28 03:22:01 PM | Train: [ 78/180] Step 300/312 Loss 4.898 Prec@(1,3) (80.4%, 98.7%), ce_loss 0.803, lat_loss 21.852
09/28 03:22:06 PM | Train: [ 78/180] Step 312/312 Loss 4.892 Prec@(1,3) (80.4%, 98.7%), ce_loss 0.803, lat_loss 21.852
09/28 03:22:06 PM | _theta_step_train: [ 78/180] Final Prec@1 80.4100% Time 114.25
09/28 03:22:11 PM | Valid: [ 78/180] Step 050/312 Loss 4.385 Prec@(1,3) (83.3%, 99.2%), ce_loss 0.803, lat_loss 21.852
09/28 03:22:15 PM | Valid: [ 78/180] Step 100/312 Loss 4.529 Prec@(1,3) (82.4%, 99.2%), ce_loss 0.803, lat_loss 21.852
09/28 03:22:19 PM | Valid: [ 78/180] Step 150/312 Loss 4.578 Prec@(1,3) (82.5%, 99.1%), ce_loss 0.803, lat_loss 21.852
09/28 03:22:24 PM | Valid: [ 78/180] Step 200/312 Loss 4.693 Prec@(1,3) (81.8%, 99.0%), ce_loss 0.802, lat_loss 21.852
09/28 03:22:28 PM | Valid: [ 78/180] Step 250/312 Loss 4.714 Prec@(1,3) (81.7%, 98.9%), ce_loss 0.802, lat_loss 21.852
09/28 03:22:32 PM | Valid: [ 78/180] Step 300/312 Loss 4.649 Prec@(1,3) (81.9%, 98.9%), ce_loss 0.802, lat_loss 21.852
09/28 03:22:33 PM | Valid: [ 78/180] Step 312/312 Loss 4.667 Prec@(1,3) (81.9%, 99.0%), ce_loss 0.802, lat_loss 21.852
09/28 03:22:33 PM | val: [ 78/180] Final Prec@1 81.9000% Time 27.03
09/28 03:22:33 PM | Start to train weights for epoch 78
09/28 03:22:59 PM | Train: [ 79/180] Step 050/1249 Loss 3.854 Prec@(1,3) (85.5%, 99.4%), ce_loss 0.802, lat_loss 21.852
09/28 03:23:24 PM | Train: [ 79/180] Step 100/1249 Loss 4.579 Prec@(1,3) (83.2%, 98.7%), ce_loss 0.802, lat_loss 21.852
09/28 03:23:49 PM | Train: [ 79/180] Step 150/1249 Loss 4.416 Prec@(1,3) (83.4%, 98.7%), ce_loss 0.802, lat_loss 21.852
09/28 03:24:14 PM | Train: [ 79/180] Step 200/1249 Loss 4.407 Prec@(1,3) (83.5%, 98.8%), ce_loss 0.802, lat_loss 21.852
09/28 03:24:39 PM | Train: [ 79/180] Step 250/1249 Loss 4.333 Prec@(1,3) (83.7%, 98.9%), ce_loss 0.802, lat_loss 21.852
09/28 03:25:04 PM | Train: [ 79/180] Step 300/1249 Loss 4.353 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.802, lat_loss 21.852
09/28 03:25:29 PM | Train: [ 79/180] Step 350/1249 Loss 4.350 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.802, lat_loss 21.852
09/28 03:25:54 PM | Train: [ 79/180] Step 400/1249 Loss 4.365 Prec@(1,3) (83.4%, 99.0%), ce_loss 0.801, lat_loss 21.852
09/28 03:26:19 PM | Train: [ 79/180] Step 450/1249 Loss 4.357 Prec@(1,3) (83.4%, 99.0%), ce_loss 0.801, lat_loss 21.852
09/28 03:26:44 PM | Train: [ 79/180] Step 500/1249 Loss 4.321 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.801, lat_loss 21.851
09/28 03:27:09 PM | Train: [ 79/180] Step 550/1249 Loss 4.334 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.801, lat_loss 21.851
09/28 03:27:34 PM | Train: [ 79/180] Step 600/1249 Loss 4.325 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.801, lat_loss 21.851
09/28 03:27:59 PM | Train: [ 79/180] Step 650/1249 Loss 4.318 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.801, lat_loss 21.851
09/28 03:28:24 PM | Train: [ 79/180] Step 700/1249 Loss 4.318 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.801, lat_loss 21.851
09/28 03:28:48 PM | Train: [ 79/180] Step 750/1249 Loss 4.283 Prec@(1,3) (83.8%, 99.0%), ce_loss 0.801, lat_loss 21.851
09/28 03:29:07 PM | Train: [ 79/180] Step 800/1249 Loss 4.306 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.801, lat_loss 21.851
09/28 03:29:26 PM | Train: [ 79/180] Step 850/1249 Loss 4.298 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:29:45 PM | Train: [ 79/180] Step 900/1249 Loss 4.328 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:30:05 PM | Train: [ 79/180] Step 950/1249 Loss 4.316 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:30:30 PM | Train: [ 79/180] Step 1000/1249 Loss 4.320 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:30:55 PM | Train: [ 79/180] Step 1050/1249 Loss 4.333 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:31:20 PM | Train: [ 79/180] Step 1100/1249 Loss 4.327 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:31:44 PM | Train: [ 79/180] Step 1150/1249 Loss 4.314 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:32:08 PM | Train: [ 79/180] Step 1200/1249 Loss 4.315 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:32:32 PM | Train: [ 79/180] Step 1249/1249 Loss 4.314 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.800, lat_loss 21.851
09/28 03:32:32 PM | _w_step_train: [ 79/180] Final Prec@1 83.7050% Time 599.19
09/28 03:32:32 PM | Start to train theta for epoch 78
09/28 03:32:53 PM | Train: [ 79/180] Step 050/312 Loss 5.169 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.800, lat_loss 21.851
09/28 03:33:14 PM | Train: [ 79/180] Step 100/312 Loss 4.960 Prec@(1,3) (82.0%, 98.6%), ce_loss 0.799, lat_loss 21.851
09/28 03:33:34 PM | Train: [ 79/180] Step 150/312 Loss 5.036 Prec@(1,3) (81.5%, 98.6%), ce_loss 0.799, lat_loss 21.850
09/28 03:33:55 PM | Train: [ 79/180] Step 200/312 Loss 5.001 Prec@(1,3) (81.9%, 98.6%), ce_loss 0.799, lat_loss 21.850
09/28 03:34:16 PM | Train: [ 79/180] Step 250/312 Loss 4.979 Prec@(1,3) (81.8%, 98.7%), ce_loss 0.799, lat_loss 21.850
09/28 03:34:37 PM | Train: [ 79/180] Step 300/312 Loss 5.073 Prec@(1,3) (81.6%, 98.6%), ce_loss 0.799, lat_loss 21.850
09/28 03:34:42 PM | Train: [ 79/180] Step 312/312 Loss 5.054 Prec@(1,3) (81.7%, 98.6%), ce_loss 0.799, lat_loss 21.850
09/28 03:34:42 PM | _theta_step_train: [ 79/180] Final Prec@1 81.6700% Time 130.06
09/28 03:34:47 PM | Valid: [ 79/180] Step 050/312 Loss 4.986 Prec@(1,3) (81.2%, 98.8%), ce_loss 0.799, lat_loss 21.850
09/28 03:34:52 PM | Valid: [ 79/180] Step 100/312 Loss 5.152 Prec@(1,3) (80.4%, 98.6%), ce_loss 0.799, lat_loss 21.850
09/28 03:34:56 PM | Valid: [ 79/180] Step 150/312 Loss 5.064 Prec@(1,3) (81.1%, 98.6%), ce_loss 0.799, lat_loss 21.850
09/28 03:35:00 PM | Valid: [ 79/180] Step 200/312 Loss 4.961 Prec@(1,3) (81.4%, 98.7%), ce_loss 0.799, lat_loss 21.850
09/28 03:35:04 PM | Valid: [ 79/180] Step 250/312 Loss 4.955 Prec@(1,3) (81.3%, 98.8%), ce_loss 0.799, lat_loss 21.850
09/28 03:35:09 PM | Valid: [ 79/180] Step 300/312 Loss 4.880 Prec@(1,3) (81.6%, 98.8%), ce_loss 0.799, lat_loss 21.850
09/28 03:35:10 PM | Valid: [ 79/180] Step 312/312 Loss 4.869 Prec@(1,3) (81.7%, 98.9%), ce_loss 0.799, lat_loss 21.850
09/28 03:35:10 PM | val: [ 79/180] Final Prec@1 81.6900% Time 27.62
09/28 03:35:10 PM | Start to train weights for epoch 79
09/28 03:35:37 PM | Train: [ 80/180] Step 050/1249 Loss 4.419 Prec@(1,3) (83.8%, 98.7%), ce_loss 0.798, lat_loss 21.850
09/28 03:36:00 PM | Train: [ 80/180] Step 100/1249 Loss 4.157 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.798, lat_loss 21.850
09/28 03:36:25 PM | Train: [ 80/180] Step 150/1249 Loss 4.146 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.798, lat_loss 21.850
09/28 03:36:50 PM | Train: [ 80/180] Step 200/1249 Loss 4.090 Prec@(1,3) (84.5%, 99.2%), ce_loss 0.798, lat_loss 21.850
09/28 03:37:15 PM | Train: [ 80/180] Step 250/1249 Loss 4.185 Prec@(1,3) (84.3%, 99.2%), ce_loss 0.798, lat_loss 21.850
09/28 03:37:40 PM | Train: [ 80/180] Step 300/1249 Loss 4.093 Prec@(1,3) (84.6%, 99.1%), ce_loss 0.798, lat_loss 21.850
09/28 03:38:04 PM | Train: [ 80/180] Step 350/1249 Loss 4.162 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.798, lat_loss 21.850
09/28 03:38:28 PM | Train: [ 80/180] Step 400/1249 Loss 4.158 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.798, lat_loss 21.850
09/28 03:38:51 PM | Train: [ 80/180] Step 450/1249 Loss 4.179 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.798, lat_loss 21.849
09/28 03:39:14 PM | Train: [ 80/180] Step 500/1249 Loss 4.180 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.797, lat_loss 21.849
09/28 03:39:39 PM | Train: [ 80/180] Step 550/1249 Loss 4.222 Prec@(1,3) (84.0%, 99.0%), ce_loss 0.797, lat_loss 21.849
09/28 03:40:04 PM | Train: [ 80/180] Step 600/1249 Loss 4.210 Prec@(1,3) (84.1%, 99.0%), ce_loss 0.797, lat_loss 21.849
09/28 03:40:29 PM | Train: [ 80/180] Step 650/1249 Loss 4.219 Prec@(1,3) (84.0%, 99.1%), ce_loss 0.797, lat_loss 21.849
09/28 03:40:54 PM | Train: [ 80/180] Step 700/1249 Loss 4.237 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.797, lat_loss 21.849
09/28 03:41:18 PM | Train: [ 80/180] Step 750/1249 Loss 4.236 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.797, lat_loss 21.849
09/28 03:41:41 PM | Train: [ 80/180] Step 800/1249 Loss 4.225 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.797, lat_loss 21.849
09/28 03:42:04 PM | Train: [ 80/180] Step 850/1249 Loss 4.211 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.797, lat_loss 21.849
09/28 03:42:27 PM | Train: [ 80/180] Step 900/1249 Loss 4.228 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.797, lat_loss 21.849
09/28 03:42:52 PM | Train: [ 80/180] Step 950/1249 Loss 4.222 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.797, lat_loss 21.849
09/28 03:43:15 PM | Train: [ 80/180] Step 1000/1249 Loss 4.226 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.796, lat_loss 21.849
09/28 03:43:39 PM | Train: [ 80/180] Step 1050/1249 Loss 4.253 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.796, lat_loss 21.849
09/28 03:44:03 PM | Train: [ 80/180] Step 1100/1249 Loss 4.227 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.796, lat_loss 21.849
09/28 03:44:28 PM | Train: [ 80/180] Step 1150/1249 Loss 4.210 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.796, lat_loss 21.849
09/28 03:44:53 PM | Train: [ 80/180] Step 1200/1249 Loss 4.204 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.796, lat_loss 21.849
09/28 03:45:17 PM | Train: [ 80/180] Step 1249/1249 Loss 4.208 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.796, lat_loss 21.849
09/28 03:45:17 PM | _w_step_train: [ 80/180] Final Prec@1 83.8800% Time 607.13
09/28 03:45:17 PM | Start to train theta for epoch 79
09/28 03:45:39 PM | Train: [ 80/180] Step 050/312 Loss 5.043 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.796, lat_loss 21.849
09/28 03:45:59 PM | Train: [ 80/180] Step 100/312 Loss 4.798 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.796, lat_loss 21.848
09/28 03:46:19 PM | Train: [ 80/180] Step 150/312 Loss 4.724 Prec@(1,3) (82.4%, 98.9%), ce_loss 0.796, lat_loss 21.848
09/28 03:46:40 PM | Train: [ 80/180] Step 200/312 Loss 4.877 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.796, lat_loss 21.848
09/28 03:47:01 PM | Train: [ 80/180] Step 250/312 Loss 4.879 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:21 PM | Train: [ 80/180] Step 300/312 Loss 4.844 Prec@(1,3) (82.0%, 98.7%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:26 PM | Train: [ 80/180] Step 312/312 Loss 4.866 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:26 PM | _theta_step_train: [ 80/180] Final Prec@1 81.8600% Time 129.22
09/28 03:47:32 PM | Valid: [ 80/180] Step 050/312 Loss 4.593 Prec@(1,3) (82.1%, 99.1%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:36 PM | Valid: [ 80/180] Step 100/312 Loss 4.835 Prec@(1,3) (81.8%, 98.8%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:41 PM | Valid: [ 80/180] Step 150/312 Loss 4.834 Prec@(1,3) (81.9%, 98.7%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:46 PM | Valid: [ 80/180] Step 200/312 Loss 4.788 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:50 PM | Valid: [ 80/180] Step 250/312 Loss 4.801 Prec@(1,3) (82.0%, 98.6%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:55 PM | Valid: [ 80/180] Step 300/312 Loss 4.790 Prec@(1,3) (82.0%, 98.6%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:56 PM | Valid: [ 80/180] Step 312/312 Loss 4.793 Prec@(1,3) (82.0%, 98.6%), ce_loss 0.795, lat_loss 21.848
09/28 03:47:56 PM | val: [ 80/180] Final Prec@1 82.0400% Time 29.53
09/28 03:47:56 PM | Start to train weights for epoch 80
09/28 03:48:21 PM | Train: [ 81/180] Step 050/1249 Loss 3.809 Prec@(1,3) (85.6%, 99.0%), ce_loss 0.795, lat_loss 21.848
09/28 03:48:46 PM | Train: [ 81/180] Step 100/1249 Loss 3.877 Prec@(1,3) (85.2%, 99.0%), ce_loss 0.795, lat_loss 21.848
09/28 03:49:10 PM | Train: [ 81/180] Step 150/1249 Loss 3.979 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.794, lat_loss 21.848
09/28 03:49:35 PM | Train: [ 81/180] Step 200/1249 Loss 3.976 Prec@(1,3) (84.6%, 99.1%), ce_loss 0.794, lat_loss 21.848
09/28 03:49:59 PM | Train: [ 81/180] Step 250/1249 Loss 3.978 Prec@(1,3) (84.5%, 99.0%), ce_loss 0.794, lat_loss 21.848
09/28 03:50:24 PM | Train: [ 81/180] Step 300/1249 Loss 4.007 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.794, lat_loss 21.848
09/28 03:50:49 PM | Train: [ 81/180] Step 350/1249 Loss 4.007 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.794, lat_loss 21.848
09/28 03:51:13 PM | Train: [ 81/180] Step 400/1249 Loss 4.021 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.794, lat_loss 21.847
09/28 03:51:38 PM | Train: [ 81/180] Step 450/1249 Loss 4.092 Prec@(1,3) (84.4%, 99.0%), ce_loss 0.794, lat_loss 21.847
09/28 03:52:03 PM | Train: [ 81/180] Step 500/1249 Loss 4.120 Prec@(1,3) (84.2%, 99.0%), ce_loss 0.794, lat_loss 21.847
09/28 03:52:27 PM | Train: [ 81/180] Step 550/1249 Loss 4.092 Prec@(1,3) (84.2%, 99.0%), ce_loss 0.794, lat_loss 21.847
09/28 03:52:51 PM | Train: [ 81/180] Step 600/1249 Loss 4.110 Prec@(1,3) (84.2%, 99.0%), ce_loss 0.793, lat_loss 21.847
09/28 03:53:16 PM | Train: [ 81/180] Step 650/1249 Loss 4.114 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.793, lat_loss 21.847
09/28 03:53:41 PM | Train: [ 81/180] Step 700/1249 Loss 4.103 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.793, lat_loss 21.847
09/28 03:54:05 PM | Train: [ 81/180] Step 750/1249 Loss 4.096 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.793, lat_loss 21.847
09/28 03:54:29 PM | Train: [ 81/180] Step 800/1249 Loss 4.134 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.793, lat_loss 21.847
09/28 03:54:54 PM | Train: [ 81/180] Step 850/1249 Loss 4.129 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.793, lat_loss 21.847
09/28 03:55:19 PM | Train: [ 81/180] Step 900/1249 Loss 4.132 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.793, lat_loss 21.847
09/28 03:55:44 PM | Train: [ 81/180] Step 950/1249 Loss 4.132 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.793, lat_loss 21.847
09/28 03:56:08 PM | Train: [ 81/180] Step 1000/1249 Loss 4.135 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.793, lat_loss 21.847
09/28 03:56:33 PM | Train: [ 81/180] Step 1050/1249 Loss 4.119 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.792, lat_loss 21.847
09/28 03:56:58 PM | Train: [ 81/180] Step 1100/1249 Loss 4.148 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.792, lat_loss 21.847
09/28 03:57:23 PM | Train: [ 81/180] Step 1150/1249 Loss 4.147 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.792, lat_loss 21.847
09/28 03:57:47 PM | Train: [ 81/180] Step 1200/1249 Loss 4.162 Prec@(1,3) (84.0%, 99.1%), ce_loss 0.792, lat_loss 21.847
09/28 03:58:12 PM | Train: [ 81/180] Step 1249/1249 Loss 4.175 Prec@(1,3) (84.0%, 99.1%), ce_loss 0.792, lat_loss 21.847
09/28 03:58:12 PM | _w_step_train: [ 81/180] Final Prec@1 83.9575% Time 616.01
09/28 03:58:12 PM | Start to train theta for epoch 80
09/28 03:58:34 PM | Train: [ 81/180] Step 050/312 Loss 4.676 Prec@(1,3) (81.2%, 98.7%), ce_loss 0.792, lat_loss 21.847
09/28 03:58:54 PM | Train: [ 81/180] Step 100/312 Loss 4.727 Prec@(1,3) (80.9%, 98.7%), ce_loss 0.792, lat_loss 21.847
09/28 03:59:15 PM | Train: [ 81/180] Step 150/312 Loss 4.687 Prec@(1,3) (81.5%, 98.8%), ce_loss 0.792, lat_loss 21.846
09/28 03:59:36 PM | Train: [ 81/180] Step 200/312 Loss 4.729 Prec@(1,3) (81.3%, 98.8%), ce_loss 0.792, lat_loss 21.846
09/28 03:59:57 PM | Train: [ 81/180] Step 250/312 Loss 4.750 Prec@(1,3) (81.3%, 98.8%), ce_loss 0.792, lat_loss 21.846
09/28 04:00:17 PM | Train: [ 81/180] Step 300/312 Loss 4.860 Prec@(1,3) (81.1%, 98.6%), ce_loss 0.792, lat_loss 21.846
09/28 04:00:23 PM | Train: [ 81/180] Step 312/312 Loss 4.866 Prec@(1,3) (81.1%, 98.6%), ce_loss 0.792, lat_loss 21.846
09/28 04:00:23 PM | _theta_step_train: [ 81/180] Final Prec@1 81.1200% Time 130.67
09/28 04:00:28 PM | Valid: [ 81/180] Step 050/312 Loss 4.283 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.791, lat_loss 21.846
09/28 04:00:33 PM | Valid: [ 81/180] Step 100/312 Loss 4.743 Prec@(1,3) (82.4%, 98.6%), ce_loss 0.791, lat_loss 21.846
09/28 04:00:37 PM | Valid: [ 81/180] Step 150/312 Loss 4.581 Prec@(1,3) (82.8%, 98.7%), ce_loss 0.791, lat_loss 21.846
09/28 04:00:42 PM | Valid: [ 81/180] Step 200/312 Loss 4.525 Prec@(1,3) (82.8%, 98.8%), ce_loss 0.791, lat_loss 21.846
09/28 04:00:46 PM | Valid: [ 81/180] Step 250/312 Loss 4.776 Prec@(1,3) (82.3%, 98.5%), ce_loss 0.791, lat_loss 21.846
09/28 04:00:51 PM | Valid: [ 81/180] Step 300/312 Loss 4.689 Prec@(1,3) (82.6%, 98.7%), ce_loss 0.791, lat_loss 21.846
09/28 04:00:52 PM | Valid: [ 81/180] Step 312/312 Loss 4.669 Prec@(1,3) (82.6%, 98.7%), ce_loss 0.791, lat_loss 21.846
09/28 04:00:52 PM | val: [ 81/180] Final Prec@1 82.6400% Time 29.69
09/28 04:00:52 PM | Best top1 acc by now. Save model
09/28 04:00:52 PM | Start to train weights for epoch 81
09/28 04:01:18 PM | Train: [ 82/180] Step 050/1249 Loss 4.745 Prec@(1,3) (82.5%, 98.8%), ce_loss 0.791, lat_loss 21.846
09/28 04:01:41 PM | Train: [ 82/180] Step 100/1249 Loss 4.336 Prec@(1,3) (84.1%, 99.0%), ce_loss 0.791, lat_loss 21.846
09/28 04:02:04 PM | Train: [ 82/180] Step 150/1249 Loss 4.150 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.791, lat_loss 21.846
09/28 04:02:27 PM | Train: [ 82/180] Step 200/1249 Loss 4.109 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.791, lat_loss 21.846
09/28 04:02:49 PM | Train: [ 82/180] Step 250/1249 Loss 4.052 Prec@(1,3) (84.7%, 99.2%), ce_loss 0.790, lat_loss 21.846
09/28 04:03:13 PM | Train: [ 82/180] Step 300/1249 Loss 4.003 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.790, lat_loss 21.846
09/28 04:03:35 PM | Train: [ 82/180] Step 350/1249 Loss 4.098 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.790, lat_loss 21.846
09/28 04:03:58 PM | Train: [ 82/180] Step 400/1249 Loss 4.146 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.790, lat_loss 21.846
09/28 04:04:22 PM | Train: [ 82/180] Step 450/1249 Loss 4.136 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.790, lat_loss 21.845
09/28 04:04:46 PM | Train: [ 82/180] Step 500/1249 Loss 4.119 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.790, lat_loss 21.845
09/28 04:05:09 PM | Train: [ 82/180] Step 550/1249 Loss 4.198 Prec@(1,3) (84.3%, 99.0%), ce_loss 0.790, lat_loss 21.845
09/28 04:05:32 PM | Train: [ 82/180] Step 600/1249 Loss 4.184 Prec@(1,3) (84.2%, 99.0%), ce_loss 0.790, lat_loss 21.845
09/28 04:05:56 PM | Train: [ 82/180] Step 650/1249 Loss 4.178 Prec@(1,3) (84.2%, 99.0%), ce_loss 0.790, lat_loss 21.845
09/28 04:06:18 PM | Train: [ 82/180] Step 700/1249 Loss 4.146 Prec@(1,3) (84.4%, 99.0%), ce_loss 0.790, lat_loss 21.845
09/28 04:06:42 PM | Train: [ 82/180] Step 750/1249 Loss 4.149 Prec@(1,3) (84.3%, 99.0%), ce_loss 0.789, lat_loss 21.845
09/28 04:07:04 PM | Train: [ 82/180] Step 800/1249 Loss 4.145 Prec@(1,3) (84.4%, 99.0%), ce_loss 0.789, lat_loss 21.845
09/28 04:07:26 PM | Train: [ 82/180] Step 850/1249 Loss 4.126 Prec@(1,3) (84.4%, 99.0%), ce_loss 0.789, lat_loss 21.845
09/28 04:07:49 PM | Train: [ 82/180] Step 900/1249 Loss 4.100 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.789, lat_loss 21.845
09/28 04:08:12 PM | Train: [ 82/180] Step 950/1249 Loss 4.093 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.789, lat_loss 21.845
09/28 04:08:35 PM | Train: [ 82/180] Step 1000/1249 Loss 4.108 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.789, lat_loss 21.845
09/28 04:08:58 PM | Train: [ 82/180] Step 1050/1249 Loss 4.105 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.789, lat_loss 21.845
09/28 04:09:21 PM | Train: [ 82/180] Step 1100/1249 Loss 4.105 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.789, lat_loss 21.845
09/28 04:09:44 PM | Train: [ 82/180] Step 1150/1249 Loss 4.102 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.789, lat_loss 21.845
09/28 04:10:07 PM | Train: [ 82/180] Step 1200/1249 Loss 4.086 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.788, lat_loss 21.845
09/28 04:10:32 PM | Train: [ 82/180] Step 1249/1249 Loss 4.112 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.788, lat_loss 21.845
09/28 04:10:32 PM | _w_step_train: [ 82/180] Final Prec@1 84.3550% Time 579.27
09/28 04:10:32 PM | Start to train theta for epoch 81
09/28 04:10:53 PM | Train: [ 82/180] Step 050/312 Loss 5.069 Prec@(1,3) (80.2%, 98.3%), ce_loss 0.788, lat_loss 21.845
09/28 04:11:14 PM | Train: [ 82/180] Step 100/312 Loss 4.902 Prec@(1,3) (80.8%, 98.5%), ce_loss 0.788, lat_loss 21.845
09/28 04:11:35 PM | Train: [ 82/180] Step 150/312 Loss 4.953 Prec@(1,3) (81.1%, 98.5%), ce_loss 0.788, lat_loss 21.845
09/28 04:11:56 PM | Train: [ 82/180] Step 200/312 Loss 4.957 Prec@(1,3) (81.2%, 98.5%), ce_loss 0.788, lat_loss 21.844
09/28 04:12:17 PM | Train: [ 82/180] Step 250/312 Loss 4.963 Prec@(1,3) (81.1%, 98.6%), ce_loss 0.788, lat_loss 21.844
09/28 04:12:38 PM | Train: [ 82/180] Step 300/312 Loss 4.947 Prec@(1,3) (81.2%, 98.6%), ce_loss 0.788, lat_loss 21.844
09/28 04:12:43 PM | Train: [ 82/180] Step 312/312 Loss 4.974 Prec@(1,3) (81.1%, 98.6%), ce_loss 0.788, lat_loss 21.844
09/28 04:12:43 PM | _theta_step_train: [ 82/180] Final Prec@1 81.1200% Time 130.99
09/28 04:12:48 PM | Valid: [ 82/180] Step 050/312 Loss 4.812 Prec@(1,3) (81.1%, 98.8%), ce_loss 0.788, lat_loss 21.844
09/28 04:12:52 PM | Valid: [ 82/180] Step 100/312 Loss 4.853 Prec@(1,3) (81.7%, 98.7%), ce_loss 0.788, lat_loss 21.844
09/28 04:12:57 PM | Valid: [ 82/180] Step 150/312 Loss 4.853 Prec@(1,3) (81.8%, 98.5%), ce_loss 0.788, lat_loss 21.844
09/28 04:13:01 PM | Valid: [ 82/180] Step 200/312 Loss 4.852 Prec@(1,3) (81.9%, 98.6%), ce_loss 0.788, lat_loss 21.844
09/28 04:13:05 PM | Valid: [ 82/180] Step 250/312 Loss 4.886 Prec@(1,3) (81.8%, 98.6%), ce_loss 0.788, lat_loss 21.844
09/28 04:13:09 PM | Valid: [ 82/180] Step 300/312 Loss 4.852 Prec@(1,3) (81.8%, 98.6%), ce_loss 0.787, lat_loss 21.844
09/28 04:13:10 PM | Valid: [ 82/180] Step 312/312 Loss 4.951 Prec@(1,3) (81.6%, 98.5%), ce_loss 0.787, lat_loss 21.844
09/28 04:13:10 PM | val: [ 82/180] Final Prec@1 81.5700% Time 27.54
09/28 04:13:10 PM | Start to train weights for epoch 82
09/28 04:13:35 PM | Train: [ 83/180] Step 050/1249 Loss 3.862 Prec@(1,3) (85.0%, 98.8%), ce_loss 0.787, lat_loss 21.844
09/28 04:13:58 PM | Train: [ 83/180] Step 100/1249 Loss 3.979 Prec@(1,3) (85.0%, 99.0%), ce_loss 0.787, lat_loss 21.844
09/28 04:14:22 PM | Train: [ 83/180] Step 150/1249 Loss 4.016 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.787, lat_loss 21.844
09/28 04:14:38 PM | Train: [ 83/180] Step 200/1249 Loss 4.052 Prec@(1,3) (84.3%, 99.2%), ce_loss 0.787, lat_loss 21.844
09/28 04:14:54 PM | Train: [ 83/180] Step 250/1249 Loss 4.039 Prec@(1,3) (84.4%, 99.2%), ce_loss 0.787, lat_loss 21.844
09/28 04:15:11 PM | Train: [ 83/180] Step 300/1249 Loss 4.091 Prec@(1,3) (84.3%, 99.2%), ce_loss 0.787, lat_loss 21.844
09/28 04:15:27 PM | Train: [ 83/180] Step 350/1249 Loss 4.048 Prec@(1,3) (84.3%, 99.2%), ce_loss 0.787, lat_loss 21.844
09/28 04:15:44 PM | Train: [ 83/180] Step 400/1249 Loss 4.000 Prec@(1,3) (84.5%, 99.3%), ce_loss 0.787, lat_loss 21.844
09/28 04:16:00 PM | Train: [ 83/180] Step 450/1249 Loss 3.996 Prec@(1,3) (84.7%, 99.2%), ce_loss 0.786, lat_loss 21.844
09/28 04:16:16 PM | Train: [ 83/180] Step 500/1249 Loss 4.028 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.786, lat_loss 21.844
09/28 04:16:32 PM | Train: [ 83/180] Step 550/1249 Loss 4.021 Prec@(1,3) (84.7%, 99.2%), ce_loss 0.786, lat_loss 21.844
09/28 04:16:48 PM | Train: [ 83/180] Step 600/1249 Loss 4.024 Prec@(1,3) (84.7%, 99.2%), ce_loss 0.786, lat_loss 21.843
09/28 04:17:04 PM | Train: [ 83/180] Step 650/1249 Loss 4.028 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.786, lat_loss 21.843
09/28 04:17:20 PM | Train: [ 83/180] Step 700/1249 Loss 4.018 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.786, lat_loss 21.843
09/28 04:17:36 PM | Train: [ 83/180] Step 750/1249 Loss 4.053 Prec@(1,3) (84.5%, 99.2%), ce_loss 0.786, lat_loss 21.843
09/28 04:17:52 PM | Train: [ 83/180] Step 800/1249 Loss 4.075 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.786, lat_loss 21.843
09/28 04:18:13 PM | Train: [ 83/180] Step 850/1249 Loss 4.095 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.786, lat_loss 21.843
09/28 04:18:38 PM | Train: [ 83/180] Step 900/1249 Loss 4.095 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.786, lat_loss 21.843
09/28 04:19:03 PM | Train: [ 83/180] Step 950/1249 Loss 4.120 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.785, lat_loss 21.843
09/28 04:19:27 PM | Train: [ 83/180] Step 1000/1249 Loss 4.108 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.785, lat_loss 21.843
09/28 04:19:50 PM | Train: [ 83/180] Step 1050/1249 Loss 4.110 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.785, lat_loss 21.843
09/28 04:20:14 PM | Train: [ 83/180] Step 1100/1249 Loss 4.114 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.785, lat_loss 21.843
09/28 04:20:36 PM | Train: [ 83/180] Step 1150/1249 Loss 4.118 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.785, lat_loss 21.843
09/28 04:20:56 PM | Train: [ 83/180] Step 1200/1249 Loss 4.107 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.785, lat_loss 21.843
09/28 04:21:12 PM | Train: [ 83/180] Step 1249/1249 Loss 4.112 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.785, lat_loss 21.843
09/28 04:21:12 PM | _w_step_train: [ 83/180] Final Prec@1 84.2025% Time 481.98
09/28 04:21:12 PM | Start to train theta for epoch 82
09/28 04:21:34 PM | Train: [ 83/180] Step 050/312 Loss 4.732 Prec@(1,3) (82.4%, 98.8%), ce_loss 0.785, lat_loss 21.843
09/28 04:21:55 PM | Train: [ 83/180] Step 100/312 Loss 4.816 Prec@(1,3) (81.7%, 98.9%), ce_loss 0.785, lat_loss 21.843
09/28 04:22:15 PM | Train: [ 83/180] Step 150/312 Loss 4.604 Prec@(1,3) (82.5%, 98.8%), ce_loss 0.785, lat_loss 21.843
09/28 04:22:36 PM | Train: [ 83/180] Step 200/312 Loss 4.623 Prec@(1,3) (82.4%, 98.8%), ce_loss 0.784, lat_loss 21.843
09/28 04:22:57 PM | Train: [ 83/180] Step 250/312 Loss 4.623 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.784, lat_loss 21.843
09/28 04:23:18 PM | Train: [ 83/180] Step 300/312 Loss 4.584 Prec@(1,3) (82.6%, 98.9%), ce_loss 0.784, lat_loss 21.843
09/28 04:23:23 PM | Train: [ 83/180] Step 312/312 Loss 4.594 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.784, lat_loss 21.843
09/28 04:23:23 PM | _theta_step_train: [ 83/180] Final Prec@1 82.5100% Time 130.81
09/28 04:23:29 PM | Valid: [ 83/180] Step 050/312 Loss 4.299 Prec@(1,3) (83.9%, 99.3%), ce_loss 0.784, lat_loss 21.842
09/28 04:23:33 PM | Valid: [ 83/180] Step 100/312 Loss 4.788 Prec@(1,3) (82.4%, 99.0%), ce_loss 0.784, lat_loss 21.842
09/28 04:23:38 PM | Valid: [ 83/180] Step 150/312 Loss 4.770 Prec@(1,3) (82.4%, 98.9%), ce_loss 0.784, lat_loss 21.842
09/28 04:23:42 PM | Valid: [ 83/180] Step 200/312 Loss 4.690 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.784, lat_loss 21.842
09/28 04:23:47 PM | Valid: [ 83/180] Step 250/312 Loss 4.703 Prec@(1,3) (82.7%, 98.9%), ce_loss 0.784, lat_loss 21.842
09/28 04:23:52 PM | Valid: [ 83/180] Step 300/312 Loss 4.821 Prec@(1,3) (82.3%, 98.8%), ce_loss 0.784, lat_loss 21.842
09/28 04:23:54 PM | Valid: [ 83/180] Step 312/312 Loss 4.806 Prec@(1,3) (82.3%, 98.8%), ce_loss 0.784, lat_loss 21.842
09/28 04:23:54 PM | val: [ 83/180] Final Prec@1 82.2800% Time 30.74
09/28 04:23:54 PM | Start to train weights for epoch 83
09/28 04:24:20 PM | Train: [ 84/180] Step 050/1249 Loss 4.088 Prec@(1,3) (85.0%, 98.9%), ce_loss 0.784, lat_loss 21.842
09/28 04:24:43 PM | Train: [ 84/180] Step 100/1249 Loss 4.315 Prec@(1,3) (83.7%, 98.8%), ce_loss 0.784, lat_loss 21.842
09/28 04:25:06 PM | Train: [ 84/180] Step 150/1249 Loss 4.135 Prec@(1,3) (84.5%, 98.9%), ce_loss 0.783, lat_loss 21.842
09/28 04:25:30 PM | Train: [ 84/180] Step 200/1249 Loss 4.150 Prec@(1,3) (84.4%, 99.0%), ce_loss 0.783, lat_loss 21.842
09/28 04:25:53 PM | Train: [ 84/180] Step 250/1249 Loss 4.032 Prec@(1,3) (84.7%, 99.0%), ce_loss 0.783, lat_loss 21.842
09/28 04:26:18 PM | Train: [ 84/180] Step 300/1249 Loss 4.016 Prec@(1,3) (84.9%, 99.0%), ce_loss 0.783, lat_loss 21.842
09/28 04:26:42 PM | Train: [ 84/180] Step 350/1249 Loss 4.031 Prec@(1,3) (84.7%, 99.0%), ce_loss 0.783, lat_loss 21.842
09/28 04:27:05 PM | Train: [ 84/180] Step 400/1249 Loss 4.018 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.783, lat_loss 21.842
09/28 04:27:29 PM | Train: [ 84/180] Step 450/1249 Loss 4.030 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.783, lat_loss 21.842
09/28 04:27:55 PM | Train: [ 84/180] Step 500/1249 Loss 4.030 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.783, lat_loss 21.842
09/28 04:28:20 PM | Train: [ 84/180] Step 550/1249 Loss 4.032 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.783, lat_loss 21.842
09/28 04:28:45 PM | Train: [ 84/180] Step 600/1249 Loss 4.024 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.783, lat_loss 21.842
09/28 04:29:11 PM | Train: [ 84/180] Step 650/1249 Loss 4.000 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.782, lat_loss 21.842
09/28 04:29:36 PM | Train: [ 84/180] Step 700/1249 Loss 3.955 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.782, lat_loss 21.842
09/28 04:30:01 PM | Train: [ 84/180] Step 750/1249 Loss 3.952 Prec@(1,3) (84.7%, 99.2%), ce_loss 0.782, lat_loss 21.842
09/28 04:30:26 PM | Train: [ 84/180] Step 800/1249 Loss 3.976 Prec@(1,3) (84.6%, 99.1%), ce_loss 0.782, lat_loss 21.842
09/28 04:30:51 PM | Train: [ 84/180] Step 850/1249 Loss 3.987 Prec@(1,3) (84.6%, 99.1%), ce_loss 0.782, lat_loss 21.841
09/28 04:31:16 PM | Train: [ 84/180] Step 900/1249 Loss 3.968 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.782, lat_loss 21.841
09/28 04:31:41 PM | Train: [ 84/180] Step 950/1249 Loss 3.977 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.782, lat_loss 21.841
09/28 04:32:07 PM | Train: [ 84/180] Step 1000/1249 Loss 3.966 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.782, lat_loss 21.841
09/28 04:32:32 PM | Train: [ 84/180] Step 1050/1249 Loss 3.989 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.782, lat_loss 21.841
09/28 04:32:57 PM | Train: [ 84/180] Step 1100/1249 Loss 3.988 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.781, lat_loss 21.841
09/28 04:33:22 PM | Train: [ 84/180] Step 1150/1249 Loss 4.001 Prec@(1,3) (84.6%, 99.1%), ce_loss 0.781, lat_loss 21.841
09/28 04:33:44 PM | Train: [ 84/180] Step 1200/1249 Loss 4.011 Prec@(1,3) (84.6%, 99.1%), ce_loss 0.781, lat_loss 21.841
09/28 04:33:58 PM | Train: [ 84/180] Step 1249/1249 Loss 4.006 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.781, lat_loss 21.841
09/28 04:33:58 PM | _w_step_train: [ 84/180] Final Prec@1 84.5725% Time 604.47
09/28 04:33:58 PM | Start to train theta for epoch 83
09/28 04:34:20 PM | Train: [ 84/180] Step 050/312 Loss 4.329 Prec@(1,3) (83.2%, 98.8%), ce_loss 0.781, lat_loss 21.841
09/28 04:34:41 PM | Train: [ 84/180] Step 100/312 Loss 4.525 Prec@(1,3) (82.5%, 98.8%), ce_loss 0.781, lat_loss 21.841
09/28 04:35:02 PM | Train: [ 84/180] Step 150/312 Loss 4.580 Prec@(1,3) (82.8%, 98.9%), ce_loss 0.781, lat_loss 21.841
09/28 04:35:23 PM | Train: [ 84/180] Step 200/312 Loss 4.606 Prec@(1,3) (82.8%, 98.8%), ce_loss 0.781, lat_loss 21.841
09/28 04:35:43 PM | Train: [ 84/180] Step 250/312 Loss 4.620 Prec@(1,3) (82.8%, 98.8%), ce_loss 0.781, lat_loss 21.841
09/28 04:36:04 PM | Train: [ 84/180] Step 300/312 Loss 4.544 Prec@(1,3) (83.0%, 98.8%), ce_loss 0.781, lat_loss 21.841
09/28 04:36:09 PM | Train: [ 84/180] Step 312/312 Loss 4.578 Prec@(1,3) (82.9%, 98.8%), ce_loss 0.781, lat_loss 21.841
09/28 04:36:10 PM | _theta_step_train: [ 84/180] Final Prec@1 82.9100% Time 131.55
09/28 04:36:15 PM | Valid: [ 84/180] Step 050/312 Loss 4.161 Prec@(1,3) (84.2%, 99.4%), ce_loss 0.780, lat_loss 21.841
09/28 04:36:20 PM | Valid: [ 84/180] Step 100/312 Loss 4.610 Prec@(1,3) (82.8%, 98.6%), ce_loss 0.780, lat_loss 21.841
09/28 04:36:25 PM | Valid: [ 84/180] Step 150/312 Loss 4.507 Prec@(1,3) (83.2%, 98.7%), ce_loss 0.780, lat_loss 21.841
09/28 04:36:29 PM | Valid: [ 84/180] Step 200/312 Loss 4.503 Prec@(1,3) (83.3%, 98.8%), ce_loss 0.780, lat_loss 21.841
09/28 04:36:33 PM | Valid: [ 84/180] Step 250/312 Loss 4.469 Prec@(1,3) (83.4%, 98.9%), ce_loss 0.780, lat_loss 21.841
09/28 04:36:38 PM | Valid: [ 84/180] Step 300/312 Loss 4.404 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.780, lat_loss 21.841
09/28 04:36:39 PM | Valid: [ 84/180] Step 312/312 Loss 4.399 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.780, lat_loss 21.841
09/28 04:36:39 PM | val: [ 84/180] Final Prec@1 83.5700% Time 29.04
09/28 04:36:39 PM | Best top1 acc by now. Save model
09/28 04:36:39 PM | Start to train weights for epoch 84
09/28 04:37:05 PM | Train: [ 85/180] Step 050/1249 Loss 3.809 Prec@(1,3) (85.1%, 99.4%), ce_loss 0.780, lat_loss 21.841
09/28 04:37:30 PM | Train: [ 85/180] Step 100/1249 Loss 3.768 Prec@(1,3) (85.7%, 99.1%), ce_loss 0.780, lat_loss 21.840
09/28 04:37:53 PM | Train: [ 85/180] Step 150/1249 Loss 3.720 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.780, lat_loss 21.840
09/28 04:38:18 PM | Train: [ 85/180] Step 200/1249 Loss 3.731 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.780, lat_loss 21.840
09/28 04:38:43 PM | Train: [ 85/180] Step 250/1249 Loss 3.820 Prec@(1,3) (85.6%, 99.1%), ce_loss 0.779, lat_loss 21.840
09/28 04:39:08 PM | Train: [ 85/180] Step 300/1249 Loss 3.843 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.779, lat_loss 21.840
09/28 04:39:33 PM | Train: [ 85/180] Step 350/1249 Loss 3.825 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.779, lat_loss 21.840
09/28 04:39:57 PM | Train: [ 85/180] Step 400/1249 Loss 3.805 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.779, lat_loss 21.840
09/28 04:40:22 PM | Train: [ 85/180] Step 450/1249 Loss 3.815 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.779, lat_loss 21.840
09/28 04:40:47 PM | Train: [ 85/180] Step 500/1249 Loss 3.857 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.779, lat_loss 21.840
09/28 04:41:12 PM | Train: [ 85/180] Step 550/1249 Loss 3.901 Prec@(1,3) (85.2%, 99.1%), ce_loss 0.779, lat_loss 21.840
09/28 04:41:37 PM | Train: [ 85/180] Step 600/1249 Loss 3.899 Prec@(1,3) (85.2%, 99.1%), ce_loss 0.779, lat_loss 21.840
09/28 04:42:02 PM | Train: [ 85/180] Step 650/1249 Loss 3.921 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.779, lat_loss 21.840
09/28 04:42:27 PM | Train: [ 85/180] Step 700/1249 Loss 3.927 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.779, lat_loss 21.840
09/28 04:42:52 PM | Train: [ 85/180] Step 750/1249 Loss 3.916 Prec@(1,3) (85.2%, 99.1%), ce_loss 0.778, lat_loss 21.840
09/28 04:43:17 PM | Train: [ 85/180] Step 800/1249 Loss 3.926 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.778, lat_loss 21.840
09/28 04:43:42 PM | Train: [ 85/180] Step 850/1249 Loss 3.963 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.778, lat_loss 21.840
09/28 04:44:07 PM | Train: [ 85/180] Step 900/1249 Loss 3.976 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.778, lat_loss 21.840
09/28 04:44:32 PM | Train: [ 85/180] Step 950/1249 Loss 4.000 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.778, lat_loss 21.840
09/28 04:44:54 PM | Train: [ 85/180] Step 1000/1249 Loss 4.058 Prec@(1,3) (84.8%, 99.0%), ce_loss 0.778, lat_loss 21.840
09/28 04:45:19 PM | Train: [ 85/180] Step 1050/1249 Loss 4.039 Prec@(1,3) (84.9%, 99.0%), ce_loss 0.778, lat_loss 21.840
09/28 04:45:44 PM | Train: [ 85/180] Step 1100/1249 Loss 4.042 Prec@(1,3) (84.8%, 99.0%), ce_loss 0.778, lat_loss 21.840
09/28 04:46:09 PM | Train: [ 85/180] Step 1150/1249 Loss 4.023 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.778, lat_loss 21.840
09/28 04:46:30 PM | Train: [ 85/180] Step 1200/1249 Loss 4.009 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.778, lat_loss 21.840
09/28 04:46:52 PM | Train: [ 85/180] Step 1249/1249 Loss 4.012 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.777, lat_loss 21.839
09/28 04:46:53 PM | _w_step_train: [ 85/180] Final Prec@1 84.9450% Time 613.35
09/28 04:46:53 PM | Start to train theta for epoch 84
09/28 04:47:14 PM | Train: [ 85/180] Step 050/312 Loss 4.887 Prec@(1,3) (82.1%, 99.0%), ce_loss 0.777, lat_loss 21.839
09/28 04:47:35 PM | Train: [ 85/180] Step 100/312 Loss 4.868 Prec@(1,3) (82.3%, 98.8%), ce_loss 0.777, lat_loss 21.839
09/28 04:47:57 PM | Train: [ 85/180] Step 150/312 Loss 4.845 Prec@(1,3) (82.0%, 98.9%), ce_loss 0.777, lat_loss 21.839
09/28 04:48:18 PM | Train: [ 85/180] Step 200/312 Loss 4.855 Prec@(1,3) (82.1%, 99.0%), ce_loss 0.777, lat_loss 21.839
09/28 04:48:38 PM | Train: [ 85/180] Step 250/312 Loss 4.848 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.777, lat_loss 21.839
09/28 04:48:58 PM | Train: [ 85/180] Step 300/312 Loss 4.808 Prec@(1,3) (82.2%, 98.8%), ce_loss 0.777, lat_loss 21.839
09/28 04:49:01 PM | Train: [ 85/180] Step 312/312 Loss 4.832 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.777, lat_loss 21.839
09/28 04:49:01 PM | _theta_step_train: [ 85/180] Final Prec@1 82.0600% Time 128.64
09/28 04:49:07 PM | Valid: [ 85/180] Step 050/312 Loss 4.170 Prec@(1,3) (83.4%, 99.5%), ce_loss 0.777, lat_loss 21.839
09/28 04:49:11 PM | Valid: [ 85/180] Step 100/312 Loss 4.410 Prec@(1,3) (83.0%, 99.3%), ce_loss 0.777, lat_loss 21.839
09/28 04:49:16 PM | Valid: [ 85/180] Step 150/312 Loss 4.369 Prec@(1,3) (83.3%, 99.2%), ce_loss 0.777, lat_loss 21.839
09/28 04:49:20 PM | Valid: [ 85/180] Step 200/312 Loss 4.411 Prec@(1,3) (83.0%, 99.1%), ce_loss 0.777, lat_loss 21.839
09/28 04:49:25 PM | Valid: [ 85/180] Step 250/312 Loss 4.487 Prec@(1,3) (82.9%, 99.0%), ce_loss 0.777, lat_loss 21.839
09/28 04:49:30 PM | Valid: [ 85/180] Step 300/312 Loss 4.496 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.776, lat_loss 21.839
09/28 04:49:31 PM | Valid: [ 85/180] Step 312/312 Loss 4.513 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.776, lat_loss 21.839
09/28 04:49:32 PM | val: [ 85/180] Final Prec@1 83.0000% Time 30.30
09/28 04:49:32 PM | Start to train weights for epoch 85
09/28 04:49:58 PM | Train: [ 86/180] Step 050/1249 Loss 3.749 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.776, lat_loss 21.839
09/28 04:50:23 PM | Train: [ 86/180] Step 100/1249 Loss 3.765 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.776, lat_loss 21.839
09/28 04:50:47 PM | Train: [ 86/180] Step 150/1249 Loss 3.897 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.776, lat_loss 21.839
09/28 04:51:12 PM | Train: [ 86/180] Step 200/1249 Loss 3.801 Prec@(1,3) (85.7%, 99.3%), ce_loss 0.776, lat_loss 21.839
09/28 04:51:37 PM | Train: [ 86/180] Step 250/1249 Loss 3.780 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.776, lat_loss 21.839
09/28 04:52:02 PM | Train: [ 86/180] Step 300/1249 Loss 3.740 Prec@(1,3) (85.8%, 99.2%), ce_loss 0.776, lat_loss 21.839
09/28 04:52:27 PM | Train: [ 86/180] Step 350/1249 Loss 3.718 Prec@(1,3) (85.8%, 99.2%), ce_loss 0.776, lat_loss 21.839
09/28 04:52:51 PM | Train: [ 86/180] Step 400/1249 Loss 3.729 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.776, lat_loss 21.839
09/28 04:53:16 PM | Train: [ 86/180] Step 450/1249 Loss 3.776 Prec@(1,3) (85.6%, 99.3%), ce_loss 0.775, lat_loss 21.839
09/28 04:53:40 PM | Train: [ 86/180] Step 500/1249 Loss 3.808 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.775, lat_loss 21.838
09/28 04:54:04 PM | Train: [ 86/180] Step 550/1249 Loss 3.888 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.775, lat_loss 21.838
09/28 04:54:28 PM | Train: [ 86/180] Step 600/1249 Loss 3.920 Prec@(1,3) (85.2%, 99.2%), ce_loss 0.775, lat_loss 21.838
09/28 04:54:53 PM | Train: [ 86/180] Step 650/1249 Loss 3.962 Prec@(1,3) (85.0%, 99.2%), ce_loss 0.775, lat_loss 21.838
09/28 04:55:19 PM | Train: [ 86/180] Step 700/1249 Loss 3.995 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.775, lat_loss 21.838
09/28 04:55:43 PM | Train: [ 86/180] Step 750/1249 Loss 3.976 Prec@(1,3) (85.0%, 99.2%), ce_loss 0.775, lat_loss 21.838
09/28 04:56:07 PM | Train: [ 86/180] Step 800/1249 Loss 3.984 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.775, lat_loss 21.838
09/28 04:56:32 PM | Train: [ 86/180] Step 850/1249 Loss 3.984 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.775, lat_loss 21.838
09/28 04:56:57 PM | Train: [ 86/180] Step 900/1249 Loss 3.962 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.775, lat_loss 21.838
09/28 04:57:22 PM | Train: [ 86/180] Step 950/1249 Loss 3.949 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.774, lat_loss 21.838
09/28 04:57:46 PM | Train: [ 86/180] Step 1000/1249 Loss 3.965 Prec@(1,3) (84.8%, 99.2%), ce_loss 0.774, lat_loss 21.838
09/28 04:58:12 PM | Train: [ 86/180] Step 1050/1249 Loss 3.960 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.774, lat_loss 21.838
09/28 04:58:36 PM | Train: [ 86/180] Step 1100/1249 Loss 3.956 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.774, lat_loss 21.838
09/28 04:59:01 PM | Train: [ 86/180] Step 1150/1249 Loss 3.956 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.774, lat_loss 21.838
09/28 04:59:26 PM | Train: [ 86/180] Step 1200/1249 Loss 3.952 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.774, lat_loss 21.838
09/28 04:59:50 PM | Train: [ 86/180] Step 1249/1249 Loss 3.943 Prec@(1,3) (85.0%, 99.2%), ce_loss 0.774, lat_loss 21.838
09/28 04:59:50 PM | _w_step_train: [ 86/180] Final Prec@1 84.9625% Time 618.67
09/28 04:59:50 PM | Start to train theta for epoch 85
09/28 05:00:11 PM | Train: [ 86/180] Step 050/312 Loss 4.202 Prec@(1,3) (83.8%, 99.3%), ce_loss 0.774, lat_loss 21.838
09/28 05:00:32 PM | Train: [ 86/180] Step 100/312 Loss 4.575 Prec@(1,3) (82.8%, 98.9%), ce_loss 0.774, lat_loss 21.838
09/28 05:00:52 PM | Train: [ 86/180] Step 150/312 Loss 4.553 Prec@(1,3) (82.9%, 99.0%), ce_loss 0.774, lat_loss 21.838
09/28 05:01:13 PM | Train: [ 86/180] Step 200/312 Loss 4.512 Prec@(1,3) (83.2%, 99.0%), ce_loss 0.773, lat_loss 21.838
09/28 05:01:32 PM | Train: [ 86/180] Step 250/312 Loss 4.586 Prec@(1,3) (82.9%, 99.0%), ce_loss 0.773, lat_loss 21.838
09/28 05:01:52 PM | Train: [ 86/180] Step 300/312 Loss 4.563 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.773, lat_loss 21.838
09/28 05:01:57 PM | Train: [ 86/180] Step 312/312 Loss 4.549 Prec@(1,3) (83.2%, 99.0%), ce_loss 0.773, lat_loss 21.838
09/28 05:01:57 PM | _theta_step_train: [ 86/180] Final Prec@1 83.1500% Time 127.02
09/28 05:02:03 PM | Valid: [ 86/180] Step 050/312 Loss 4.039 Prec@(1,3) (84.6%, 99.3%), ce_loss 0.773, lat_loss 21.837
09/28 05:02:07 PM | Valid: [ 86/180] Step 100/312 Loss 4.232 Prec@(1,3) (84.0%, 99.0%), ce_loss 0.773, lat_loss 21.837
09/28 05:02:12 PM | Valid: [ 86/180] Step 150/312 Loss 4.351 Prec@(1,3) (83.5%, 98.8%), ce_loss 0.773, lat_loss 21.837
09/28 05:02:16 PM | Valid: [ 86/180] Step 200/312 Loss 4.394 Prec@(1,3) (83.5%, 98.9%), ce_loss 0.773, lat_loss 21.837
09/28 05:02:21 PM | Valid: [ 86/180] Step 250/312 Loss 4.338 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.773, lat_loss 21.837
09/28 05:02:25 PM | Valid: [ 86/180] Step 300/312 Loss 4.251 Prec@(1,3) (84.0%, 99.0%), ce_loss 0.773, lat_loss 21.837
09/28 05:02:27 PM | Valid: [ 86/180] Step 312/312 Loss 4.242 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.773, lat_loss 21.837
09/28 05:02:27 PM | val: [ 86/180] Final Prec@1 83.9200% Time 29.41
09/28 05:02:27 PM | Best top1 acc by now. Save model
09/28 05:02:27 PM | Start to train weights for epoch 86
09/28 05:02:52 PM | Train: [ 87/180] Step 050/1249 Loss 4.100 Prec@(1,3) (85.3%, 99.1%), ce_loss 0.773, lat_loss 21.837
09/28 05:03:17 PM | Train: [ 87/180] Step 100/1249 Loss 3.785 Prec@(1,3) (86.2%, 99.2%), ce_loss 0.773, lat_loss 21.837
09/28 05:03:42 PM | Train: [ 87/180] Step 150/1249 Loss 3.670 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.772, lat_loss 21.837
09/28 05:04:07 PM | Train: [ 87/180] Step 200/1249 Loss 3.672 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.772, lat_loss 21.837
09/28 05:04:32 PM | Train: [ 87/180] Step 250/1249 Loss 3.814 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.772, lat_loss 21.837
09/28 05:04:57 PM | Train: [ 87/180] Step 300/1249 Loss 3.835 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.772, lat_loss 21.837
09/28 05:05:22 PM | Train: [ 87/180] Step 350/1249 Loss 3.808 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.772, lat_loss 21.837
09/28 05:05:47 PM | Train: [ 87/180] Step 400/1249 Loss 3.781 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.772, lat_loss 21.837
09/28 05:06:11 PM | Train: [ 87/180] Step 450/1249 Loss 3.763 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.772, lat_loss 21.837
09/28 05:06:36 PM | Train: [ 87/180] Step 500/1249 Loss 3.799 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.772, lat_loss 21.837
09/28 05:07:02 PM | Train: [ 87/180] Step 550/1249 Loss 3.800 Prec@(1,3) (85.7%, 99.3%), ce_loss 0.772, lat_loss 21.837
09/28 05:07:26 PM | Train: [ 87/180] Step 600/1249 Loss 3.846 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.771, lat_loss 21.837
09/28 05:07:52 PM | Train: [ 87/180] Step 650/1249 Loss 3.849 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.771, lat_loss 21.837
09/28 05:08:17 PM | Train: [ 87/180] Step 700/1249 Loss 3.846 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.771, lat_loss 21.837
09/28 05:08:42 PM | Train: [ 87/180] Step 750/1249 Loss 3.848 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.771, lat_loss 21.837
09/28 05:09:07 PM | Train: [ 87/180] Step 800/1249 Loss 3.848 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.771, lat_loss 21.837
09/28 05:09:31 PM | Train: [ 87/180] Step 850/1249 Loss 3.836 Prec@(1,3) (85.5%, 99.3%), ce_loss 0.771, lat_loss 21.836
09/28 05:09:56 PM | Train: [ 87/180] Step 900/1249 Loss 3.836 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.771, lat_loss 21.836
09/28 05:10:20 PM | Train: [ 87/180] Step 950/1249 Loss 3.860 Prec@(1,3) (85.3%, 99.3%), ce_loss 0.771, lat_loss 21.836
09/28 05:10:45 PM | Train: [ 87/180] Step 1000/1249 Loss 3.845 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.771, lat_loss 21.836
09/28 05:11:10 PM | Train: [ 87/180] Step 1050/1249 Loss 3.845 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.771, lat_loss 21.836
09/28 05:11:34 PM | Train: [ 87/180] Step 1100/1249 Loss 3.839 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.770, lat_loss 21.836
09/28 05:11:58 PM | Train: [ 87/180] Step 1150/1249 Loss 3.861 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.770, lat_loss 21.836
09/28 05:12:20 PM | Train: [ 87/180] Step 1200/1249 Loss 3.872 Prec@(1,3) (85.3%, 99.3%), ce_loss 0.770, lat_loss 21.836
09/28 05:12:43 PM | Train: [ 87/180] Step 1249/1249 Loss 3.878 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.770, lat_loss 21.836
09/28 05:12:43 PM | _w_step_train: [ 87/180] Final Prec@1 85.3425% Time 615.98
09/28 05:12:43 PM | Start to train theta for epoch 86
09/28 05:13:05 PM | Train: [ 87/180] Step 050/312 Loss 4.526 Prec@(1,3) (82.0%, 98.8%), ce_loss 0.770, lat_loss 21.836
09/28 05:13:25 PM | Train: [ 87/180] Step 100/312 Loss 4.593 Prec@(1,3) (82.1%, 98.9%), ce_loss 0.770, lat_loss 21.836
09/28 05:13:45 PM | Train: [ 87/180] Step 150/312 Loss 4.591 Prec@(1,3) (82.5%, 98.8%), ce_loss 0.770, lat_loss 21.836
09/28 05:14:05 PM | Train: [ 87/180] Step 200/312 Loss 4.624 Prec@(1,3) (82.2%, 98.8%), ce_loss 0.770, lat_loss 21.836
09/28 05:14:25 PM | Train: [ 87/180] Step 250/312 Loss 4.594 Prec@(1,3) (82.4%, 98.9%), ce_loss 0.770, lat_loss 21.836
09/28 05:14:45 PM | Train: [ 87/180] Step 300/312 Loss 4.584 Prec@(1,3) (82.5%, 98.8%), ce_loss 0.770, lat_loss 21.836
09/28 05:14:50 PM | Train: [ 87/180] Step 312/312 Loss 4.570 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.770, lat_loss 21.836
09/28 05:14:50 PM | _theta_step_train: [ 87/180] Final Prec@1 82.5500% Time 127.57
09/28 05:14:56 PM | Valid: [ 87/180] Step 050/312 Loss 3.911 Prec@(1,3) (84.6%, 99.3%), ce_loss 0.770, lat_loss 21.836
09/28 05:15:00 PM | Valid: [ 87/180] Step 100/312 Loss 4.191 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.769, lat_loss 21.836
09/28 05:15:05 PM | Valid: [ 87/180] Step 150/312 Loss 4.297 Prec@(1,3) (84.1%, 98.9%), ce_loss 0.769, lat_loss 21.836
09/28 05:15:09 PM | Valid: [ 87/180] Step 200/312 Loss 4.375 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.769, lat_loss 21.836
09/28 05:15:14 PM | Valid: [ 87/180] Step 250/312 Loss 4.364 Prec@(1,3) (83.5%, 99.1%), ce_loss 0.769, lat_loss 21.836
09/28 05:15:19 PM | Valid: [ 87/180] Step 300/312 Loss 4.487 Prec@(1,3) (83.2%, 98.9%), ce_loss 0.769, lat_loss 21.836
09/28 05:15:20 PM | Valid: [ 87/180] Step 312/312 Loss 4.466 Prec@(1,3) (83.2%, 98.9%), ce_loss 0.769, lat_loss 21.836
09/28 05:15:20 PM | val: [ 87/180] Final Prec@1 83.2200% Time 29.66
09/28 05:15:20 PM | Start to train weights for epoch 87
09/28 05:15:44 PM | Train: [ 88/180] Step 050/1249 Loss 4.087 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.769, lat_loss 21.836
09/28 05:16:08 PM | Train: [ 88/180] Step 100/1249 Loss 3.889 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.769, lat_loss 21.836
09/28 05:16:33 PM | Train: [ 88/180] Step 150/1249 Loss 3.742 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.769, lat_loss 21.835
09/28 05:16:56 PM | Train: [ 88/180] Step 200/1249 Loss 3.681 Prec@(1,3) (85.8%, 99.2%), ce_loss 0.769, lat_loss 21.835
09/28 05:17:20 PM | Train: [ 88/180] Step 250/1249 Loss 3.805 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.769, lat_loss 21.835
09/28 05:17:41 PM | Train: [ 88/180] Step 300/1249 Loss 3.810 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.769, lat_loss 21.835
09/28 05:18:02 PM | Train: [ 88/180] Step 350/1249 Loss 3.844 Prec@(1,3) (85.5%, 99.1%), ce_loss 0.768, lat_loss 21.835
09/28 05:18:25 PM | Train: [ 88/180] Step 400/1249 Loss 3.815 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.768, lat_loss 21.835
09/28 05:18:49 PM | Train: [ 88/180] Step 450/1249 Loss 3.818 Prec@(1,3) (85.5%, 99.1%), ce_loss 0.768, lat_loss 21.835
09/28 05:19:13 PM | Train: [ 88/180] Step 500/1249 Loss 3.827 Prec@(1,3) (85.5%, 99.1%), ce_loss 0.768, lat_loss 21.835
09/28 05:19:35 PM | Train: [ 88/180] Step 550/1249 Loss 3.791 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.768, lat_loss 21.835
09/28 05:19:59 PM | Train: [ 88/180] Step 600/1249 Loss 3.793 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.768, lat_loss 21.835
09/28 05:20:25 PM | Train: [ 88/180] Step 650/1249 Loss 3.780 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.768, lat_loss 21.835
09/28 05:20:50 PM | Train: [ 88/180] Step 700/1249 Loss 3.771 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.768, lat_loss 21.835
09/28 05:21:14 PM | Train: [ 88/180] Step 750/1249 Loss 3.771 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.768, lat_loss 21.835
09/28 05:21:38 PM | Train: [ 88/180] Step 800/1249 Loss 3.781 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:22:03 PM | Train: [ 88/180] Step 850/1249 Loss 3.802 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:22:28 PM | Train: [ 88/180] Step 900/1249 Loss 3.811 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:22:54 PM | Train: [ 88/180] Step 950/1249 Loss 3.847 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:23:19 PM | Train: [ 88/180] Step 1000/1249 Loss 3.850 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:23:43 PM | Train: [ 88/180] Step 1050/1249 Loss 3.852 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:24:08 PM | Train: [ 88/180] Step 1100/1249 Loss 3.847 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:24:34 PM | Train: [ 88/180] Step 1150/1249 Loss 3.843 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:24:59 PM | Train: [ 88/180] Step 1200/1249 Loss 3.840 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:25:23 PM | Train: [ 88/180] Step 1249/1249 Loss 3.824 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.767, lat_loss 21.835
09/28 05:25:24 PM | _w_step_train: [ 88/180] Final Prec@1 85.3475% Time 603.52
09/28 05:25:24 PM | Start to train theta for epoch 87
09/28 05:25:45 PM | Train: [ 88/180] Step 050/312 Loss 4.665 Prec@(1,3) (83.2%, 98.8%), ce_loss 0.766, lat_loss 21.834
09/28 05:26:03 PM | Train: [ 88/180] Step 100/312 Loss 4.567 Prec@(1,3) (83.3%, 99.1%), ce_loss 0.766, lat_loss 21.834
09/28 05:26:21 PM | Train: [ 88/180] Step 150/312 Loss 4.558 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.766, lat_loss 21.834
09/28 05:26:42 PM | Train: [ 88/180] Step 200/312 Loss 4.552 Prec@(1,3) (83.0%, 98.9%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:01 PM | Train: [ 88/180] Step 250/312 Loss 4.471 Prec@(1,3) (83.2%, 99.0%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:22 PM | Train: [ 88/180] Step 300/312 Loss 4.491 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:27 PM | Train: [ 88/180] Step 312/312 Loss 4.484 Prec@(1,3) (83.4%, 99.0%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:27 PM | _theta_step_train: [ 88/180] Final Prec@1 83.3500% Time 123.19
09/28 05:27:32 PM | Valid: [ 88/180] Step 050/312 Loss 4.251 Prec@(1,3) (83.5%, 99.4%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:37 PM | Valid: [ 88/180] Step 100/312 Loss 4.409 Prec@(1,3) (83.3%, 99.4%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:41 PM | Valid: [ 88/180] Step 150/312 Loss 4.392 Prec@(1,3) (83.4%, 99.3%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:46 PM | Valid: [ 88/180] Step 200/312 Loss 4.482 Prec@(1,3) (83.1%, 99.1%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:51 PM | Valid: [ 88/180] Step 250/312 Loss 4.419 Prec@(1,3) (83.1%, 99.2%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:55 PM | Valid: [ 88/180] Step 300/312 Loss 4.386 Prec@(1,3) (83.5%, 99.2%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:56 PM | Valid: [ 88/180] Step 312/312 Loss 4.382 Prec@(1,3) (83.5%, 99.2%), ce_loss 0.766, lat_loss 21.834
09/28 05:27:56 PM | val: [ 88/180] Final Prec@1 83.4700% Time 29.57
09/28 05:27:56 PM | Start to train weights for epoch 88
09/28 05:28:23 PM | Train: [ 89/180] Step 050/1249 Loss 3.441 Prec@(1,3) (86.3%, 98.8%), ce_loss 0.765, lat_loss 21.834
09/28 05:28:47 PM | Train: [ 89/180] Step 100/1249 Loss 3.567 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.765, lat_loss 21.834
09/28 05:29:12 PM | Train: [ 89/180] Step 150/1249 Loss 3.537 Prec@(1,3) (86.6%, 99.0%), ce_loss 0.765, lat_loss 21.834
09/28 05:29:35 PM | Train: [ 89/180] Step 200/1249 Loss 3.563 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.765, lat_loss 21.834
09/28 05:29:59 PM | Train: [ 89/180] Step 250/1249 Loss 3.546 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.765, lat_loss 21.834
09/28 05:30:15 PM | Train: [ 89/180] Step 300/1249 Loss 3.592 Prec@(1,3) (86.0%, 99.1%), ce_loss 0.765, lat_loss 21.834
09/28 05:30:31 PM | Train: [ 89/180] Step 350/1249 Loss 3.595 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.765, lat_loss 21.834
09/28 05:30:55 PM | Train: [ 89/180] Step 400/1249 Loss 3.668 Prec@(1,3) (85.9%, 99.1%), ce_loss 0.765, lat_loss 21.834
09/28 05:31:19 PM | Train: [ 89/180] Step 450/1249 Loss 3.652 Prec@(1,3) (86.0%, 99.2%), ce_loss 0.765, lat_loss 21.834
09/28 05:31:42 PM | Train: [ 89/180] Step 500/1249 Loss 3.642 Prec@(1,3) (86.0%, 99.2%), ce_loss 0.764, lat_loss 21.834
09/28 05:32:05 PM | Train: [ 89/180] Step 550/1249 Loss 3.664 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.764, lat_loss 21.834
09/28 05:32:22 PM | Train: [ 89/180] Step 600/1249 Loss 3.658 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.764, lat_loss 21.834
09/28 05:32:38 PM | Train: [ 89/180] Step 650/1249 Loss 3.656 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.764, lat_loss 21.833
09/28 05:32:54 PM | Train: [ 89/180] Step 700/1249 Loss 3.649 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.764, lat_loss 21.833
09/28 05:33:13 PM | Train: [ 89/180] Step 750/1249 Loss 3.652 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.764, lat_loss 21.833
09/28 05:33:35 PM | Train: [ 89/180] Step 800/1249 Loss 3.661 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.764, lat_loss 21.833
09/28 05:34:00 PM | Train: [ 89/180] Step 850/1249 Loss 3.666 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.764, lat_loss 21.833
09/28 05:34:18 PM | Train: [ 89/180] Step 900/1249 Loss 3.670 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.764, lat_loss 21.833
09/28 05:34:34 PM | Train: [ 89/180] Step 950/1249 Loss 3.665 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.763, lat_loss 21.833
09/28 05:34:50 PM | Train: [ 89/180] Step 1000/1249 Loss 3.667 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.763, lat_loss 21.833
09/28 05:35:06 PM | Train: [ 89/180] Step 1050/1249 Loss 3.661 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.763, lat_loss 21.833
09/28 05:35:22 PM | Train: [ 89/180] Step 1100/1249 Loss 3.666 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.763, lat_loss 21.833
09/28 05:35:41 PM | Train: [ 89/180] Step 1150/1249 Loss 3.670 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.763, lat_loss 21.833
09/28 05:36:06 PM | Train: [ 89/180] Step 1200/1249 Loss 3.658 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.763, lat_loss 21.833
09/28 05:36:30 PM | Train: [ 89/180] Step 1249/1249 Loss 3.692 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.763, lat_loss 21.833
09/28 05:36:30 PM | _w_step_train: [ 89/180] Final Prec@1 85.8525% Time 514.05
09/28 05:36:30 PM | Start to train theta for epoch 88
09/28 05:36:52 PM | Train: [ 89/180] Step 050/312 Loss 4.221 Prec@(1,3) (83.3%, 99.1%), ce_loss 0.763, lat_loss 21.833
09/28 05:37:12 PM | Train: [ 89/180] Step 100/312 Loss 4.703 Prec@(1,3) (82.3%, 98.7%), ce_loss 0.763, lat_loss 21.833
09/28 05:37:32 PM | Train: [ 89/180] Step 150/312 Loss 4.568 Prec@(1,3) (82.9%, 98.7%), ce_loss 0.763, lat_loss 21.833
09/28 05:37:53 PM | Train: [ 89/180] Step 200/312 Loss 4.502 Prec@(1,3) (83.0%, 98.8%), ce_loss 0.763, lat_loss 21.833
09/28 05:38:14 PM | Train: [ 89/180] Step 250/312 Loss 4.471 Prec@(1,3) (82.9%, 98.9%), ce_loss 0.762, lat_loss 21.833
09/28 05:38:34 PM | Train: [ 89/180] Step 300/312 Loss 4.497 Prec@(1,3) (82.8%, 98.9%), ce_loss 0.762, lat_loss 21.833
09/28 05:38:39 PM | Train: [ 89/180] Step 312/312 Loss 4.477 Prec@(1,3) (82.8%, 98.9%), ce_loss 0.762, lat_loss 21.833
09/28 05:38:39 PM | _theta_step_train: [ 89/180] Final Prec@1 82.8400% Time 128.59
09/28 05:38:44 PM | Valid: [ 89/180] Step 050/312 Loss 4.019 Prec@(1,3) (84.8%, 99.2%), ce_loss 0.762, lat_loss 21.833
09/28 05:38:49 PM | Valid: [ 89/180] Step 100/312 Loss 4.176 Prec@(1,3) (84.2%, 99.2%), ce_loss 0.762, lat_loss 21.833
09/28 05:38:54 PM | Valid: [ 89/180] Step 150/312 Loss 4.108 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.762, lat_loss 21.833
09/28 05:38:58 PM | Valid: [ 89/180] Step 200/312 Loss 4.124 Prec@(1,3) (84.4%, 99.2%), ce_loss 0.762, lat_loss 21.833
09/28 05:39:03 PM | Valid: [ 89/180] Step 250/312 Loss 4.192 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.762, lat_loss 21.833
09/28 05:39:07 PM | Valid: [ 89/180] Step 300/312 Loss 4.203 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.762, lat_loss 21.833
09/28 05:39:08 PM | Valid: [ 89/180] Step 312/312 Loss 4.196 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.762, lat_loss 21.833
09/28 05:39:09 PM | val: [ 89/180] Final Prec@1 84.1600% Time 29.49
09/28 05:39:09 PM | Best top1 acc by now. Save model
09/28 05:39:09 PM | Start to train weights for epoch 89
09/28 05:39:31 PM | Train: [ 90/180] Step 050/1249 Loss 4.287 Prec@(1,3) (84.9%, 99.0%), ce_loss 0.762, lat_loss 21.832
09/28 05:39:53 PM | Train: [ 90/180] Step 100/1249 Loss 4.033 Prec@(1,3) (85.2%, 99.0%), ce_loss 0.762, lat_loss 21.832
09/28 05:40:15 PM | Train: [ 90/180] Step 150/1249 Loss 3.854 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.762, lat_loss 21.832
09/28 05:40:36 PM | Train: [ 90/180] Step 200/1249 Loss 3.698 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.761, lat_loss 21.832
09/28 05:40:59 PM | Train: [ 90/180] Step 250/1249 Loss 3.656 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.761, lat_loss 21.832
09/28 05:41:21 PM | Train: [ 90/180] Step 300/1249 Loss 3.723 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.761, lat_loss 21.832
09/28 05:41:44 PM | Train: [ 90/180] Step 350/1249 Loss 3.683 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.761, lat_loss 21.832
09/28 05:42:07 PM | Train: [ 90/180] Step 400/1249 Loss 3.641 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.761, lat_loss 21.832
09/28 05:42:31 PM | Train: [ 90/180] Step 450/1249 Loss 3.668 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.761, lat_loss 21.832
09/28 05:42:55 PM | Train: [ 90/180] Step 500/1249 Loss 3.722 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.761, lat_loss 21.832
09/28 05:43:20 PM | Train: [ 90/180] Step 550/1249 Loss 3.685 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.761, lat_loss 21.832
09/28 05:43:44 PM | Train: [ 90/180] Step 600/1249 Loss 3.678 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.761, lat_loss 21.832
09/28 05:44:07 PM | Train: [ 90/180] Step 650/1249 Loss 3.677 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.760, lat_loss 21.832
09/28 05:44:28 PM | Train: [ 90/180] Step 700/1249 Loss 3.662 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.760, lat_loss 21.832
09/28 05:44:50 PM | Train: [ 90/180] Step 750/1249 Loss 3.626 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.760, lat_loss 21.832
09/28 05:45:11 PM | Train: [ 90/180] Step 800/1249 Loss 3.637 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.760, lat_loss 21.832
09/28 05:45:32 PM | Train: [ 90/180] Step 850/1249 Loss 3.652 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.760, lat_loss 21.832
09/28 05:45:54 PM | Train: [ 90/180] Step 900/1249 Loss 3.638 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.760, lat_loss 21.832
09/28 05:46:16 PM | Train: [ 90/180] Step 950/1249 Loss 3.622 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.760, lat_loss 21.832
09/28 05:46:38 PM | Train: [ 90/180] Step 1000/1249 Loss 3.639 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.760, lat_loss 21.832
09/28 05:46:59 PM | Train: [ 90/180] Step 1050/1249 Loss 3.644 Prec@(1,3) (86.0%, 99.4%), ce_loss 0.760, lat_loss 21.832
09/28 05:47:20 PM | Train: [ 90/180] Step 1100/1249 Loss 3.628 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.760, lat_loss 21.832
09/28 05:47:41 PM | Train: [ 90/180] Step 1150/1249 Loss 3.656 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.759, lat_loss 21.832
09/28 05:48:02 PM | Train: [ 90/180] Step 1200/1249 Loss 3.662 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.759, lat_loss 21.832
09/28 05:48:24 PM | Train: [ 90/180] Step 1249/1249 Loss 3.671 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.759, lat_loss 21.832
09/28 05:48:24 PM | _w_step_train: [ 90/180] Final Prec@1 85.9550% Time 555.55
09/28 05:48:24 PM | Start to train theta for epoch 89
09/28 05:48:45 PM | Train: [ 90/180] Step 050/312 Loss 4.251 Prec@(1,3) (83.9%, 99.0%), ce_loss 0.759, lat_loss 21.831
09/28 05:49:04 PM | Train: [ 90/180] Step 100/312 Loss 4.399 Prec@(1,3) (83.7%, 98.8%), ce_loss 0.759, lat_loss 21.831
09/28 05:49:16 PM | Train: [ 90/180] Step 150/312 Loss 4.246 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.759, lat_loss 21.831
09/28 05:49:28 PM | Train: [ 90/180] Step 200/312 Loss 4.270 Prec@(1,3) (83.6%, 99.1%), ce_loss 0.759, lat_loss 21.831
09/28 05:49:40 PM | Train: [ 90/180] Step 250/312 Loss 4.338 Prec@(1,3) (83.4%, 99.0%), ce_loss 0.759, lat_loss 21.831
09/28 05:49:53 PM | Train: [ 90/180] Step 300/312 Loss 4.367 Prec@(1,3) (83.4%, 99.0%), ce_loss 0.759, lat_loss 21.831
09/28 05:49:56 PM | Train: [ 90/180] Step 312/312 Loss 4.348 Prec@(1,3) (83.5%, 99.1%), ce_loss 0.759, lat_loss 21.831
09/28 05:49:56 PM | _theta_step_train: [ 90/180] Final Prec@1 83.4600% Time 91.30
09/28 05:50:01 PM | Valid: [ 90/180] Step 050/312 Loss 4.045 Prec@(1,3) (84.9%, 99.6%), ce_loss 0.759, lat_loss 21.831
09/28 05:50:06 PM | Valid: [ 90/180] Step 100/312 Loss 4.142 Prec@(1,3) (84.5%, 99.4%), ce_loss 0.759, lat_loss 21.831
09/28 05:50:10 PM | Valid: [ 90/180] Step 150/312 Loss 4.090 Prec@(1,3) (84.6%, 99.4%), ce_loss 0.758, lat_loss 21.831
09/28 05:50:15 PM | Valid: [ 90/180] Step 200/312 Loss 4.214 Prec@(1,3) (84.3%, 99.3%), ce_loss 0.758, lat_loss 21.831
09/28 05:50:19 PM | Valid: [ 90/180] Step 250/312 Loss 4.186 Prec@(1,3) (84.4%, 99.3%), ce_loss 0.758, lat_loss 21.831
09/28 05:50:24 PM | Valid: [ 90/180] Step 300/312 Loss 4.150 Prec@(1,3) (84.3%, 99.4%), ce_loss 0.758, lat_loss 21.831
09/28 05:50:25 PM | Valid: [ 90/180] Step 312/312 Loss 4.141 Prec@(1,3) (84.3%, 99.4%), ce_loss 0.758, lat_loss 21.831
09/28 05:50:25 PM | val: [ 90/180] Final Prec@1 84.3100% Time 29.79
09/28 05:50:25 PM | Best top1 acc by now. Save model
09/28 05:50:26 PM | Start to train weights for epoch 90
09/28 05:50:43 PM | Train: [ 91/180] Step 050/1249 Loss 3.265 Prec@(1,3) (87.1%, 99.2%), ce_loss 0.758, lat_loss 21.831
09/28 05:50:59 PM | Train: [ 91/180] Step 100/1249 Loss 3.393 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.758, lat_loss 21.831
09/28 05:51:15 PM | Train: [ 91/180] Step 150/1249 Loss 3.440 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.758, lat_loss 21.831
09/28 05:51:31 PM | Train: [ 91/180] Step 200/1249 Loss 3.590 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.758, lat_loss 21.831
09/28 05:51:47 PM | Train: [ 91/180] Step 250/1249 Loss 3.491 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.758, lat_loss 21.831
09/28 05:52:03 PM | Train: [ 91/180] Step 300/1249 Loss 3.606 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.758, lat_loss 21.831
09/28 05:52:19 PM | Train: [ 91/180] Step 350/1249 Loss 3.545 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.757, lat_loss 21.831
09/28 05:52:34 PM | Train: [ 91/180] Step 400/1249 Loss 3.549 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.757, lat_loss 21.831
09/28 05:52:50 PM | Train: [ 91/180] Step 450/1249 Loss 3.545 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.757, lat_loss 21.831
09/28 05:53:06 PM | Train: [ 91/180] Step 500/1249 Loss 3.548 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.757, lat_loss 21.831
09/28 05:53:22 PM | Train: [ 91/180] Step 550/1249 Loss 3.513 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.757, lat_loss 21.831
09/28 05:53:38 PM | Train: [ 91/180] Step 600/1249 Loss 3.612 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.757, lat_loss 21.831
09/28 05:53:54 PM | Train: [ 91/180] Step 650/1249 Loss 3.610 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.757, lat_loss 21.830
09/28 05:54:10 PM | Train: [ 91/180] Step 700/1249 Loss 3.628 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.757, lat_loss 21.830
09/28 05:54:26 PM | Train: [ 91/180] Step 750/1249 Loss 3.673 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.757, lat_loss 21.830
09/28 05:54:42 PM | Train: [ 91/180] Step 800/1249 Loss 3.679 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.757, lat_loss 21.830
09/28 05:54:58 PM | Train: [ 91/180] Step 850/1249 Loss 3.676 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:55:13 PM | Train: [ 91/180] Step 900/1249 Loss 3.683 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:55:29 PM | Train: [ 91/180] Step 950/1249 Loss 3.683 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:55:45 PM | Train: [ 91/180] Step 1000/1249 Loss 3.680 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:56:01 PM | Train: [ 91/180] Step 1050/1249 Loss 3.669 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:56:17 PM | Train: [ 91/180] Step 1100/1249 Loss 3.673 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:56:37 PM | Train: [ 91/180] Step 1150/1249 Loss 3.681 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:56:53 PM | Train: [ 91/180] Step 1200/1249 Loss 3.675 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:57:08 PM | Train: [ 91/180] Step 1249/1249 Loss 3.671 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.756, lat_loss 21.830
09/28 05:57:08 PM | _w_step_train: [ 91/180] Final Prec@1 85.9475% Time 402.67
09/28 05:57:08 PM | Start to train theta for epoch 90
09/28 05:57:28 PM | Train: [ 91/180] Step 050/312 Loss 4.223 Prec@(1,3) (84.0%, 98.9%), ce_loss 0.756, lat_loss 21.830
09/28 05:57:45 PM | Train: [ 91/180] Step 100/312 Loss 4.485 Prec@(1,3) (83.1%, 98.9%), ce_loss 0.755, lat_loss 21.830
09/28 05:58:02 PM | Train: [ 91/180] Step 150/312 Loss 4.367 Prec@(1,3) (83.4%, 98.9%), ce_loss 0.755, lat_loss 21.830
09/28 05:58:16 PM | Train: [ 91/180] Step 200/312 Loss 4.370 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.755, lat_loss 21.830
09/28 05:58:33 PM | Train: [ 91/180] Step 250/312 Loss 4.397 Prec@(1,3) (83.4%, 98.9%), ce_loss 0.755, lat_loss 21.830
09/28 05:58:53 PM | Train: [ 91/180] Step 300/312 Loss 4.387 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.755, lat_loss 21.830
09/28 05:58:58 PM | Train: [ 91/180] Step 312/312 Loss 4.382 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.755, lat_loss 21.830
09/28 05:58:58 PM | _theta_step_train: [ 91/180] Final Prec@1 83.6500% Time 109.38
09/28 05:59:03 PM | Valid: [ 91/180] Step 050/312 Loss 3.854 Prec@(1,3) (85.3%, 99.3%), ce_loss 0.755, lat_loss 21.830
09/28 05:59:08 PM | Valid: [ 91/180] Step 100/312 Loss 4.140 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.755, lat_loss 21.830
09/28 05:59:12 PM | Valid: [ 91/180] Step 150/312 Loss 4.040 Prec@(1,3) (85.0%, 99.0%), ce_loss 0.755, lat_loss 21.830
09/28 05:59:17 PM | Valid: [ 91/180] Step 200/312 Loss 4.021 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.755, lat_loss 21.830
09/28 05:59:21 PM | Valid: [ 91/180] Step 250/312 Loss 4.024 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.755, lat_loss 21.830
09/28 05:59:26 PM | Valid: [ 91/180] Step 300/312 Loss 4.034 Prec@(1,3) (84.8%, 99.2%), ce_loss 0.755, lat_loss 21.830
09/28 05:59:27 PM | Valid: [ 91/180] Step 312/312 Loss 4.024 Prec@(1,3) (84.8%, 99.2%), ce_loss 0.755, lat_loss 21.830
09/28 05:59:27 PM | val: [ 91/180] Final Prec@1 84.7600% Time 29.45
09/28 05:59:27 PM | Best top1 acc by now. Save model
09/28 05:59:27 PM | Start to train weights for epoch 91
09/28 05:59:51 PM | Train: [ 92/180] Step 050/1249 Loss 3.539 Prec@(1,3) (86.8%, 99.6%), ce_loss 0.755, lat_loss 21.829
09/28 06:00:14 PM | Train: [ 92/180] Step 100/1249 Loss 3.791 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.754, lat_loss 21.829
09/28 06:00:36 PM | Train: [ 92/180] Step 150/1249 Loss 3.686 Prec@(1,3) (86.4%, 99.5%), ce_loss 0.754, lat_loss 21.829
09/28 06:00:58 PM | Train: [ 92/180] Step 200/1249 Loss 3.617 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.754, lat_loss 21.829
09/28 06:01:21 PM | Train: [ 92/180] Step 250/1249 Loss 3.632 Prec@(1,3) (86.4%, 99.5%), ce_loss 0.754, lat_loss 21.829
09/28 06:01:43 PM | Train: [ 92/180] Step 300/1249 Loss 3.653 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.754, lat_loss 21.829
09/28 06:02:07 PM | Train: [ 92/180] Step 350/1249 Loss 3.611 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.754, lat_loss 21.829
09/28 06:02:29 PM | Train: [ 92/180] Step 400/1249 Loss 3.593 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.754, lat_loss 21.829
09/28 06:02:53 PM | Train: [ 92/180] Step 450/1249 Loss 3.551 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.754, lat_loss 21.829
09/28 06:03:15 PM | Train: [ 92/180] Step 500/1249 Loss 3.545 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.754, lat_loss 21.829
09/28 06:03:38 PM | Train: [ 92/180] Step 550/1249 Loss 3.524 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:04:01 PM | Train: [ 92/180] Step 600/1249 Loss 3.556 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:04:23 PM | Train: [ 92/180] Step 650/1249 Loss 3.556 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:04:46 PM | Train: [ 92/180] Step 700/1249 Loss 3.528 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:05:09 PM | Train: [ 92/180] Step 750/1249 Loss 3.500 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:05:34 PM | Train: [ 92/180] Step 800/1249 Loss 3.479 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:05:59 PM | Train: [ 92/180] Step 850/1249 Loss 3.454 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:06:24 PM | Train: [ 92/180] Step 900/1249 Loss 3.453 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:06:49 PM | Train: [ 92/180] Step 950/1249 Loss 3.458 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.753, lat_loss 21.829
09/28 06:07:14 PM | Train: [ 92/180] Step 1000/1249 Loss 3.449 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.752, lat_loss 21.829
09/28 06:07:39 PM | Train: [ 92/180] Step 1050/1249 Loss 3.468 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.752, lat_loss 21.829
09/28 06:08:04 PM | Train: [ 92/180] Step 1100/1249 Loss 3.477 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.752, lat_loss 21.829
09/28 06:08:30 PM | Train: [ 92/180] Step 1150/1249 Loss 3.498 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.752, lat_loss 21.829
09/28 06:08:55 PM | Train: [ 92/180] Step 1200/1249 Loss 3.495 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.752, lat_loss 21.829
09/28 06:09:19 PM | Train: [ 92/180] Step 1249/1249 Loss 3.502 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.752, lat_loss 21.829
09/28 06:09:19 PM | _w_step_train: [ 92/180] Final Prec@1 86.3975% Time 591.70
09/28 06:09:19 PM | Start to train theta for epoch 91
09/28 06:09:41 PM | Train: [ 92/180] Step 050/312 Loss 4.924 Prec@(1,3) (81.7%, 98.8%), ce_loss 0.752, lat_loss 21.828
09/28 06:10:01 PM | Train: [ 92/180] Step 100/312 Loss 4.528 Prec@(1,3) (82.9%, 98.9%), ce_loss 0.752, lat_loss 21.828
09/28 06:10:18 PM | Train: [ 92/180] Step 150/312 Loss 4.490 Prec@(1,3) (83.1%, 98.8%), ce_loss 0.752, lat_loss 21.828
09/28 06:10:38 PM | Train: [ 92/180] Step 200/312 Loss 4.413 Prec@(1,3) (83.2%, 98.9%), ce_loss 0.752, lat_loss 21.828
09/28 06:10:59 PM | Train: [ 92/180] Step 250/312 Loss 4.373 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.752, lat_loss 21.828
09/28 06:11:19 PM | Train: [ 92/180] Step 300/312 Loss 4.377 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.752, lat_loss 21.828
09/28 06:11:24 PM | Train: [ 92/180] Step 312/312 Loss 4.361 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.751, lat_loss 21.828
09/28 06:11:24 PM | _theta_step_train: [ 92/180] Final Prec@1 83.2800% Time 125.43
09/28 06:11:30 PM | Valid: [ 92/180] Step 050/312 Loss 4.025 Prec@(1,3) (84.8%, 99.3%), ce_loss 0.751, lat_loss 21.828
09/28 06:11:34 PM | Valid: [ 92/180] Step 100/312 Loss 4.202 Prec@(1,3) (83.9%, 98.9%), ce_loss 0.751, lat_loss 21.828
09/28 06:11:39 PM | Valid: [ 92/180] Step 150/312 Loss 4.111 Prec@(1,3) (84.3%, 98.8%), ce_loss 0.751, lat_loss 21.828
09/28 06:11:43 PM | Valid: [ 92/180] Step 200/312 Loss 4.114 Prec@(1,3) (84.3%, 98.9%), ce_loss 0.751, lat_loss 21.828
09/28 06:11:48 PM | Valid: [ 92/180] Step 250/312 Loss 4.278 Prec@(1,3) (84.2%, 98.8%), ce_loss 0.751, lat_loss 21.828
09/28 06:11:52 PM | Valid: [ 92/180] Step 300/312 Loss 4.193 Prec@(1,3) (84.4%, 98.9%), ce_loss 0.751, lat_loss 21.828
09/28 06:11:53 PM | Valid: [ 92/180] Step 312/312 Loss 4.177 Prec@(1,3) (84.5%, 98.9%), ce_loss 0.751, lat_loss 21.828
09/28 06:11:54 PM | val: [ 92/180] Final Prec@1 84.4500% Time 29.09
09/28 06:11:54 PM | Start to train weights for epoch 92
09/28 06:12:20 PM | Train: [ 93/180] Step 050/1249 Loss 3.235 Prec@(1,3) (86.8%, 99.8%), ce_loss 0.751, lat_loss 21.828
09/28 06:12:45 PM | Train: [ 93/180] Step 100/1249 Loss 3.217 Prec@(1,3) (87.0%, 99.6%), ce_loss 0.751, lat_loss 21.828
09/28 06:13:10 PM | Train: [ 93/180] Step 150/1249 Loss 3.165 Prec@(1,3) (87.3%, 99.6%), ce_loss 0.751, lat_loss 21.828
09/28 06:13:35 PM | Train: [ 93/180] Step 200/1249 Loss 3.196 Prec@(1,3) (87.3%, 99.6%), ce_loss 0.751, lat_loss 21.828
09/28 06:14:00 PM | Train: [ 93/180] Step 250/1249 Loss 3.302 Prec@(1,3) (87.1%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:14:25 PM | Train: [ 93/180] Step 300/1249 Loss 3.335 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:14:50 PM | Train: [ 93/180] Step 350/1249 Loss 3.357 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:15:12 PM | Train: [ 93/180] Step 400/1249 Loss 3.340 Prec@(1,3) (87.0%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:15:37 PM | Train: [ 93/180] Step 450/1249 Loss 3.340 Prec@(1,3) (87.0%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:16:02 PM | Train: [ 93/180] Step 500/1249 Loss 3.369 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:16:27 PM | Train: [ 93/180] Step 550/1249 Loss 3.325 Prec@(1,3) (87.0%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:16:51 PM | Train: [ 93/180] Step 600/1249 Loss 3.333 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:17:16 PM | Train: [ 93/180] Step 650/1249 Loss 3.324 Prec@(1,3) (87.0%, 99.5%), ce_loss 0.750, lat_loss 21.828
09/28 06:17:41 PM | Train: [ 93/180] Step 700/1249 Loss 3.351 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:18:06 PM | Train: [ 93/180] Step 750/1249 Loss 3.370 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:18:30 PM | Train: [ 93/180] Step 800/1249 Loss 3.372 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:18:55 PM | Train: [ 93/180] Step 850/1249 Loss 3.371 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:19:18 PM | Train: [ 93/180] Step 900/1249 Loss 3.368 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:19:42 PM | Train: [ 93/180] Step 950/1249 Loss 3.387 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:20:06 PM | Train: [ 93/180] Step 1000/1249 Loss 3.398 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:20:27 PM | Train: [ 93/180] Step 1050/1249 Loss 3.390 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:20:46 PM | Train: [ 93/180] Step 1100/1249 Loss 3.392 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:21:06 PM | Train: [ 93/180] Step 1150/1249 Loss 3.399 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.749, lat_loss 21.827
09/28 06:21:26 PM | Train: [ 93/180] Step 1200/1249 Loss 3.403 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.748, lat_loss 21.827
09/28 06:21:48 PM | Train: [ 93/180] Step 1249/1249 Loss 3.389 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.748, lat_loss 21.827
09/28 06:21:48 PM | _w_step_train: [ 93/180] Final Prec@1 86.9025% Time 594.35
09/28 06:21:48 PM | Start to train theta for epoch 92
09/28 06:22:08 PM | Train: [ 93/180] Step 050/312 Loss 4.349 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.748, lat_loss 21.827
09/28 06:22:27 PM | Train: [ 93/180] Step 100/312 Loss 4.400 Prec@(1,3) (83.5%, 98.8%), ce_loss 0.748, lat_loss 21.827
09/28 06:22:46 PM | Train: [ 93/180] Step 150/312 Loss 4.361 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.748, lat_loss 21.827
09/28 06:23:05 PM | Train: [ 93/180] Step 200/312 Loss 4.399 Prec@(1,3) (83.4%, 99.1%), ce_loss 0.748, lat_loss 21.827
09/28 06:23:25 PM | Train: [ 93/180] Step 250/312 Loss 4.445 Prec@(1,3) (83.4%, 99.1%), ce_loss 0.748, lat_loss 21.827
09/28 06:23:45 PM | Train: [ 93/180] Step 300/312 Loss 4.460 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.748, lat_loss 21.827
09/28 06:23:50 PM | Train: [ 93/180] Step 312/312 Loss 4.454 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.748, lat_loss 21.827
09/28 06:23:50 PM | _theta_step_train: [ 93/180] Final Prec@1 83.5600% Time 121.96
09/28 06:23:56 PM | Valid: [ 93/180] Step 050/312 Loss 3.818 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.748, lat_loss 21.827
09/28 06:24:00 PM | Valid: [ 93/180] Step 100/312 Loss 4.002 Prec@(1,3) (85.1%, 99.2%), ce_loss 0.748, lat_loss 21.827
09/28 06:24:05 PM | Valid: [ 93/180] Step 150/312 Loss 4.669 Prec@(1,3) (83.5%, 98.6%), ce_loss 0.748, lat_loss 21.827
09/28 06:24:09 PM | Valid: [ 93/180] Step 200/312 Loss 4.612 Prec@(1,3) (83.5%, 98.7%), ce_loss 0.748, lat_loss 21.827
09/28 06:24:14 PM | Valid: [ 93/180] Step 250/312 Loss 4.512 Prec@(1,3) (83.7%, 98.8%), ce_loss 0.748, lat_loss 21.827
09/28 06:24:18 PM | Valid: [ 93/180] Step 300/312 Loss 4.406 Prec@(1,3) (83.9%, 98.9%), ce_loss 0.747, lat_loss 21.827
09/28 06:24:20 PM | Valid: [ 93/180] Step 312/312 Loss 4.404 Prec@(1,3) (83.9%, 98.9%), ce_loss 0.747, lat_loss 21.827
09/28 06:24:20 PM | val: [ 93/180] Final Prec@1 83.9200% Time 29.74
09/28 06:24:20 PM | Start to train weights for epoch 93
09/28 06:24:43 PM | Train: [ 94/180] Step 050/1249 Loss 3.539 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.747, lat_loss 21.827
09/28 06:25:04 PM | Train: [ 94/180] Step 100/1249 Loss 3.255 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.747, lat_loss 21.826
09/28 06:25:25 PM | Train: [ 94/180] Step 150/1249 Loss 3.413 Prec@(1,3) (87.2%, 99.5%), ce_loss 0.747, lat_loss 21.826
09/28 06:25:47 PM | Train: [ 94/180] Step 200/1249 Loss 3.336 Prec@(1,3) (87.2%, 99.5%), ce_loss 0.747, lat_loss 21.826
09/28 06:26:12 PM | Train: [ 94/180] Step 250/1249 Loss 3.320 Prec@(1,3) (87.3%, 99.5%), ce_loss 0.747, lat_loss 21.826
09/28 06:26:35 PM | Train: [ 94/180] Step 300/1249 Loss 3.495 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.747, lat_loss 21.826
09/28 06:26:57 PM | Train: [ 94/180] Step 350/1249 Loss 3.459 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.747, lat_loss 21.826
09/28 06:27:20 PM | Train: [ 94/180] Step 400/1249 Loss 3.460 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.747, lat_loss 21.826
09/28 06:27:37 PM | Train: [ 94/180] Step 450/1249 Loss 3.480 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.746, lat_loss 21.826
09/28 06:27:53 PM | Train: [ 94/180] Step 500/1249 Loss 3.471 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.746, lat_loss 21.826
09/28 06:28:09 PM | Train: [ 94/180] Step 550/1249 Loss 3.444 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.746, lat_loss 21.826
09/28 06:28:27 PM | Train: [ 94/180] Step 600/1249 Loss 3.461 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.746, lat_loss 21.826
09/28 06:28:51 PM | Train: [ 94/180] Step 650/1249 Loss 3.445 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.746, lat_loss 21.826
09/28 06:29:14 PM | Train: [ 94/180] Step 700/1249 Loss 3.424 Prec@(1,3) (87.0%, 99.5%), ce_loss 0.746, lat_loss 21.826
09/28 06:29:37 PM | Train: [ 94/180] Step 750/1249 Loss 3.430 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.746, lat_loss 21.826
09/28 06:30:01 PM | Train: [ 94/180] Step 800/1249 Loss 3.436 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.746, lat_loss 21.826
09/28 06:30:24 PM | Train: [ 94/180] Step 850/1249 Loss 3.423 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.746, lat_loss 21.826
09/28 06:30:48 PM | Train: [ 94/180] Step 900/1249 Loss 3.438 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.746, lat_loss 21.826
09/28 06:31:11 PM | Train: [ 94/180] Step 950/1249 Loss 3.436 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.745, lat_loss 21.826
09/28 06:31:35 PM | Train: [ 94/180] Step 1000/1249 Loss 3.417 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.745, lat_loss 21.826
09/28 06:31:59 PM | Train: [ 94/180] Step 1050/1249 Loss 3.417 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.745, lat_loss 21.826
09/28 06:32:21 PM | Train: [ 94/180] Step 1100/1249 Loss 3.408 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.745, lat_loss 21.826
09/28 06:32:44 PM | Train: [ 94/180] Step 1150/1249 Loss 3.410 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.745, lat_loss 21.826
09/28 06:33:09 PM | Train: [ 94/180] Step 1200/1249 Loss 3.396 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.745, lat_loss 21.826
09/28 06:33:32 PM | Train: [ 94/180] Step 1249/1249 Loss 3.397 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.745, lat_loss 21.826
09/28 06:33:32 PM | _w_step_train: [ 94/180] Final Prec@1 86.8950% Time 552.27
09/28 06:33:32 PM | Start to train theta for epoch 93
09/28 06:33:45 PM | Train: [ 94/180] Step 050/312 Loss 5.115 Prec@(1,3) (81.2%, 98.8%), ce_loss 0.745, lat_loss 21.826
09/28 06:33:58 PM | Train: [ 94/180] Step 100/312 Loss 4.941 Prec@(1,3) (82.1%, 98.8%), ce_loss 0.745, lat_loss 21.826
09/28 06:34:19 PM | Train: [ 94/180] Step 150/312 Loss 4.722 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.745, lat_loss 21.825
09/28 06:34:37 PM | Train: [ 94/180] Step 200/312 Loss 4.719 Prec@(1,3) (82.2%, 99.0%), ce_loss 0.745, lat_loss 21.825
09/28 06:34:57 PM | Train: [ 94/180] Step 250/312 Loss 4.716 Prec@(1,3) (82.4%, 99.1%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:15 PM | Train: [ 94/180] Step 300/312 Loss 4.669 Prec@(1,3) (82.7%, 99.0%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:18 PM | Train: [ 94/180] Step 312/312 Loss 4.694 Prec@(1,3) (82.6%, 99.1%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:18 PM | _theta_step_train: [ 94/180] Final Prec@1 82.6200% Time 106.21
09/28 06:35:24 PM | Valid: [ 94/180] Step 050/312 Loss 4.194 Prec@(1,3) (84.2%, 99.4%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:28 PM | Valid: [ 94/180] Step 100/312 Loss 4.319 Prec@(1,3) (83.5%, 99.3%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:32 PM | Valid: [ 94/180] Step 150/312 Loss 4.304 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:36 PM | Valid: [ 94/180] Step 200/312 Loss 4.260 Prec@(1,3) (84.1%, 99.2%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:40 PM | Valid: [ 94/180] Step 250/312 Loss 4.273 Prec@(1,3) (84.0%, 99.2%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:45 PM | Valid: [ 94/180] Step 300/312 Loss 4.217 Prec@(1,3) (84.2%, 99.3%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:46 PM | Valid: [ 94/180] Step 312/312 Loss 4.208 Prec@(1,3) (84.2%, 99.3%), ce_loss 0.744, lat_loss 21.825
09/28 06:35:46 PM | val: [ 94/180] Final Prec@1 84.2400% Time 27.58
09/28 06:35:46 PM | Start to train weights for epoch 94
09/28 06:36:11 PM | Train: [ 95/180] Step 050/1249 Loss 3.362 Prec@(1,3) (86.9%, 99.6%), ce_loss 0.744, lat_loss 21.825
09/28 06:36:35 PM | Train: [ 95/180] Step 100/1249 Loss 3.395 Prec@(1,3) (87.0%, 99.5%), ce_loss 0.744, lat_loss 21.825
09/28 06:36:59 PM | Train: [ 95/180] Step 150/1249 Loss 3.431 Prec@(1,3) (86.6%, 99.5%), ce_loss 0.744, lat_loss 21.825
09/28 06:37:21 PM | Train: [ 95/180] Step 200/1249 Loss 3.665 Prec@(1,3) (86.2%, 99.2%), ce_loss 0.744, lat_loss 21.825
09/28 06:37:44 PM | Train: [ 95/180] Step 250/1249 Loss 3.586 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.743, lat_loss 21.825
09/28 06:38:08 PM | Train: [ 95/180] Step 300/1249 Loss 3.571 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.743, lat_loss 21.825
09/28 06:38:32 PM | Train: [ 95/180] Step 350/1249 Loss 3.476 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.743, lat_loss 21.825
09/28 06:38:55 PM | Train: [ 95/180] Step 400/1249 Loss 3.444 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.743, lat_loss 21.825
09/28 06:39:18 PM | Train: [ 95/180] Step 450/1249 Loss 3.485 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.743, lat_loss 21.825
09/28 06:39:41 PM | Train: [ 95/180] Step 500/1249 Loss 3.464 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.743, lat_loss 21.825
09/28 06:40:04 PM | Train: [ 95/180] Step 550/1249 Loss 3.499 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.743, lat_loss 21.825
09/28 06:40:26 PM | Train: [ 95/180] Step 600/1249 Loss 3.500 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.743, lat_loss 21.825
09/28 06:40:50 PM | Train: [ 95/180] Step 650/1249 Loss 3.509 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.743, lat_loss 21.825
09/28 06:41:13 PM | Train: [ 95/180] Step 700/1249 Loss 3.494 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.743, lat_loss 21.825
09/28 06:41:36 PM | Train: [ 95/180] Step 750/1249 Loss 3.453 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.742, lat_loss 21.824
09/28 06:41:58 PM | Train: [ 95/180] Step 800/1249 Loss 3.429 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.742, lat_loss 21.824
09/28 06:42:14 PM | Train: [ 95/180] Step 850/1249 Loss 3.424 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.742, lat_loss 21.824
09/28 06:42:30 PM | Train: [ 95/180] Step 900/1249 Loss 3.421 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.742, lat_loss 21.824
09/28 06:42:46 PM | Train: [ 95/180] Step 950/1249 Loss 3.422 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.742, lat_loss 21.824
09/28 06:43:02 PM | Train: [ 95/180] Step 1000/1249 Loss 3.454 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.742, lat_loss 21.824
09/28 06:43:18 PM | Train: [ 95/180] Step 1050/1249 Loss 3.458 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.742, lat_loss 21.824
09/28 06:43:36 PM | Train: [ 95/180] Step 1100/1249 Loss 3.452 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.742, lat_loss 21.824
09/28 06:43:56 PM | Train: [ 95/180] Step 1150/1249 Loss 3.459 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.742, lat_loss 21.824
09/28 06:44:16 PM | Train: [ 95/180] Step 1200/1249 Loss 3.460 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.742, lat_loss 21.824
09/28 06:44:31 PM | Train: [ 95/180] Step 1249/1249 Loss 3.468 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.741, lat_loss 21.824
09/28 06:44:31 PM | _w_step_train: [ 95/180] Final Prec@1 86.5850% Time 525.49
09/28 06:44:31 PM | Start to train theta for epoch 94
09/28 06:44:44 PM | Train: [ 95/180] Step 050/312 Loss 4.218 Prec@(1,3) (84.1%, 99.3%), ce_loss 0.741, lat_loss 21.824
09/28 06:44:57 PM | Train: [ 95/180] Step 100/312 Loss 4.298 Prec@(1,3) (83.9%, 99.0%), ce_loss 0.741, lat_loss 21.824
09/28 06:45:09 PM | Train: [ 95/180] Step 150/312 Loss 4.396 Prec@(1,3) (83.3%, 98.9%), ce_loss 0.741, lat_loss 21.824
09/28 06:45:21 PM | Train: [ 95/180] Step 200/312 Loss 4.279 Prec@(1,3) (83.7%, 98.9%), ce_loss 0.741, lat_loss 21.824
09/28 06:45:33 PM | Train: [ 95/180] Step 250/312 Loss 4.366 Prec@(1,3) (83.3%, 98.9%), ce_loss 0.741, lat_loss 21.824
09/28 06:45:45 PM | Train: [ 95/180] Step 300/312 Loss 4.364 Prec@(1,3) (83.3%, 98.9%), ce_loss 0.741, lat_loss 21.824
09/28 06:45:48 PM | Train: [ 95/180] Step 312/312 Loss 4.338 Prec@(1,3) (83.4%, 99.0%), ce_loss 0.741, lat_loss 21.824
09/28 06:45:48 PM | _theta_step_train: [ 95/180] Final Prec@1 83.4300% Time 77.18
09/28 06:45:54 PM | Valid: [ 95/180] Step 050/312 Loss 4.231 Prec@(1,3) (84.1%, 98.8%), ce_loss 0.741, lat_loss 21.824
09/28 06:45:58 PM | Valid: [ 95/180] Step 100/312 Loss 4.123 Prec@(1,3) (84.3%, 98.9%), ce_loss 0.741, lat_loss 21.824
09/28 06:46:03 PM | Valid: [ 95/180] Step 150/312 Loss 4.192 Prec@(1,3) (84.2%, 98.8%), ce_loss 0.741, lat_loss 21.824
09/28 06:46:07 PM | Valid: [ 95/180] Step 200/312 Loss 4.180 Prec@(1,3) (84.3%, 98.9%), ce_loss 0.741, lat_loss 21.824
09/28 06:46:12 PM | Valid: [ 95/180] Step 250/312 Loss 4.138 Prec@(1,3) (84.4%, 99.0%), ce_loss 0.741, lat_loss 21.824
09/28 06:46:16 PM | Valid: [ 95/180] Step 300/312 Loss 4.091 Prec@(1,3) (84.5%, 99.0%), ce_loss 0.741, lat_loss 21.824
09/28 06:46:17 PM | Valid: [ 95/180] Step 312/312 Loss 4.096 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.740, lat_loss 21.824
09/28 06:46:17 PM | val: [ 95/180] Final Prec@1 84.4800% Time 29.03
09/28 06:46:17 PM | Start to train weights for epoch 95
09/28 06:46:42 PM | Train: [ 96/180] Step 050/1249 Loss 3.208 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.740, lat_loss 21.824
09/28 06:47:04 PM | Train: [ 96/180] Step 100/1249 Loss 3.365 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.740, lat_loss 21.824
09/28 06:47:25 PM | Train: [ 96/180] Step 150/1249 Loss 3.296 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.740, lat_loss 21.823
09/28 06:47:46 PM | Train: [ 96/180] Step 200/1249 Loss 3.309 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.740, lat_loss 21.823
09/28 06:48:07 PM | Train: [ 96/180] Step 250/1249 Loss 3.326 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.740, lat_loss 21.823
09/28 06:48:30 PM | Train: [ 96/180] Step 300/1249 Loss 3.295 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.740, lat_loss 21.823
09/28 06:48:55 PM | Train: [ 96/180] Step 350/1249 Loss 3.318 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.740, lat_loss 21.823
09/28 06:49:20 PM | Train: [ 96/180] Step 400/1249 Loss 3.334 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.740, lat_loss 21.823
09/28 06:49:45 PM | Train: [ 96/180] Step 450/1249 Loss 3.352 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.740, lat_loss 21.823
09/28 06:50:10 PM | Train: [ 96/180] Step 500/1249 Loss 3.331 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.739, lat_loss 21.823
09/28 06:50:35 PM | Train: [ 96/180] Step 550/1249 Loss 3.306 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.739, lat_loss 21.823
09/28 06:51:00 PM | Train: [ 96/180] Step 600/1249 Loss 3.341 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.739, lat_loss 21.823
09/28 06:51:24 PM | Train: [ 96/180] Step 650/1249 Loss 3.340 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.739, lat_loss 21.823
09/28 06:51:38 PM | Train: [ 96/180] Step 700/1249 Loss 3.344 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.739, lat_loss 21.823
09/28 06:51:53 PM | Train: [ 96/180] Step 750/1249 Loss 3.325 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.739, lat_loss 21.823
09/28 06:52:08 PM | Train: [ 96/180] Step 800/1249 Loss 3.322 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.739, lat_loss 21.823
09/28 06:52:22 PM | Train: [ 96/180] Step 850/1249 Loss 3.324 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.739, lat_loss 21.823
09/28 06:52:37 PM | Train: [ 96/180] Step 900/1249 Loss 3.301 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.739, lat_loss 21.823
09/28 06:52:51 PM | Train: [ 96/180] Step 950/1249 Loss 3.295 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.739, lat_loss 21.823
09/28 06:53:06 PM | Train: [ 96/180] Step 1000/1249 Loss 3.294 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.738, lat_loss 21.823
09/28 06:53:20 PM | Train: [ 96/180] Step 1050/1249 Loss 3.308 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.738, lat_loss 21.823
09/28 06:53:34 PM | Train: [ 96/180] Step 1100/1249 Loss 3.310 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.738, lat_loss 21.823
09/28 06:53:49 PM | Train: [ 96/180] Step 1150/1249 Loss 3.318 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.738, lat_loss 21.823
09/28 06:54:03 PM | Train: [ 96/180] Step 1200/1249 Loss 3.313 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.738, lat_loss 21.823
09/28 06:54:18 PM | Train: [ 96/180] Step 1249/1249 Loss 3.321 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.738, lat_loss 21.823
09/28 06:54:18 PM | _w_step_train: [ 96/180] Final Prec@1 87.1225% Time 480.47
09/28 06:54:18 PM | Start to train theta for epoch 95
09/28 06:54:40 PM | Train: [ 96/180] Step 050/312 Loss 3.971 Prec@(1,3) (85.5%, 99.0%), ce_loss 0.738, lat_loss 21.823
09/28 06:55:01 PM | Train: [ 96/180] Step 100/312 Loss 4.082 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.738, lat_loss 21.823
09/28 06:55:21 PM | Train: [ 96/180] Step 150/312 Loss 4.204 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.738, lat_loss 21.823
09/28 06:55:42 PM | Train: [ 96/180] Step 200/312 Loss 4.234 Prec@(1,3) (84.2%, 99.0%), ce_loss 0.738, lat_loss 21.822
09/28 06:56:03 PM | Train: [ 96/180] Step 250/312 Loss 4.272 Prec@(1,3) (83.8%, 99.0%), ce_loss 0.738, lat_loss 21.822
09/28 06:56:24 PM | Train: [ 96/180] Step 300/312 Loss 4.238 Prec@(1,3) (83.8%, 99.0%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:29 PM | Train: [ 96/180] Step 312/312 Loss 4.220 Prec@(1,3) (83.9%, 99.0%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:29 PM | _theta_step_train: [ 96/180] Final Prec@1 83.9400% Time 131.09
09/28 06:56:34 PM | Valid: [ 96/180] Step 050/312 Loss 4.265 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:39 PM | Valid: [ 96/180] Step 100/312 Loss 4.257 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:43 PM | Valid: [ 96/180] Step 150/312 Loss 4.160 Prec@(1,3) (84.1%, 99.0%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:48 PM | Valid: [ 96/180] Step 200/312 Loss 4.183 Prec@(1,3) (84.0%, 99.1%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:52 PM | Valid: [ 96/180] Step 250/312 Loss 4.123 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:57 PM | Valid: [ 96/180] Step 300/312 Loss 4.067 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:58 PM | Valid: [ 96/180] Step 312/312 Loss 4.050 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.737, lat_loss 21.822
09/28 06:56:58 PM | val: [ 96/180] Final Prec@1 84.4200% Time 29.15
09/28 06:56:58 PM | Start to train weights for epoch 96
09/28 06:57:21 PM | Train: [ 97/180] Step 050/1249 Loss 2.952 Prec@(1,3) (87.7%, 99.8%), ce_loss 0.737, lat_loss 21.822
09/28 06:57:45 PM | Train: [ 97/180] Step 100/1249 Loss 3.329 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.737, lat_loss 21.822
09/28 06:58:08 PM | Train: [ 97/180] Step 150/1249 Loss 3.218 Prec@(1,3) (87.5%, 99.5%), ce_loss 0.737, lat_loss 21.822
09/28 06:58:31 PM | Train: [ 97/180] Step 200/1249 Loss 3.132 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.737, lat_loss 21.822
09/28 06:58:53 PM | Train: [ 97/180] Step 250/1249 Loss 3.165 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.736, lat_loss 21.822
09/28 06:59:14 PM | Train: [ 97/180] Step 300/1249 Loss 3.113 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.736, lat_loss 21.822
09/28 06:59:38 PM | Train: [ 97/180] Step 350/1249 Loss 3.147 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.736, lat_loss 21.822
09/28 07:00:01 PM | Train: [ 97/180] Step 400/1249 Loss 3.167 Prec@(1,3) (87.8%, 99.6%), ce_loss 0.736, lat_loss 21.822
09/28 07:00:24 PM | Train: [ 97/180] Step 450/1249 Loss 3.174 Prec@(1,3) (87.8%, 99.6%), ce_loss 0.736, lat_loss 21.822
09/28 07:00:47 PM | Train: [ 97/180] Step 500/1249 Loss 3.177 Prec@(1,3) (87.7%, 99.6%), ce_loss 0.736, lat_loss 21.822
09/28 07:01:11 PM | Train: [ 97/180] Step 550/1249 Loss 3.171 Prec@(1,3) (87.8%, 99.6%), ce_loss 0.736, lat_loss 21.822
09/28 07:01:32 PM | Train: [ 97/180] Step 600/1249 Loss 3.232 Prec@(1,3) (87.6%, 99.5%), ce_loss 0.736, lat_loss 21.822
09/28 07:01:54 PM | Train: [ 97/180] Step 650/1249 Loss 3.245 Prec@(1,3) (87.5%, 99.5%), ce_loss 0.736, lat_loss 21.822
09/28 07:02:17 PM | Train: [ 97/180] Step 700/1249 Loss 3.269 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.736, lat_loss 21.822
09/28 07:02:40 PM | Train: [ 97/180] Step 750/1249 Loss 3.277 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.735, lat_loss 21.822
09/28 07:03:03 PM | Train: [ 97/180] Step 800/1249 Loss 3.285 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.735, lat_loss 21.822
09/28 07:03:26 PM | Train: [ 97/180] Step 850/1249 Loss 3.305 Prec@(1,3) (87.3%, 99.5%), ce_loss 0.735, lat_loss 21.821
09/28 07:03:50 PM | Train: [ 97/180] Step 900/1249 Loss 3.287 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.735, lat_loss 21.821
09/28 07:04:13 PM | Train: [ 97/180] Step 950/1249 Loss 3.286 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.735, lat_loss 21.821
09/28 07:04:37 PM | Train: [ 97/180] Step 1000/1249 Loss 3.277 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.735, lat_loss 21.821
09/28 07:05:02 PM | Train: [ 97/180] Step 1050/1249 Loss 3.277 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.735, lat_loss 21.821
09/28 07:05:27 PM | Train: [ 97/180] Step 1100/1249 Loss 3.269 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.735, lat_loss 21.821
09/28 07:05:52 PM | Train: [ 97/180] Step 1150/1249 Loss 3.255 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.735, lat_loss 21.821
09/28 07:06:17 PM | Train: [ 97/180] Step 1200/1249 Loss 3.269 Prec@(1,3) (87.3%, 99.5%), ce_loss 0.734, lat_loss 21.821
09/28 07:06:42 PM | Train: [ 97/180] Step 1249/1249 Loss 3.271 Prec@(1,3) (87.3%, 99.5%), ce_loss 0.734, lat_loss 21.821
09/28 07:06:42 PM | _w_step_train: [ 97/180] Final Prec@1 87.3475% Time 583.45
09/28 07:06:42 PM | Start to train theta for epoch 96
09/28 07:07:03 PM | Train: [ 97/180] Step 050/312 Loss 4.488 Prec@(1,3) (83.7%, 98.9%), ce_loss 0.734, lat_loss 21.821
09/28 07:07:18 PM | Train: [ 97/180] Step 100/312 Loss 4.315 Prec@(1,3) (84.2%, 98.9%), ce_loss 0.734, lat_loss 21.821
09/28 07:07:34 PM | Train: [ 97/180] Step 150/312 Loss 4.357 Prec@(1,3) (83.8%, 99.0%), ce_loss 0.734, lat_loss 21.821
09/28 07:07:49 PM | Train: [ 97/180] Step 200/312 Loss 4.291 Prec@(1,3) (83.8%, 99.0%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:02 PM | Train: [ 97/180] Step 250/312 Loss 4.314 Prec@(1,3) (83.8%, 99.0%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:15 PM | Train: [ 97/180] Step 300/312 Loss 4.272 Prec@(1,3) (84.0%, 99.1%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:18 PM | Train: [ 97/180] Step 312/312 Loss 4.237 Prec@(1,3) (84.1%, 99.1%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:18 PM | _theta_step_train: [ 97/180] Final Prec@1 84.0800% Time 95.94
09/28 07:08:23 PM | Valid: [ 97/180] Step 050/312 Loss 3.759 Prec@(1,3) (85.7%, 99.6%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:28 PM | Valid: [ 97/180] Step 100/312 Loss 4.057 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:32 PM | Valid: [ 97/180] Step 150/312 Loss 4.205 Prec@(1,3) (84.6%, 98.9%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:37 PM | Valid: [ 97/180] Step 200/312 Loss 4.128 Prec@(1,3) (84.8%, 99.0%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:42 PM | Valid: [ 97/180] Step 250/312 Loss 4.061 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.734, lat_loss 21.821
09/28 07:08:46 PM | Valid: [ 97/180] Step 300/312 Loss 4.021 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.733, lat_loss 21.821
09/28 07:08:47 PM | Valid: [ 97/180] Step 312/312 Loss 4.036 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.733, lat_loss 21.821
09/28 07:08:47 PM | val: [ 97/180] Final Prec@1 84.9600% Time 29.67
09/28 07:08:47 PM | Best top1 acc by now. Save model
09/28 07:08:47 PM | Start to train weights for epoch 97
09/28 07:09:12 PM | Train: [ 98/180] Step 050/1249 Loss 3.332 Prec@(1,3) (87.4%, 99.6%), ce_loss 0.733, lat_loss 21.821
09/28 07:09:36 PM | Train: [ 98/180] Step 100/1249 Loss 3.719 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.733, lat_loss 21.821
09/28 07:09:59 PM | Train: [ 98/180] Step 150/1249 Loss 3.647 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.733, lat_loss 21.821
09/28 07:10:20 PM | Train: [ 98/180] Step 200/1249 Loss 3.576 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.733, lat_loss 21.821
09/28 07:10:44 PM | Train: [ 98/180] Step 250/1249 Loss 3.572 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.733, lat_loss 21.821
09/28 07:11:08 PM | Train: [ 98/180] Step 300/1249 Loss 3.512 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.733, lat_loss 21.821
09/28 07:11:31 PM | Train: [ 98/180] Step 350/1249 Loss 3.476 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.733, lat_loss 21.820
09/28 07:11:56 PM | Train: [ 98/180] Step 400/1249 Loss 3.461 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.733, lat_loss 21.820
09/28 07:12:20 PM | Train: [ 98/180] Step 450/1249 Loss 3.458 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.733, lat_loss 21.820
09/28 07:12:45 PM | Train: [ 98/180] Step 500/1249 Loss 3.451 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.733, lat_loss 21.820
09/28 07:13:08 PM | Train: [ 98/180] Step 550/1249 Loss 3.438 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:13:31 PM | Train: [ 98/180] Step 600/1249 Loss 3.447 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:13:54 PM | Train: [ 98/180] Step 650/1249 Loss 3.396 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:14:16 PM | Train: [ 98/180] Step 700/1249 Loss 3.386 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:14:41 PM | Train: [ 98/180] Step 750/1249 Loss 3.393 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:15:05 PM | Train: [ 98/180] Step 800/1249 Loss 3.395 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:15:23 PM | Train: [ 98/180] Step 850/1249 Loss 3.370 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:15:41 PM | Train: [ 98/180] Step 900/1249 Loss 3.369 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:16:05 PM | Train: [ 98/180] Step 950/1249 Loss 3.346 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.732, lat_loss 21.820
09/28 07:16:30 PM | Train: [ 98/180] Step 1000/1249 Loss 3.323 Prec@(1,3) (87.3%, 99.5%), ce_loss 0.731, lat_loss 21.820
09/28 07:16:56 PM | Train: [ 98/180] Step 1050/1249 Loss 3.339 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.731, lat_loss 21.820
09/28 07:17:21 PM | Train: [ 98/180] Step 1100/1249 Loss 3.320 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.731, lat_loss 21.820
09/28 07:17:46 PM | Train: [ 98/180] Step 1150/1249 Loss 3.320 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.731, lat_loss 21.820
09/28 07:18:11 PM | Train: [ 98/180] Step 1200/1249 Loss 3.320 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.731, lat_loss 21.820
09/28 07:18:36 PM | Train: [ 98/180] Step 1249/1249 Loss 3.327 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.731, lat_loss 21.820
09/28 07:18:36 PM | _w_step_train: [ 98/180] Final Prec@1 87.3425% Time 588.53
09/28 07:18:36 PM | Start to train theta for epoch 97
09/28 07:18:57 PM | Train: [ 98/180] Step 050/312 Loss 4.079 Prec@(1,3) (84.4%, 99.2%), ce_loss 0.731, lat_loss 21.820
09/28 07:19:17 PM | Train: [ 98/180] Step 100/312 Loss 3.935 Prec@(1,3) (85.2%, 99.3%), ce_loss 0.731, lat_loss 21.820
09/28 07:19:37 PM | Train: [ 98/180] Step 150/312 Loss 3.988 Prec@(1,3) (84.9%, 99.3%), ce_loss 0.731, lat_loss 21.820
09/28 07:19:57 PM | Train: [ 98/180] Step 200/312 Loss 3.959 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.731, lat_loss 21.820
09/28 07:20:16 PM | Train: [ 98/180] Step 250/312 Loss 3.994 Prec@(1,3) (84.9%, 99.3%), ce_loss 0.731, lat_loss 21.820
09/28 07:20:35 PM | Train: [ 98/180] Step 300/312 Loss 4.089 Prec@(1,3) (84.5%, 99.3%), ce_loss 0.731, lat_loss 21.820
09/28 07:20:40 PM | Train: [ 98/180] Step 312/312 Loss 4.114 Prec@(1,3) (84.5%, 99.3%), ce_loss 0.731, lat_loss 21.820
09/28 07:20:40 PM | _theta_step_train: [ 98/180] Final Prec@1 84.4700% Time 124.11
09/28 07:20:45 PM | Valid: [ 98/180] Step 050/312 Loss 4.024 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.730, lat_loss 21.820
09/28 07:20:50 PM | Valid: [ 98/180] Step 100/312 Loss 4.102 Prec@(1,3) (84.9%, 99.0%), ce_loss 0.730, lat_loss 21.820
09/28 07:20:54 PM | Valid: [ 98/180] Step 150/312 Loss 4.102 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.730, lat_loss 21.819
09/28 07:20:59 PM | Valid: [ 98/180] Step 200/312 Loss 4.052 Prec@(1,3) (84.8%, 99.0%), ce_loss 0.730, lat_loss 21.819
09/28 07:21:04 PM | Valid: [ 98/180] Step 250/312 Loss 4.138 Prec@(1,3) (84.3%, 98.9%), ce_loss 0.730, lat_loss 21.819
09/28 07:21:08 PM | Valid: [ 98/180] Step 300/312 Loss 4.111 Prec@(1,3) (84.3%, 98.9%), ce_loss 0.730, lat_loss 21.819
09/28 07:21:09 PM | Valid: [ 98/180] Step 312/312 Loss 4.107 Prec@(1,3) (84.2%, 98.9%), ce_loss 0.730, lat_loss 21.819
09/28 07:21:09 PM | val: [ 98/180] Final Prec@1 84.2300% Time 29.03
09/28 07:21:09 PM | Start to train weights for epoch 98
09/28 07:21:27 PM | Train: [ 99/180] Step 050/1249 Loss 3.318 Prec@(1,3) (87.1%, 99.6%), ce_loss 0.730, lat_loss 21.819
09/28 07:21:43 PM | Train: [ 99/180] Step 100/1249 Loss 3.149 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.730, lat_loss 21.819
09/28 07:21:58 PM | Train: [ 99/180] Step 150/1249 Loss 3.032 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.730, lat_loss 21.819
09/28 07:22:14 PM | Train: [ 99/180] Step 200/1249 Loss 2.966 Prec@(1,3) (88.6%, 99.5%), ce_loss 0.730, lat_loss 21.819
09/28 07:22:30 PM | Train: [ 99/180] Step 250/1249 Loss 3.023 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.730, lat_loss 21.819
09/28 07:22:46 PM | Train: [ 99/180] Step 300/1249 Loss 3.045 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.729, lat_loss 21.819
09/28 07:23:02 PM | Train: [ 99/180] Step 350/1249 Loss 3.167 Prec@(1,3) (88.0%, 99.4%), ce_loss 0.729, lat_loss 21.819
09/28 07:23:18 PM | Train: [ 99/180] Step 400/1249 Loss 3.140 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.729, lat_loss 21.819
09/28 07:23:34 PM | Train: [ 99/180] Step 450/1249 Loss 3.196 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.729, lat_loss 21.819
09/28 07:23:50 PM | Train: [ 99/180] Step 500/1249 Loss 3.190 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.729, lat_loss 21.819
09/28 07:24:06 PM | Train: [ 99/180] Step 550/1249 Loss 3.189 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.729, lat_loss 21.819
09/28 07:24:22 PM | Train: [ 99/180] Step 600/1249 Loss 3.171 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.729, lat_loss 21.819
09/28 07:24:37 PM | Train: [ 99/180] Step 650/1249 Loss 3.170 Prec@(1,3) (88.0%, 99.4%), ce_loss 0.729, lat_loss 21.819
09/28 07:24:53 PM | Train: [ 99/180] Step 700/1249 Loss 3.175 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.729, lat_loss 21.819
09/28 07:25:09 PM | Train: [ 99/180] Step 750/1249 Loss 3.201 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.729, lat_loss 21.819
09/28 07:25:25 PM | Train: [ 99/180] Step 800/1249 Loss 3.196 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.728, lat_loss 21.819
09/28 07:25:41 PM | Train: [ 99/180] Step 850/1249 Loss 3.198 Prec@(1,3) (87.8%, 99.5%), ce_loss 0.728, lat_loss 21.819
09/28 07:25:57 PM | Train: [ 99/180] Step 900/1249 Loss 3.184 Prec@(1,3) (87.9%, 99.5%), ce_loss 0.728, lat_loss 21.819
09/28 07:26:13 PM | Train: [ 99/180] Step 950/1249 Loss 3.196 Prec@(1,3) (87.8%, 99.5%), ce_loss 0.728, lat_loss 21.819
09/28 07:26:29 PM | Train: [ 99/180] Step 1000/1249 Loss 3.194 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.728, lat_loss 21.819
09/28 07:26:45 PM | Train: [ 99/180] Step 1050/1249 Loss 3.205 Prec@(1,3) (87.8%, 99.4%), ce_loss 0.728, lat_loss 21.819
09/28 07:27:01 PM | Train: [ 99/180] Step 1100/1249 Loss 3.210 Prec@(1,3) (87.7%, 99.4%), ce_loss 0.728, lat_loss 21.819
09/28 07:27:17 PM | Train: [ 99/180] Step 1150/1249 Loss 3.208 Prec@(1,3) (87.7%, 99.4%), ce_loss 0.728, lat_loss 21.819
09/28 07:27:33 PM | Train: [ 99/180] Step 1200/1249 Loss 3.218 Prec@(1,3) (87.7%, 99.4%), ce_loss 0.728, lat_loss 21.819
09/28 07:27:48 PM | Train: [ 99/180] Step 1249/1249 Loss 3.204 Prec@(1,3) (87.7%, 99.4%), ce_loss 0.728, lat_loss 21.819
09/28 07:27:48 PM | _w_step_train: [ 99/180] Final Prec@1 87.7125% Time 399.05
09/28 07:27:48 PM | Start to train theta for epoch 98
09/28 07:28:10 PM | Train: [ 99/180] Step 050/312 Loss 3.728 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.727, lat_loss 21.818
09/28 07:28:31 PM | Train: [ 99/180] Step 100/312 Loss 4.176 Prec@(1,3) (84.2%, 99.0%), ce_loss 0.727, lat_loss 21.818
09/28 07:28:51 PM | Train: [ 99/180] Step 150/312 Loss 4.395 Prec@(1,3) (83.7%, 98.9%), ce_loss 0.727, lat_loss 21.818
09/28 07:29:11 PM | Train: [ 99/180] Step 200/312 Loss 4.457 Prec@(1,3) (83.4%, 98.9%), ce_loss 0.727, lat_loss 21.818
09/28 07:29:31 PM | Train: [ 99/180] Step 250/312 Loss 4.489 Prec@(1,3) (83.4%, 98.9%), ce_loss 0.727, lat_loss 21.818
09/28 07:29:51 PM | Train: [ 99/180] Step 300/312 Loss 4.417 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.727, lat_loss 21.818
09/28 07:29:56 PM | Train: [ 99/180] Step 312/312 Loss 4.394 Prec@(1,3) (83.7%, 99.0%), ce_loss 0.727, lat_loss 21.818
09/28 07:29:56 PM | _theta_step_train: [ 99/180] Final Prec@1 83.6600% Time 128.10
09/28 07:30:02 PM | Valid: [ 99/180] Step 050/312 Loss 4.060 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.727, lat_loss 21.818
09/28 07:30:06 PM | Valid: [ 99/180] Step 100/312 Loss 4.114 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.727, lat_loss 21.818
09/28 07:30:11 PM | Valid: [ 99/180] Step 150/312 Loss 4.375 Prec@(1,3) (83.7%, 98.7%), ce_loss 0.727, lat_loss 21.818
09/28 07:30:16 PM | Valid: [ 99/180] Step 200/312 Loss 4.306 Prec@(1,3) (84.0%, 98.8%), ce_loss 0.727, lat_loss 21.818
09/28 07:30:20 PM | Valid: [ 99/180] Step 250/312 Loss 4.203 Prec@(1,3) (84.5%, 98.9%), ce_loss 0.727, lat_loss 21.818
09/28 07:30:25 PM | Valid: [ 99/180] Step 300/312 Loss 4.142 Prec@(1,3) (84.7%, 99.0%), ce_loss 0.727, lat_loss 21.818
09/28 07:30:26 PM | Valid: [ 99/180] Step 312/312 Loss 4.169 Prec@(1,3) (84.5%, 99.0%), ce_loss 0.727, lat_loss 21.818
09/28 07:30:26 PM | val: [ 99/180] Final Prec@1 84.4700% Time 29.80
09/28 07:30:26 PM | Start to train weights for epoch 99
09/28 07:30:50 PM | Train: [100/180] Step 050/1249 Loss 3.201 Prec@(1,3) (87.3%, 99.6%), ce_loss 0.727, lat_loss 21.818
09/28 07:31:14 PM | Train: [100/180] Step 100/1249 Loss 3.249 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.727, lat_loss 21.818
09/28 07:31:38 PM | Train: [100/180] Step 150/1249 Loss 3.186 Prec@(1,3) (87.6%, 99.4%), ce_loss 0.726, lat_loss 21.818
09/28 07:32:01 PM | Train: [100/180] Step 200/1249 Loss 3.147 Prec@(1,3) (87.6%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:32:24 PM | Train: [100/180] Step 250/1249 Loss 3.158 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:32:47 PM | Train: [100/180] Step 300/1249 Loss 3.065 Prec@(1,3) (87.9%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:33:09 PM | Train: [100/180] Step 350/1249 Loss 3.089 Prec@(1,3) (87.8%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:33:28 PM | Train: [100/180] Step 400/1249 Loss 3.119 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:33:50 PM | Train: [100/180] Step 450/1249 Loss 3.118 Prec@(1,3) (87.8%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:34:11 PM | Train: [100/180] Step 500/1249 Loss 3.194 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:34:32 PM | Train: [100/180] Step 550/1249 Loss 3.194 Prec@(1,3) (87.6%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:34:52 PM | Train: [100/180] Step 600/1249 Loss 3.217 Prec@(1,3) (87.6%, 99.5%), ce_loss 0.726, lat_loss 21.818
09/28 07:35:15 PM | Train: [100/180] Step 650/1249 Loss 3.216 Prec@(1,3) (87.6%, 99.5%), ce_loss 0.725, lat_loss 21.818
09/28 07:35:40 PM | Train: [100/180] Step 700/1249 Loss 3.208 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.818
09/28 07:36:02 PM | Train: [100/180] Step 750/1249 Loss 3.198 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.818
09/28 07:36:26 PM | Train: [100/180] Step 800/1249 Loss 3.206 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.818
09/28 07:36:50 PM | Train: [100/180] Step 850/1249 Loss 3.212 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.818
09/28 07:37:14 PM | Train: [100/180] Step 900/1249 Loss 3.201 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.818
09/28 07:37:38 PM | Train: [100/180] Step 950/1249 Loss 3.199 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.818
09/28 07:38:01 PM | Train: [100/180] Step 1000/1249 Loss 3.190 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.817
09/28 07:38:23 PM | Train: [100/180] Step 1050/1249 Loss 3.199 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.817
09/28 07:38:46 PM | Train: [100/180] Step 1100/1249 Loss 3.196 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.725, lat_loss 21.817
09/28 07:39:10 PM | Train: [100/180] Step 1150/1249 Loss 3.195 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.724, lat_loss 21.817
09/28 07:39:34 PM | Train: [100/180] Step 1200/1249 Loss 3.202 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.724, lat_loss 21.817
09/28 07:39:58 PM | Train: [100/180] Step 1249/1249 Loss 3.206 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.724, lat_loss 21.817
09/28 07:39:58 PM | _w_step_train: [100/180] Final Prec@1 87.6525% Time 572.16
09/28 07:39:58 PM | Start to train theta for epoch 99
09/28 07:40:20 PM | Train: [100/180] Step 050/312 Loss 4.190 Prec@(1,3) (83.7%, 98.8%), ce_loss 0.724, lat_loss 21.817
09/28 07:40:41 PM | Train: [100/180] Step 100/312 Loss 4.000 Prec@(1,3) (84.3%, 99.2%), ce_loss 0.724, lat_loss 21.817
09/28 07:41:01 PM | Train: [100/180] Step 150/312 Loss 4.030 Prec@(1,3) (84.6%, 99.1%), ce_loss 0.724, lat_loss 21.817
09/28 07:41:21 PM | Train: [100/180] Step 200/312 Loss 4.032 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.724, lat_loss 21.817
09/28 07:41:41 PM | Train: [100/180] Step 250/312 Loss 4.062 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.724, lat_loss 21.817
09/28 07:42:01 PM | Train: [100/180] Step 300/312 Loss 4.070 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.724, lat_loss 21.817
09/28 07:42:06 PM | Train: [100/180] Step 312/312 Loss 4.061 Prec@(1,3) (84.8%, 99.0%), ce_loss 0.724, lat_loss 21.817
09/28 07:42:06 PM | _theta_step_train: [100/180] Final Prec@1 84.7600% Time 128.01
09/28 07:42:12 PM | Valid: [100/180] Step 050/312 Loss 3.789 Prec@(1,3) (85.0%, 99.3%), ce_loss 0.724, lat_loss 21.817
09/28 07:42:16 PM | Valid: [100/180] Step 100/312 Loss 3.865 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.724, lat_loss 21.817
09/28 07:42:21 PM | Valid: [100/180] Step 150/312 Loss 3.912 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.724, lat_loss 21.817
09/28 07:42:25 PM | Valid: [100/180] Step 200/312 Loss 3.942 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.724, lat_loss 21.817
09/28 07:42:30 PM | Valid: [100/180] Step 250/312 Loss 3.948 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.723, lat_loss 21.817
09/28 07:42:34 PM | Valid: [100/180] Step 300/312 Loss 3.912 Prec@(1,3) (84.7%, 99.2%), ce_loss 0.723, lat_loss 21.817
09/28 07:42:36 PM | Valid: [100/180] Step 312/312 Loss 3.932 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.723, lat_loss 21.817
09/28 07:42:36 PM | val: [100/180] Final Prec@1 84.6000% Time 29.21
09/28 07:42:36 PM | Start to train weights for epoch 100
09/28 07:43:02 PM | Train: [101/180] Step 050/1249 Loss 3.038 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.723, lat_loss 21.817
09/28 07:43:26 PM | Train: [101/180] Step 100/1249 Loss 2.927 Prec@(1,3) (88.8%, 99.6%), ce_loss 0.723, lat_loss 21.817
09/28 07:43:49 PM | Train: [101/180] Step 150/1249 Loss 2.874 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.723, lat_loss 21.817
09/28 07:44:11 PM | Train: [101/180] Step 200/1249 Loss 2.961 Prec@(1,3) (88.8%, 99.5%), ce_loss 0.723, lat_loss 21.817
09/28 07:44:34 PM | Train: [101/180] Step 250/1249 Loss 2.916 Prec@(1,3) (89.0%, 99.5%), ce_loss 0.723, lat_loss 21.817
09/28 07:44:50 PM | Train: [101/180] Step 300/1249 Loss 3.002 Prec@(1,3) (88.5%, 99.5%), ce_loss 0.723, lat_loss 21.817
09/28 07:45:06 PM | Train: [101/180] Step 350/1249 Loss 3.040 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.723, lat_loss 21.817
09/28 07:45:22 PM | Train: [101/180] Step 400/1249 Loss 3.056 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.723, lat_loss 21.817
09/28 07:45:38 PM | Train: [101/180] Step 450/1249 Loss 3.032 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.722, lat_loss 21.817
09/28 07:45:54 PM | Train: [101/180] Step 500/1249 Loss 3.011 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.722, lat_loss 21.817
09/28 07:46:10 PM | Train: [101/180] Step 550/1249 Loss 3.021 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.722, lat_loss 21.817
09/28 07:46:26 PM | Train: [101/180] Step 600/1249 Loss 3.030 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.722, lat_loss 21.817
09/28 07:46:42 PM | Train: [101/180] Step 650/1249 Loss 3.041 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.722, lat_loss 21.816
09/28 07:46:58 PM | Train: [101/180] Step 700/1249 Loss 3.039 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.722, lat_loss 21.816
09/28 07:47:14 PM | Train: [101/180] Step 750/1249 Loss 3.022 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.722, lat_loss 21.816
09/28 07:47:30 PM | Train: [101/180] Step 800/1249 Loss 3.038 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.722, lat_loss 21.816
09/28 07:47:46 PM | Train: [101/180] Step 850/1249 Loss 3.049 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.722, lat_loss 21.816
09/28 07:48:02 PM | Train: [101/180] Step 900/1249 Loss 3.082 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.722, lat_loss 21.816
09/28 07:48:18 PM | Train: [101/180] Step 950/1249 Loss 3.091 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.721, lat_loss 21.816
09/28 07:48:33 PM | Train: [101/180] Step 1000/1249 Loss 3.081 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.721, lat_loss 21.816
09/28 07:48:49 PM | Train: [101/180] Step 1050/1249 Loss 3.081 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.721, lat_loss 21.816
09/28 07:49:05 PM | Train: [101/180] Step 1100/1249 Loss 3.085 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.721, lat_loss 21.816
09/28 07:49:21 PM | Train: [101/180] Step 1150/1249 Loss 3.096 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.721, lat_loss 21.816
09/28 07:49:37 PM | Train: [101/180] Step 1200/1249 Loss 3.100 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.721, lat_loss 21.816
09/28 07:49:52 PM | Train: [101/180] Step 1249/1249 Loss 3.109 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.721, lat_loss 21.816
09/28 07:49:53 PM | _w_step_train: [101/180] Final Prec@1 88.0200% Time 436.98
09/28 07:49:53 PM | Start to train theta for epoch 100
09/28 07:50:11 PM | Train: [101/180] Step 050/312 Loss 4.038 Prec@(1,3) (84.1%, 99.2%), ce_loss 0.721, lat_loss 21.816
09/28 07:50:32 PM | Train: [101/180] Step 100/312 Loss 4.091 Prec@(1,3) (83.8%, 99.3%), ce_loss 0.721, lat_loss 21.816
09/28 07:50:51 PM | Train: [101/180] Step 150/312 Loss 4.045 Prec@(1,3) (84.3%, 99.3%), ce_loss 0.721, lat_loss 21.816
09/28 07:51:11 PM | Train: [101/180] Step 200/312 Loss 4.145 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.721, lat_loss 21.816
09/28 07:51:31 PM | Train: [101/180] Step 250/312 Loss 4.175 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.721, lat_loss 21.816
09/28 07:51:50 PM | Train: [101/180] Step 300/312 Loss 4.174 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.720, lat_loss 21.816
09/28 07:51:55 PM | Train: [101/180] Step 312/312 Loss 4.187 Prec@(1,3) (83.8%, 99.1%), ce_loss 0.720, lat_loss 21.816
09/28 07:51:55 PM | _theta_step_train: [101/180] Final Prec@1 83.7800% Time 122.76
09/28 07:52:01 PM | Valid: [101/180] Step 050/312 Loss 4.430 Prec@(1,3) (83.5%, 98.7%), ce_loss 0.720, lat_loss 21.816
09/28 07:52:06 PM | Valid: [101/180] Step 100/312 Loss 4.173 Prec@(1,3) (84.8%, 99.0%), ce_loss 0.720, lat_loss 21.816
09/28 07:52:10 PM | Valid: [101/180] Step 150/312 Loss 4.339 Prec@(1,3) (84.2%, 98.8%), ce_loss 0.720, lat_loss 21.816
09/28 07:52:15 PM | Valid: [101/180] Step 200/312 Loss 4.287 Prec@(1,3) (84.4%, 98.8%), ce_loss 0.720, lat_loss 21.816
09/28 07:52:19 PM | Valid: [101/180] Step 250/312 Loss 4.281 Prec@(1,3) (84.4%, 98.7%), ce_loss 0.720, lat_loss 21.816
09/28 07:52:24 PM | Valid: [101/180] Step 300/312 Loss 4.179 Prec@(1,3) (84.6%, 98.8%), ce_loss 0.720, lat_loss 21.816
09/28 07:52:25 PM | Valid: [101/180] Step 312/312 Loss 4.156 Prec@(1,3) (84.7%, 98.9%), ce_loss 0.720, lat_loss 21.816
09/28 07:52:25 PM | val: [101/180] Final Prec@1 84.6900% Time 29.89
09/28 07:52:25 PM | Start to train weights for epoch 101
09/28 07:52:51 PM | Train: [102/180] Step 050/1249 Loss 3.682 Prec@(1,3) (87.1%, 98.7%), ce_loss 0.720, lat_loss 21.816
09/28 07:53:14 PM | Train: [102/180] Step 100/1249 Loss 3.279 Prec@(1,3) (88.0%, 99.0%), ce_loss 0.720, lat_loss 21.816
09/28 07:53:37 PM | Train: [102/180] Step 150/1249 Loss 3.160 Prec@(1,3) (88.2%, 99.3%), ce_loss 0.720, lat_loss 21.816
09/28 07:54:00 PM | Train: [102/180] Step 200/1249 Loss 3.351 Prec@(1,3) (87.7%, 99.0%), ce_loss 0.720, lat_loss 21.816
09/28 07:54:24 PM | Train: [102/180] Step 250/1249 Loss 3.285 Prec@(1,3) (87.8%, 99.1%), ce_loss 0.720, lat_loss 21.816
09/28 07:54:47 PM | Train: [102/180] Step 300/1249 Loss 3.387 Prec@(1,3) (87.6%, 99.1%), ce_loss 0.720, lat_loss 21.816
09/28 07:55:10 PM | Train: [102/180] Step 350/1249 Loss 3.339 Prec@(1,3) (87.7%, 99.1%), ce_loss 0.719, lat_loss 21.816
09/28 07:55:33 PM | Train: [102/180] Step 400/1249 Loss 3.320 Prec@(1,3) (87.6%, 99.2%), ce_loss 0.719, lat_loss 21.816
09/28 07:55:54 PM | Train: [102/180] Step 450/1249 Loss 3.268 Prec@(1,3) (87.7%, 99.3%), ce_loss 0.719, lat_loss 21.815
09/28 07:56:17 PM | Train: [102/180] Step 500/1249 Loss 3.224 Prec@(1,3) (87.9%, 99.3%), ce_loss 0.719, lat_loss 21.815
09/28 07:56:38 PM | Train: [102/180] Step 550/1249 Loss 3.253 Prec@(1,3) (87.8%, 99.3%), ce_loss 0.719, lat_loss 21.815
09/28 07:57:01 PM | Train: [102/180] Step 600/1249 Loss 3.241 Prec@(1,3) (87.8%, 99.3%), ce_loss 0.719, lat_loss 21.815
09/28 07:57:24 PM | Train: [102/180] Step 650/1249 Loss 3.269 Prec@(1,3) (87.7%, 99.3%), ce_loss 0.719, lat_loss 21.815
09/28 07:57:47 PM | Train: [102/180] Step 700/1249 Loss 3.258 Prec@(1,3) (87.7%, 99.3%), ce_loss 0.719, lat_loss 21.815
09/28 07:58:12 PM | Train: [102/180] Step 750/1249 Loss 3.240 Prec@(1,3) (87.8%, 99.3%), ce_loss 0.719, lat_loss 21.815
09/28 07:58:35 PM | Train: [102/180] Step 800/1249 Loss 3.237 Prec@(1,3) (87.8%, 99.3%), ce_loss 0.719, lat_loss 21.815
09/28 07:58:59 PM | Train: [102/180] Step 850/1249 Loss 3.230 Prec@(1,3) (87.8%, 99.4%), ce_loss 0.718, lat_loss 21.815
09/28 07:59:22 PM | Train: [102/180] Step 900/1249 Loss 3.229 Prec@(1,3) (87.8%, 99.4%), ce_loss 0.718, lat_loss 21.815
09/28 07:59:46 PM | Train: [102/180] Step 950/1249 Loss 3.248 Prec@(1,3) (87.8%, 99.3%), ce_loss 0.718, lat_loss 21.815
09/28 08:00:10 PM | Train: [102/180] Step 1000/1249 Loss 3.233 Prec@(1,3) (87.8%, 99.4%), ce_loss 0.718, lat_loss 21.815
09/28 08:00:34 PM | Train: [102/180] Step 1050/1249 Loss 3.230 Prec@(1,3) (87.8%, 99.4%), ce_loss 0.718, lat_loss 21.815
09/28 08:00:58 PM | Train: [102/180] Step 1100/1249 Loss 3.199 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.718, lat_loss 21.815
09/28 08:01:21 PM | Train: [102/180] Step 1150/1249 Loss 3.199 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.718, lat_loss 21.815
09/28 08:01:45 PM | Train: [102/180] Step 1200/1249 Loss 3.220 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.718, lat_loss 21.815
09/28 08:02:09 PM | Train: [102/180] Step 1249/1249 Loss 3.236 Prec@(1,3) (87.8%, 99.4%), ce_loss 0.718, lat_loss 21.815
09/28 08:02:09 PM | _w_step_train: [102/180] Final Prec@1 87.8400% Time 583.86
09/28 08:02:09 PM | Start to train theta for epoch 101
09/28 08:02:30 PM | Train: [102/180] Step 050/312 Loss 4.885 Prec@(1,3) (81.6%, 98.9%), ce_loss 0.718, lat_loss 21.815
09/28 08:02:48 PM | Train: [102/180] Step 100/312 Loss 4.846 Prec@(1,3) (81.8%, 98.7%), ce_loss 0.718, lat_loss 21.815
09/28 08:03:07 PM | Train: [102/180] Step 150/312 Loss 4.648 Prec@(1,3) (82.5%, 98.9%), ce_loss 0.718, lat_loss 21.815
09/28 08:03:25 PM | Train: [102/180] Step 200/312 Loss 4.543 Prec@(1,3) (83.0%, 98.9%), ce_loss 0.717, lat_loss 21.815
09/28 08:03:44 PM | Train: [102/180] Step 250/312 Loss 4.475 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:04 PM | Train: [102/180] Step 300/312 Loss 4.429 Prec@(1,3) (83.4%, 99.0%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:09 PM | Train: [102/180] Step 312/312 Loss 4.440 Prec@(1,3) (83.3%, 99.1%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:09 PM | _theta_step_train: [102/180] Final Prec@1 83.2600% Time 119.95
09/28 08:04:14 PM | Valid: [102/180] Step 050/312 Loss 4.325 Prec@(1,3) (82.7%, 98.6%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:19 PM | Valid: [102/180] Step 100/312 Loss 4.433 Prec@(1,3) (83.4%, 98.7%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:23 PM | Valid: [102/180] Step 150/312 Loss 4.349 Prec@(1,3) (83.9%, 98.7%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:28 PM | Valid: [102/180] Step 200/312 Loss 4.253 Prec@(1,3) (84.0%, 98.8%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:33 PM | Valid: [102/180] Step 250/312 Loss 4.191 Prec@(1,3) (84.1%, 98.8%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:37 PM | Valid: [102/180] Step 300/312 Loss 4.103 Prec@(1,3) (84.4%, 98.9%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:38 PM | Valid: [102/180] Step 312/312 Loss 4.098 Prec@(1,3) (84.3%, 98.9%), ce_loss 0.717, lat_loss 21.815
09/28 08:04:38 PM | val: [102/180] Final Prec@1 84.3200% Time 29.19
09/28 08:04:39 PM | Start to train weights for epoch 102
09/28 08:05:02 PM | Train: [103/180] Step 050/1249 Loss 2.878 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.717, lat_loss 21.815
09/28 08:05:27 PM | Train: [103/180] Step 100/1249 Loss 2.913 Prec@(1,3) (88.7%, 99.6%), ce_loss 0.717, lat_loss 21.815
09/28 08:05:51 PM | Train: [103/180] Step 150/1249 Loss 2.892 Prec@(1,3) (88.8%, 99.7%), ce_loss 0.717, lat_loss 21.815
09/28 08:06:16 PM | Train: [103/180] Step 200/1249 Loss 2.860 Prec@(1,3) (88.9%, 99.7%), ce_loss 0.717, lat_loss 21.815
09/28 08:06:41 PM | Train: [103/180] Step 250/1249 Loss 2.853 Prec@(1,3) (88.8%, 99.7%), ce_loss 0.716, lat_loss 21.815
09/28 08:07:05 PM | Train: [103/180] Step 300/1249 Loss 2.892 Prec@(1,3) (88.8%, 99.7%), ce_loss 0.716, lat_loss 21.815
09/28 08:07:30 PM | Train: [103/180] Step 350/1249 Loss 3.012 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.716, lat_loss 21.814
09/28 08:07:55 PM | Train: [103/180] Step 400/1249 Loss 3.048 Prec@(1,3) (88.3%, 99.6%), ce_loss 0.716, lat_loss 21.814
09/28 08:08:20 PM | Train: [103/180] Step 450/1249 Loss 3.022 Prec@(1,3) (88.3%, 99.6%), ce_loss 0.716, lat_loss 21.814
09/28 08:08:44 PM | Train: [103/180] Step 500/1249 Loss 3.040 Prec@(1,3) (88.2%, 99.6%), ce_loss 0.716, lat_loss 21.814
09/28 08:09:08 PM | Train: [103/180] Step 550/1249 Loss 3.061 Prec@(1,3) (88.1%, 99.6%), ce_loss 0.716, lat_loss 21.814
09/28 08:09:33 PM | Train: [103/180] Step 600/1249 Loss 3.084 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.716, lat_loss 21.814
09/28 08:09:58 PM | Train: [103/180] Step 650/1249 Loss 3.118 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.716, lat_loss 21.814
09/28 08:10:22 PM | Train: [103/180] Step 700/1249 Loss 3.106 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.716, lat_loss 21.814
09/28 08:10:46 PM | Train: [103/180] Step 750/1249 Loss 3.094 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:11:11 PM | Train: [103/180] Step 800/1249 Loss 3.086 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:11:36 PM | Train: [103/180] Step 850/1249 Loss 3.096 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:12:00 PM | Train: [103/180] Step 900/1249 Loss 3.080 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:12:24 PM | Train: [103/180] Step 950/1249 Loss 3.095 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:12:49 PM | Train: [103/180] Step 1000/1249 Loss 3.093 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:13:13 PM | Train: [103/180] Step 1050/1249 Loss 3.102 Prec@(1,3) (87.9%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:13:38 PM | Train: [103/180] Step 1100/1249 Loss 3.128 Prec@(1,3) (87.8%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:14:02 PM | Train: [103/180] Step 1150/1249 Loss 3.138 Prec@(1,3) (87.8%, 99.5%), ce_loss 0.715, lat_loss 21.814
09/28 08:14:27 PM | Train: [103/180] Step 1200/1249 Loss 3.138 Prec@(1,3) (87.8%, 99.4%), ce_loss 0.715, lat_loss 21.814
09/28 08:14:51 PM | Train: [103/180] Step 1249/1249 Loss 3.128 Prec@(1,3) (87.9%, 99.4%), ce_loss 0.715, lat_loss 21.814
09/28 08:14:51 PM | _w_step_train: [103/180] Final Prec@1 87.8975% Time 612.43
09/28 08:14:51 PM | Start to train theta for epoch 102
09/28 08:15:10 PM | Train: [103/180] Step 050/312 Loss 4.904 Prec@(1,3) (82.0%, 98.8%), ce_loss 0.715, lat_loss 21.814
09/28 08:15:28 PM | Train: [103/180] Step 100/312 Loss 4.657 Prec@(1,3) (82.9%, 99.0%), ce_loss 0.714, lat_loss 21.814
09/28 08:15:46 PM | Train: [103/180] Step 150/312 Loss 4.571 Prec@(1,3) (83.1%, 99.2%), ce_loss 0.714, lat_loss 21.814
09/28 08:16:03 PM | Train: [103/180] Step 200/312 Loss 4.722 Prec@(1,3) (82.7%, 99.0%), ce_loss 0.714, lat_loss 21.814
09/28 08:16:22 PM | Train: [103/180] Step 250/312 Loss 4.583 Prec@(1,3) (82.9%, 99.1%), ce_loss 0.714, lat_loss 21.814
09/28 08:16:41 PM | Train: [103/180] Step 300/312 Loss 4.528 Prec@(1,3) (83.1%, 99.0%), ce_loss 0.714, lat_loss 21.814
09/28 08:16:46 PM | Train: [103/180] Step 312/312 Loss 4.537 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.714, lat_loss 21.814
09/28 08:16:46 PM | _theta_step_train: [103/180] Final Prec@1 83.0000% Time 115.19
09/28 08:16:52 PM | Valid: [103/180] Step 050/312 Loss 4.230 Prec@(1,3) (83.9%, 99.1%), ce_loss 0.714, lat_loss 21.814
09/28 08:16:56 PM | Valid: [103/180] Step 100/312 Loss 4.250 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.714, lat_loss 21.814
09/28 08:17:01 PM | Valid: [103/180] Step 150/312 Loss 4.447 Prec@(1,3) (83.5%, 98.9%), ce_loss 0.714, lat_loss 21.814
09/28 08:17:05 PM | Valid: [103/180] Step 200/312 Loss 4.375 Prec@(1,3) (83.6%, 99.0%), ce_loss 0.714, lat_loss 21.814
09/28 08:17:10 PM | Valid: [103/180] Step 250/312 Loss 4.293 Prec@(1,3) (84.0%, 99.0%), ce_loss 0.714, lat_loss 21.814
09/28 08:17:14 PM | Valid: [103/180] Step 300/312 Loss 4.202 Prec@(1,3) (84.2%, 99.1%), ce_loss 0.714, lat_loss 21.814
09/28 08:17:15 PM | Valid: [103/180] Step 312/312 Loss 4.191 Prec@(1,3) (84.3%, 99.2%), ce_loss 0.714, lat_loss 21.814
09/28 08:17:16 PM | val: [103/180] Final Prec@1 84.2700% Time 29.15
09/28 08:17:16 PM | Start to train weights for epoch 103
09/28 08:17:41 PM | Train: [104/180] Step 050/1249 Loss 2.835 Prec@(1,3) (89.1%, 99.8%), ce_loss 0.714, lat_loss 21.814
09/28 08:18:04 PM | Train: [104/180] Step 100/1249 Loss 2.910 Prec@(1,3) (88.7%, 99.6%), ce_loss 0.714, lat_loss 21.814
09/28 08:18:27 PM | Train: [104/180] Step 150/1249 Loss 2.996 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.714, lat_loss 21.813
09/28 08:18:51 PM | Train: [104/180] Step 200/1249 Loss 3.046 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:19:13 PM | Train: [104/180] Step 250/1249 Loss 3.005 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:19:34 PM | Train: [104/180] Step 300/1249 Loss 2.958 Prec@(1,3) (88.5%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:19:57 PM | Train: [104/180] Step 350/1249 Loss 2.965 Prec@(1,3) (88.5%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:20:19 PM | Train: [104/180] Step 400/1249 Loss 2.975 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:20:43 PM | Train: [104/180] Step 450/1249 Loss 2.997 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:21:06 PM | Train: [104/180] Step 500/1249 Loss 3.001 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:21:28 PM | Train: [104/180] Step 550/1249 Loss 2.994 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:21:50 PM | Train: [104/180] Step 600/1249 Loss 2.988 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:22:12 PM | Train: [104/180] Step 650/1249 Loss 2.972 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.713, lat_loss 21.813
09/28 08:22:35 PM | Train: [104/180] Step 700/1249 Loss 2.976 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:22:59 PM | Train: [104/180] Step 750/1249 Loss 2.963 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:23:22 PM | Train: [104/180] Step 800/1249 Loss 2.973 Prec@(1,3) (88.4%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:23:43 PM | Train: [104/180] Step 850/1249 Loss 3.001 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:24:07 PM | Train: [104/180] Step 900/1249 Loss 2.982 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:24:31 PM | Train: [104/180] Step 950/1249 Loss 2.992 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:24:55 PM | Train: [104/180] Step 1000/1249 Loss 3.021 Prec@(1,3) (88.2%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:25:17 PM | Train: [104/180] Step 1050/1249 Loss 3.039 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:25:39 PM | Train: [104/180] Step 1100/1249 Loss 3.038 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:26:02 PM | Train: [104/180] Step 1150/1249 Loss 3.067 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:26:25 PM | Train: [104/180] Step 1200/1249 Loss 3.075 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.712, lat_loss 21.813
09/28 08:26:49 PM | Train: [104/180] Step 1249/1249 Loss 3.066 Prec@(1,3) (88.0%, 99.5%), ce_loss 0.711, lat_loss 21.813
09/28 08:26:49 PM | _w_step_train: [104/180] Final Prec@1 87.9950% Time 573.58
09/28 08:26:49 PM | Start to train theta for epoch 103
09/28 08:27:11 PM | Train: [104/180] Step 050/312 Loss 4.373 Prec@(1,3) (83.0%, 99.0%), ce_loss 0.711, lat_loss 21.813
09/28 08:27:28 PM | Train: [104/180] Step 100/312 Loss 4.470 Prec@(1,3) (83.1%, 98.9%), ce_loss 0.711, lat_loss 21.813
09/28 08:27:47 PM | Train: [104/180] Step 150/312 Loss 4.457 Prec@(1,3) (83.2%, 99.0%), ce_loss 0.711, lat_loss 21.813
09/28 08:28:06 PM | Train: [104/180] Step 200/312 Loss 4.485 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.711, lat_loss 21.813
09/28 08:28:26 PM | Train: [104/180] Step 250/312 Loss 4.459 Prec@(1,3) (83.5%, 99.0%), ce_loss 0.711, lat_loss 21.813
09/28 08:28:46 PM | Train: [104/180] Step 300/312 Loss 4.448 Prec@(1,3) (83.3%, 99.1%), ce_loss 0.711, lat_loss 21.813
09/28 08:28:51 PM | Train: [104/180] Step 312/312 Loss 4.453 Prec@(1,3) (83.4%, 99.1%), ce_loss 0.711, lat_loss 21.813
09/28 08:28:51 PM | _theta_step_train: [104/180] Final Prec@1 83.3600% Time 122.32
09/28 08:28:57 PM | Valid: [104/180] Step 050/312 Loss 3.890 Prec@(1,3) (84.4%, 99.8%), ce_loss 0.711, lat_loss 21.813
09/28 08:29:01 PM | Valid: [104/180] Step 100/312 Loss 4.847 Prec@(1,3) (82.1%, 98.7%), ce_loss 0.711, lat_loss 21.813
09/28 08:29:06 PM | Valid: [104/180] Step 150/312 Loss 4.589 Prec@(1,3) (82.9%, 98.8%), ce_loss 0.711, lat_loss 21.813
09/28 08:29:11 PM | Valid: [104/180] Step 200/312 Loss 4.618 Prec@(1,3) (82.9%, 98.8%), ce_loss 0.711, lat_loss 21.813
09/28 08:29:15 PM | Valid: [104/180] Step 250/312 Loss 4.601 Prec@(1,3) (82.9%, 98.8%), ce_loss 0.711, lat_loss 21.812
09/28 08:29:20 PM | Valid: [104/180] Step 300/312 Loss 4.514 Prec@(1,3) (83.3%, 98.9%), ce_loss 0.711, lat_loss 21.812
09/28 08:29:21 PM | Valid: [104/180] Step 312/312 Loss 4.507 Prec@(1,3) (83.3%, 99.0%), ce_loss 0.711, lat_loss 21.812
09/28 08:29:21 PM | val: [104/180] Final Prec@1 83.2700% Time 29.66
09/28 08:29:21 PM | Start to train weights for epoch 104
09/28 08:29:48 PM | Train: [105/180] Step 050/1249 Loss 2.774 Prec@(1,3) (88.8%, 99.8%), ce_loss 0.711, lat_loss 21.812
09/28 08:30:13 PM | Train: [105/180] Step 100/1249 Loss 2.968 Prec@(1,3) (88.5%, 99.8%), ce_loss 0.711, lat_loss 21.812
09/28 08:30:38 PM | Train: [105/180] Step 150/1249 Loss 2.993 Prec@(1,3) (88.3%, 99.7%), ce_loss 0.711, lat_loss 21.812
09/28 08:31:03 PM | Train: [105/180] Step 200/1249 Loss 2.923 Prec@(1,3) (88.5%, 99.7%), ce_loss 0.710, lat_loss 21.812
09/28 08:31:28 PM | Train: [105/180] Step 250/1249 Loss 2.910 Prec@(1,3) (88.5%, 99.7%), ce_loss 0.710, lat_loss 21.812
09/28 08:31:53 PM | Train: [105/180] Step 300/1249 Loss 2.951 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.710, lat_loss 21.812
09/28 08:32:09 PM | Train: [105/180] Step 350/1249 Loss 2.973 Prec@(1,3) (88.4%, 99.6%), ce_loss 0.710, lat_loss 21.812
09/28 08:32:25 PM | Train: [105/180] Step 400/1249 Loss 2.941 Prec@(1,3) (88.6%, 99.6%), ce_loss 0.710, lat_loss 21.812
09/28 08:32:41 PM | Train: [105/180] Step 450/1249 Loss 2.968 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.710, lat_loss 21.812
09/28 08:32:57 PM | Train: [105/180] Step 500/1249 Loss 2.988 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.710, lat_loss 21.812
09/28 08:33:13 PM | Train: [105/180] Step 550/1249 Loss 2.995 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.710, lat_loss 21.812
09/28 08:33:29 PM | Train: [105/180] Step 600/1249 Loss 2.968 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.710, lat_loss 21.812
09/28 08:33:45 PM | Train: [105/180] Step 650/1249 Loss 3.013 Prec@(1,3) (88.3%, 99.5%), ce_loss 0.710, lat_loss 21.812
09/28 08:34:01 PM | Train: [105/180] Step 700/1249 Loss 2.997 Prec@(1,3) (88.4%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:34:17 PM | Train: [105/180] Step 750/1249 Loss 2.999 Prec@(1,3) (88.5%, 99.5%), ce_loss 0.709, lat_loss 21.812
09/28 08:34:33 PM | Train: [105/180] Step 800/1249 Loss 3.005 Prec@(1,3) (88.4%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:34:49 PM | Train: [105/180] Step 850/1249 Loss 3.005 Prec@(1,3) (88.4%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:35:05 PM | Train: [105/180] Step 900/1249 Loss 3.004 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:35:21 PM | Train: [105/180] Step 950/1249 Loss 2.993 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:35:37 PM | Train: [105/180] Step 1000/1249 Loss 2.990 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:35:52 PM | Train: [105/180] Step 1050/1249 Loss 2.983 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:36:08 PM | Train: [105/180] Step 1100/1249 Loss 2.977 Prec@(1,3) (88.6%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:36:24 PM | Train: [105/180] Step 1150/1249 Loss 2.986 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.709, lat_loss 21.812
09/28 08:36:40 PM | Train: [105/180] Step 1200/1249 Loss 2.981 Prec@(1,3) (88.6%, 99.6%), ce_loss 0.708, lat_loss 21.812
09/28 08:36:57 PM | Train: [105/180] Step 1249/1249 Loss 2.975 Prec@(1,3) (88.6%, 99.6%), ce_loss 0.708, lat_loss 21.812
09/28 08:36:57 PM | _w_step_train: [105/180] Final Prec@1 88.5825% Time 455.51
09/28 08:36:57 PM | Start to train theta for epoch 104
09/28 08:37:19 PM | Train: [105/180] Step 050/312 Loss 4.193 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.708, lat_loss 21.812
09/28 08:37:39 PM | Train: [105/180] Step 100/312 Loss 4.041 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.708, lat_loss 21.812
09/28 08:38:00 PM | Train: [105/180] Step 150/312 Loss 4.078 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.708, lat_loss 21.812
09/28 08:38:21 PM | Train: [105/180] Step 200/312 Loss 4.044 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.708, lat_loss 21.812
09/28 08:38:42 PM | Train: [105/180] Step 250/312 Loss 4.087 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.708, lat_loss 21.812
09/28 08:39:02 PM | Train: [105/180] Step 300/312 Loss 4.041 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:07 PM | Train: [105/180] Step 312/312 Loss 4.065 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:07 PM | _theta_step_train: [105/180] Final Prec@1 84.7200% Time 130.73
09/28 08:39:13 PM | Valid: [105/180] Step 050/312 Loss 3.560 Prec@(1,3) (86.1%, 99.8%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:17 PM | Valid: [105/180] Step 100/312 Loss 3.905 Prec@(1,3) (85.1%, 99.4%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:22 PM | Valid: [105/180] Step 150/312 Loss 4.055 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:26 PM | Valid: [105/180] Step 200/312 Loss 3.976 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:31 PM | Valid: [105/180] Step 250/312 Loss 3.914 Prec@(1,3) (84.8%, 99.2%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:36 PM | Valid: [105/180] Step 300/312 Loss 3.858 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:37 PM | Valid: [105/180] Step 312/312 Loss 3.843 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.708, lat_loss 21.811
09/28 08:39:37 PM | val: [105/180] Final Prec@1 85.1000% Time 29.42
09/28 08:39:37 PM | Best top1 acc by now. Save model
09/28 08:39:37 PM | Start to train weights for epoch 105
09/28 08:40:03 PM | Train: [106/180] Step 050/1249 Loss 2.209 Prec@(1,3) (91.9%, 99.4%), ce_loss 0.707, lat_loss 21.811
09/28 08:40:26 PM | Train: [106/180] Step 100/1249 Loss 2.602 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.707, lat_loss 21.811
09/28 08:40:51 PM | Train: [106/180] Step 150/1249 Loss 2.579 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.707, lat_loss 21.811
09/28 08:41:16 PM | Train: [106/180] Step 200/1249 Loss 2.620 Prec@(1,3) (89.8%, 99.7%), ce_loss 0.707, lat_loss 21.811
09/28 08:41:41 PM | Train: [106/180] Step 250/1249 Loss 2.690 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.707, lat_loss 21.811
09/28 08:42:06 PM | Train: [106/180] Step 300/1249 Loss 2.715 Prec@(1,3) (89.4%, 99.7%), ce_loss 0.707, lat_loss 21.811
09/28 08:42:31 PM | Train: [106/180] Step 350/1249 Loss 2.727 Prec@(1,3) (89.4%, 99.7%), ce_loss 0.707, lat_loss 21.811
09/28 08:42:55 PM | Train: [106/180] Step 400/1249 Loss 2.790 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.707, lat_loss 21.811
09/28 08:43:20 PM | Train: [106/180] Step 450/1249 Loss 2.801 Prec@(1,3) (89.1%, 99.7%), ce_loss 0.707, lat_loss 21.811
09/28 08:43:45 PM | Train: [106/180] Step 500/1249 Loss 2.834 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.707, lat_loss 21.811
09/28 08:44:09 PM | Train: [106/180] Step 550/1249 Loss 2.799 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:44:33 PM | Train: [106/180] Step 600/1249 Loss 2.827 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:44:57 PM | Train: [106/180] Step 650/1249 Loss 2.827 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:45:22 PM | Train: [106/180] Step 700/1249 Loss 2.835 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:45:37 PM | Train: [106/180] Step 750/1249 Loss 2.839 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:45:51 PM | Train: [106/180] Step 800/1249 Loss 2.856 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:46:06 PM | Train: [106/180] Step 850/1249 Loss 2.866 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:46:20 PM | Train: [106/180] Step 900/1249 Loss 2.864 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:46:35 PM | Train: [106/180] Step 950/1249 Loss 2.879 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:46:49 PM | Train: [106/180] Step 1000/1249 Loss 2.902 Prec@(1,3) (88.8%, 99.6%), ce_loss 0.706, lat_loss 21.811
09/28 08:47:04 PM | Train: [106/180] Step 1050/1249 Loss 2.900 Prec@(1,3) (88.8%, 99.6%), ce_loss 0.705, lat_loss 21.811
09/28 08:47:18 PM | Train: [106/180] Step 1100/1249 Loss 2.879 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.705, lat_loss 21.811
09/28 08:47:33 PM | Train: [106/180] Step 1150/1249 Loss 2.886 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.705, lat_loss 21.811
09/28 08:47:47 PM | Train: [106/180] Step 1200/1249 Loss 2.890 Prec@(1,3) (88.8%, 99.6%), ce_loss 0.705, lat_loss 21.811
09/28 08:48:01 PM | Train: [106/180] Step 1249/1249 Loss 2.893 Prec@(1,3) (88.8%, 99.6%), ce_loss 0.705, lat_loss 21.811
09/28 08:48:01 PM | _w_step_train: [106/180] Final Prec@1 88.8225% Time 504.44
09/28 08:48:01 PM | Start to train theta for epoch 105
09/28 08:48:23 PM | Train: [106/180] Step 050/312 Loss 4.031 Prec@(1,3) (85.2%, 99.1%), ce_loss 0.705, lat_loss 21.811
09/28 08:48:42 PM | Train: [106/180] Step 100/312 Loss 4.122 Prec@(1,3) (84.5%, 99.0%), ce_loss 0.705, lat_loss 21.811
09/28 08:49:00 PM | Train: [106/180] Step 150/312 Loss 4.095 Prec@(1,3) (84.4%, 99.0%), ce_loss 0.705, lat_loss 21.810
09/28 08:49:19 PM | Train: [106/180] Step 200/312 Loss 4.098 Prec@(1,3) (84.5%, 99.0%), ce_loss 0.705, lat_loss 21.810
09/28 08:49:38 PM | Train: [106/180] Step 250/312 Loss 4.072 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.705, lat_loss 21.810
09/28 08:49:58 PM | Train: [106/180] Step 300/312 Loss 4.149 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.705, lat_loss 21.810
09/28 08:50:03 PM | Train: [106/180] Step 312/312 Loss 4.161 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.705, lat_loss 21.810
09/28 08:50:04 PM | _theta_step_train: [106/180] Final Prec@1 84.4100% Time 122.19
09/28 08:50:09 PM | Valid: [106/180] Step 050/312 Loss 4.879 Prec@(1,3) (84.6%, 98.5%), ce_loss 0.705, lat_loss 21.810
09/28 08:50:14 PM | Valid: [106/180] Step 100/312 Loss 4.610 Prec@(1,3) (84.4%, 98.6%), ce_loss 0.705, lat_loss 21.810
09/28 08:50:19 PM | Valid: [106/180] Step 150/312 Loss 4.409 Prec@(1,3) (84.8%, 98.7%), ce_loss 0.705, lat_loss 21.810
09/28 08:50:23 PM | Valid: [106/180] Step 200/312 Loss 4.284 Prec@(1,3) (84.9%, 98.9%), ce_loss 0.705, lat_loss 21.810
09/28 08:50:28 PM | Valid: [106/180] Step 250/312 Loss 4.227 Prec@(1,3) (84.8%, 99.0%), ce_loss 0.704, lat_loss 21.810
09/28 08:50:32 PM | Valid: [106/180] Step 300/312 Loss 4.165 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.704, lat_loss 21.810
09/28 08:50:34 PM | Valid: [106/180] Step 312/312 Loss 4.134 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.704, lat_loss 21.810
09/28 08:50:34 PM | val: [106/180] Final Prec@1 84.9500% Time 29.97
09/28 08:50:34 PM | Start to train weights for epoch 106
09/28 08:51:00 PM | Train: [107/180] Step 050/1249 Loss 2.975 Prec@(1,3) (88.5%, 99.4%), ce_loss 0.704, lat_loss 21.810
09/28 08:51:24 PM | Train: [107/180] Step 100/1249 Loss 2.787 Prec@(1,3) (89.5%, 99.4%), ce_loss 0.704, lat_loss 21.810
09/28 08:51:46 PM | Train: [107/180] Step 150/1249 Loss 2.796 Prec@(1,3) (89.4%, 99.5%), ce_loss 0.704, lat_loss 21.810
09/28 08:52:11 PM | Train: [107/180] Step 200/1249 Loss 2.808 Prec@(1,3) (89.3%, 99.5%), ce_loss 0.704, lat_loss 21.810
09/28 08:52:34 PM | Train: [107/180] Step 250/1249 Loss 2.782 Prec@(1,3) (89.3%, 99.5%), ce_loss 0.704, lat_loss 21.810
09/28 08:52:59 PM | Train: [107/180] Step 300/1249 Loss 2.782 Prec@(1,3) (89.3%, 99.5%), ce_loss 0.704, lat_loss 21.810
09/28 08:53:23 PM | Train: [107/180] Step 350/1249 Loss 2.817 Prec@(1,3) (89.1%, 99.5%), ce_loss 0.704, lat_loss 21.810
09/28 08:53:48 PM | Train: [107/180] Step 400/1249 Loss 2.776 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.704, lat_loss 21.810
09/28 08:54:11 PM | Train: [107/180] Step 450/1249 Loss 2.799 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:54:35 PM | Train: [107/180] Step 500/1249 Loss 2.805 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:54:59 PM | Train: [107/180] Step 550/1249 Loss 2.821 Prec@(1,3) (89.1%, 99.5%), ce_loss 0.703, lat_loss 21.810
09/28 08:55:23 PM | Train: [107/180] Step 600/1249 Loss 2.820 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:55:47 PM | Train: [107/180] Step 650/1249 Loss 2.804 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:56:11 PM | Train: [107/180] Step 700/1249 Loss 2.803 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:56:36 PM | Train: [107/180] Step 750/1249 Loss 2.797 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:57:00 PM | Train: [107/180] Step 800/1249 Loss 2.794 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:57:23 PM | Train: [107/180] Step 850/1249 Loss 2.783 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:57:47 PM | Train: [107/180] Step 900/1249 Loss 2.773 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.703, lat_loss 21.810
09/28 08:58:11 PM | Train: [107/180] Step 950/1249 Loss 2.759 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.702, lat_loss 21.810
09/28 08:58:35 PM | Train: [107/180] Step 1000/1249 Loss 2.769 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.702, lat_loss 21.810
09/28 08:59:00 PM | Train: [107/180] Step 1050/1249 Loss 2.776 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.702, lat_loss 21.810
09/28 08:59:24 PM | Train: [107/180] Step 1100/1249 Loss 2.766 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.702, lat_loss 21.810
09/28 08:59:48 PM | Train: [107/180] Step 1150/1249 Loss 2.773 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.702, lat_loss 21.810
09/28 09:00:12 PM | Train: [107/180] Step 1200/1249 Loss 2.773 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.702, lat_loss 21.810
09/28 09:00:36 PM | Train: [107/180] Step 1249/1249 Loss 2.766 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.702, lat_loss 21.809
09/28 09:00:36 PM | _w_step_train: [107/180] Final Prec@1 89.2050% Time 602.48
09/28 09:00:36 PM | Start to train theta for epoch 106
09/28 09:00:58 PM | Train: [107/180] Step 050/312 Loss 3.965 Prec@(1,3) (85.0%, 98.9%), ce_loss 0.702, lat_loss 21.809
09/28 09:01:17 PM | Train: [107/180] Step 100/312 Loss 4.039 Prec@(1,3) (84.9%, 99.0%), ce_loss 0.702, lat_loss 21.809
09/28 09:01:38 PM | Train: [107/180] Step 150/312 Loss 3.952 Prec@(1,3) (85.0%, 99.0%), ce_loss 0.702, lat_loss 21.809
09/28 09:01:58 PM | Train: [107/180] Step 200/312 Loss 3.977 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.702, lat_loss 21.809
09/28 09:02:19 PM | Train: [107/180] Step 250/312 Loss 4.031 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.702, lat_loss 21.809
09/28 09:02:39 PM | Train: [107/180] Step 300/312 Loss 4.011 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.702, lat_loss 21.809
09/28 09:02:45 PM | Train: [107/180] Step 312/312 Loss 4.006 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.701, lat_loss 21.809
09/28 09:02:45 PM | _theta_step_train: [107/180] Final Prec@1 84.8800% Time 128.98
09/28 09:02:51 PM | Valid: [107/180] Step 050/312 Loss 3.616 Prec@(1,3) (86.5%, 99.6%), ce_loss 0.701, lat_loss 21.809
09/28 09:02:55 PM | Valid: [107/180] Step 100/312 Loss 3.840 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.701, lat_loss 21.809
09/28 09:03:00 PM | Valid: [107/180] Step 150/312 Loss 3.854 Prec@(1,3) (85.8%, 99.1%), ce_loss 0.701, lat_loss 21.809
09/28 09:03:04 PM | Valid: [107/180] Step 200/312 Loss 3.860 Prec@(1,3) (85.8%, 99.2%), ce_loss 0.701, lat_loss 21.809
09/28 09:03:09 PM | Valid: [107/180] Step 250/312 Loss 3.845 Prec@(1,3) (85.8%, 99.2%), ce_loss 0.701, lat_loss 21.809
09/28 09:03:14 PM | Valid: [107/180] Step 300/312 Loss 3.791 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.701, lat_loss 21.809
09/28 09:03:15 PM | Valid: [107/180] Step 312/312 Loss 3.781 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.701, lat_loss 21.809
09/28 09:03:15 PM | val: [107/180] Final Prec@1 85.8000% Time 29.61
09/28 09:03:15 PM | Best top1 acc by now. Save model
09/28 09:03:15 PM | Start to train weights for epoch 107
09/28 09:03:41 PM | Train: [108/180] Step 050/1249 Loss 2.840 Prec@(1,3) (88.7%, 99.6%), ce_loss 0.701, lat_loss 21.809
09/28 09:04:06 PM | Train: [108/180] Step 100/1249 Loss 2.670 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.701, lat_loss 21.809
09/28 09:04:30 PM | Train: [108/180] Step 150/1249 Loss 2.766 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.701, lat_loss 21.809
09/28 09:04:52 PM | Train: [108/180] Step 200/1249 Loss 2.713 Prec@(1,3) (89.5%, 99.6%), ce_loss 0.701, lat_loss 21.809
09/28 09:05:16 PM | Train: [108/180] Step 250/1249 Loss 2.689 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.701, lat_loss 21.809
09/28 09:05:40 PM | Train: [108/180] Step 300/1249 Loss 2.745 Prec@(1,3) (89.2%, 99.7%), ce_loss 0.700, lat_loss 21.809
09/28 09:06:04 PM | Train: [108/180] Step 350/1249 Loss 2.820 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:06:28 PM | Train: [108/180] Step 400/1249 Loss 2.839 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:06:53 PM | Train: [108/180] Step 450/1249 Loss 2.824 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:07:18 PM | Train: [108/180] Step 500/1249 Loss 2.881 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:07:42 PM | Train: [108/180] Step 550/1249 Loss 2.857 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:08:07 PM | Train: [108/180] Step 600/1249 Loss 2.830 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:08:28 PM | Train: [108/180] Step 650/1249 Loss 2.796 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:08:43 PM | Train: [108/180] Step 700/1249 Loss 2.815 Prec@(1,3) (89.1%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:08:59 PM | Train: [108/180] Step 750/1249 Loss 2.832 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:09:15 PM | Train: [108/180] Step 800/1249 Loss 2.837 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.700, lat_loss 21.809
09/28 09:09:31 PM | Train: [108/180] Step 850/1249 Loss 2.820 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.699, lat_loss 21.809
09/28 09:09:47 PM | Train: [108/180] Step 900/1249 Loss 2.829 Prec@(1,3) (89.0%, 99.6%), ce_loss 0.699, lat_loss 21.809
09/28 09:10:03 PM | Train: [108/180] Step 950/1249 Loss 2.835 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.699, lat_loss 21.809
09/28 09:10:19 PM | Train: [108/180] Step 1000/1249 Loss 2.837 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.699, lat_loss 21.809
09/28 09:10:35 PM | Train: [108/180] Step 1050/1249 Loss 2.837 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.699, lat_loss 21.809
09/28 09:10:51 PM | Train: [108/180] Step 1100/1249 Loss 2.835 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.699, lat_loss 21.808
09/28 09:11:07 PM | Train: [108/180] Step 1150/1249 Loss 2.834 Prec@(1,3) (88.9%, 99.6%), ce_loss 0.699, lat_loss 21.808
09/28 09:11:23 PM | Train: [108/180] Step 1200/1249 Loss 2.856 Prec@(1,3) (88.8%, 99.6%), ce_loss 0.699, lat_loss 21.808
09/28 09:11:39 PM | Train: [108/180] Step 1249/1249 Loss 2.856 Prec@(1,3) (88.8%, 99.6%), ce_loss 0.699, lat_loss 21.808
09/28 09:11:39 PM | _w_step_train: [108/180] Final Prec@1 88.8050% Time 503.78
09/28 09:11:39 PM | Start to train theta for epoch 107
09/28 09:12:00 PM | Train: [108/180] Step 050/312 Loss 4.692 Prec@(1,3) (82.5%, 99.3%), ce_loss 0.699, lat_loss 21.808
09/28 09:12:20 PM | Train: [108/180] Step 100/312 Loss 4.198 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.699, lat_loss 21.808
09/28 09:12:40 PM | Train: [108/180] Step 150/312 Loss 4.175 Prec@(1,3) (84.7%, 99.1%), ce_loss 0.699, lat_loss 21.808
09/28 09:13:01 PM | Train: [108/180] Step 200/312 Loss 4.261 Prec@(1,3) (84.5%, 99.0%), ce_loss 0.698, lat_loss 21.808
09/28 09:13:21 PM | Train: [108/180] Step 250/312 Loss 4.237 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.698, lat_loss 21.808
09/28 09:13:41 PM | Train: [108/180] Step 300/312 Loss 4.227 Prec@(1,3) (84.6%, 99.0%), ce_loss 0.698, lat_loss 21.808
09/28 09:13:46 PM | Train: [108/180] Step 312/312 Loss 4.228 Prec@(1,3) (84.5%, 99.0%), ce_loss 0.698, lat_loss 21.808
09/28 09:13:46 PM | _theta_step_train: [108/180] Final Prec@1 84.5300% Time 127.14
09/28 09:13:52 PM | Valid: [108/180] Step 050/312 Loss 3.580 Prec@(1,3) (86.5%, 99.5%), ce_loss 0.698, lat_loss 21.808
09/28 09:13:57 PM | Valid: [108/180] Step 100/312 Loss 3.785 Prec@(1,3) (85.9%, 99.4%), ce_loss 0.698, lat_loss 21.808
09/28 09:14:01 PM | Valid: [108/180] Step 150/312 Loss 3.880 Prec@(1,3) (85.7%, 99.3%), ce_loss 0.698, lat_loss 21.808
09/28 09:14:06 PM | Valid: [108/180] Step 200/312 Loss 3.921 Prec@(1,3) (85.6%, 99.3%), ce_loss 0.698, lat_loss 21.808
09/28 09:14:10 PM | Valid: [108/180] Step 250/312 Loss 4.130 Prec@(1,3) (85.3%, 99.1%), ce_loss 0.698, lat_loss 21.808
09/28 09:14:15 PM | Valid: [108/180] Step 300/312 Loss 4.051 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.698, lat_loss 21.808
09/28 09:14:16 PM | Valid: [108/180] Step 312/312 Loss 4.056 Prec@(1,3) (85.2%, 99.2%), ce_loss 0.698, lat_loss 21.808
09/28 09:14:16 PM | val: [108/180] Final Prec@1 85.2400% Time 29.83
09/28 09:14:16 PM | Start to train weights for epoch 108
09/28 09:14:43 PM | Train: [109/180] Step 050/1249 Loss 2.503 Prec@(1,3) (90.1%, 99.6%), ce_loss 0.698, lat_loss 21.808
09/28 09:15:08 PM | Train: [109/180] Step 100/1249 Loss 2.537 Prec@(1,3) (90.2%, 99.6%), ce_loss 0.698, lat_loss 21.808
09/28 09:15:33 PM | Train: [109/180] Step 150/1249 Loss 2.810 Prec@(1,3) (89.9%, 99.5%), ce_loss 0.698, lat_loss 21.808
09/28 09:15:57 PM | Train: [109/180] Step 200/1249 Loss 2.811 Prec@(1,3) (89.7%, 99.5%), ce_loss 0.698, lat_loss 21.808
09/28 09:16:23 PM | Train: [109/180] Step 250/1249 Loss 2.768 Prec@(1,3) (89.7%, 99.6%), ce_loss 0.698, lat_loss 21.808
09/28 09:16:48 PM | Train: [109/180] Step 300/1249 Loss 2.771 Prec@(1,3) (89.6%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:17:13 PM | Train: [109/180] Step 350/1249 Loss 2.798 Prec@(1,3) (89.4%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:17:38 PM | Train: [109/180] Step 400/1249 Loss 2.783 Prec@(1,3) (89.4%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:18:02 PM | Train: [109/180] Step 450/1249 Loss 2.728 Prec@(1,3) (89.5%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:18:27 PM | Train: [109/180] Step 500/1249 Loss 2.751 Prec@(1,3) (89.4%, 99.7%), ce_loss 0.697, lat_loss 21.808
09/28 09:18:52 PM | Train: [109/180] Step 550/1249 Loss 2.784 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:19:17 PM | Train: [109/180] Step 600/1249 Loss 2.781 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:19:41 PM | Train: [109/180] Step 650/1249 Loss 2.792 Prec@(1,3) (89.4%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:20:06 PM | Train: [109/180] Step 700/1249 Loss 2.799 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:20:31 PM | Train: [109/180] Step 750/1249 Loss 2.800 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.697, lat_loss 21.808
09/28 09:20:56 PM | Train: [109/180] Step 800/1249 Loss 2.807 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.696, lat_loss 21.808
09/28 09:21:20 PM | Train: [109/180] Step 850/1249 Loss 2.789 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.696, lat_loss 21.808
09/28 09:21:43 PM | Train: [109/180] Step 900/1249 Loss 2.786 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.696, lat_loss 21.808
09/28 09:22:07 PM | Train: [109/180] Step 950/1249 Loss 2.783 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.696, lat_loss 21.808
09/28 09:22:31 PM | Train: [109/180] Step 1000/1249 Loss 2.766 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.696, lat_loss 21.808
09/28 09:22:55 PM | Train: [109/180] Step 1050/1249 Loss 2.776 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.696, lat_loss 21.808
09/28 09:23:18 PM | Train: [109/180] Step 1100/1249 Loss 2.774 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.696, lat_loss 21.807
09/28 09:23:42 PM | Train: [109/180] Step 1150/1249 Loss 2.772 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.696, lat_loss 21.807
09/28 09:24:07 PM | Train: [109/180] Step 1200/1249 Loss 2.767 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.696, lat_loss 21.807
09/28 09:24:31 PM | Train: [109/180] Step 1249/1249 Loss 2.782 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.696, lat_loss 21.807
09/28 09:24:31 PM | _w_step_train: [109/180] Final Prec@1 89.1950% Time 615.18
09/28 09:24:31 PM | Start to train theta for epoch 108
09/28 09:24:44 PM | Train: [109/180] Step 050/312 Loss 4.315 Prec@(1,3) (84.1%, 99.4%), ce_loss 0.696, lat_loss 21.807
09/28 09:24:57 PM | Train: [109/180] Step 100/312 Loss 4.230 Prec@(1,3) (83.8%, 99.3%), ce_loss 0.695, lat_loss 21.807
09/28 09:25:09 PM | Train: [109/180] Step 150/312 Loss 4.084 Prec@(1,3) (84.6%, 99.3%), ce_loss 0.695, lat_loss 21.807
09/28 09:25:21 PM | Train: [109/180] Step 200/312 Loss 4.068 Prec@(1,3) (84.8%, 99.3%), ce_loss 0.695, lat_loss 21.807
09/28 09:25:34 PM | Train: [109/180] Step 250/312 Loss 4.141 Prec@(1,3) (84.5%, 99.3%), ce_loss 0.695, lat_loss 21.807
09/28 09:25:46 PM | Train: [109/180] Step 300/312 Loss 4.108 Prec@(1,3) (84.8%, 99.3%), ce_loss 0.695, lat_loss 21.807
09/28 09:25:49 PM | Train: [109/180] Step 312/312 Loss 4.072 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.695, lat_loss 21.807
09/28 09:25:49 PM | _theta_step_train: [109/180] Final Prec@1 84.8500% Time 77.92
09/28 09:25:55 PM | Valid: [109/180] Step 050/312 Loss 3.753 Prec@(1,3) (85.4%, 99.4%), ce_loss 0.695, lat_loss 21.807
09/28 09:25:59 PM | Valid: [109/180] Step 100/312 Loss 3.904 Prec@(1,3) (84.8%, 99.2%), ce_loss 0.695, lat_loss 21.807
09/28 09:26:04 PM | Valid: [109/180] Step 150/312 Loss 3.911 Prec@(1,3) (84.8%, 98.9%), ce_loss 0.695, lat_loss 21.807
09/28 09:26:09 PM | Valid: [109/180] Step 200/312 Loss 3.856 Prec@(1,3) (84.9%, 99.0%), ce_loss 0.695, lat_loss 21.807
09/28 09:26:13 PM | Valid: [109/180] Step 250/312 Loss 3.808 Prec@(1,3) (85.2%, 99.1%), ce_loss 0.695, lat_loss 21.807
09/28 09:26:18 PM | Valid: [109/180] Step 300/312 Loss 3.730 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.695, lat_loss 21.807
09/28 09:26:19 PM | Valid: [109/180] Step 312/312 Loss 3.721 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.695, lat_loss 21.807
09/28 09:26:19 PM | val: [109/180] Final Prec@1 85.3700% Time 29.87
09/28 09:26:19 PM | Start to train weights for epoch 109
09/28 09:26:36 PM | Train: [110/180] Step 050/1249 Loss 2.525 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.695, lat_loss 21.807
09/28 09:26:52 PM | Train: [110/180] Step 100/1249 Loss 2.580 Prec@(1,3) (89.2%, 99.7%), ce_loss 0.695, lat_loss 21.807
09/28 09:27:08 PM | Train: [110/180] Step 150/1249 Loss 2.580 Prec@(1,3) (89.3%, 99.7%), ce_loss 0.695, lat_loss 21.807
09/28 09:27:24 PM | Train: [110/180] Step 200/1249 Loss 2.477 Prec@(1,3) (89.8%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:27:40 PM | Train: [110/180] Step 250/1249 Loss 2.543 Prec@(1,3) (89.8%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:27:56 PM | Train: [110/180] Step 300/1249 Loss 2.535 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:28:12 PM | Train: [110/180] Step 350/1249 Loss 2.571 Prec@(1,3) (89.7%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:28:28 PM | Train: [110/180] Step 400/1249 Loss 2.596 Prec@(1,3) (89.7%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:28:43 PM | Train: [110/180] Step 450/1249 Loss 2.638 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:28:59 PM | Train: [110/180] Step 500/1249 Loss 2.658 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:29:15 PM | Train: [110/180] Step 550/1249 Loss 2.656 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:29:31 PM | Train: [110/180] Step 600/1249 Loss 2.649 Prec@(1,3) (89.5%, 99.6%), ce_loss 0.694, lat_loss 21.807
09/28 09:29:47 PM | Train: [110/180] Step 650/1249 Loss 2.641 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.694, lat_loss 21.807
09/28 09:30:03 PM | Train: [110/180] Step 700/1249 Loss 2.646 Prec@(1,3) (89.5%, 99.6%), ce_loss 0.693, lat_loss 21.807
09/28 09:30:19 PM | Train: [110/180] Step 750/1249 Loss 2.661 Prec@(1,3) (89.5%, 99.6%), ce_loss 0.693, lat_loss 21.807
09/28 09:30:35 PM | Train: [110/180] Step 800/1249 Loss 2.679 Prec@(1,3) (89.4%, 99.6%), ce_loss 0.693, lat_loss 21.807
09/28 09:30:51 PM | Train: [110/180] Step 850/1249 Loss 2.704 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.693, lat_loss 21.807
09/28 09:31:07 PM | Train: [110/180] Step 900/1249 Loss 2.719 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.693, lat_loss 21.807
09/28 09:31:23 PM | Train: [110/180] Step 950/1249 Loss 2.733 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.693, lat_loss 21.807
09/28 09:31:39 PM | Train: [110/180] Step 1000/1249 Loss 2.737 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.693, lat_loss 21.807
09/28 09:31:54 PM | Train: [110/180] Step 1050/1249 Loss 2.733 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.693, lat_loss 21.807
09/28 09:32:10 PM | Train: [110/180] Step 1100/1249 Loss 2.727 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.693, lat_loss 21.806
09/28 09:32:26 PM | Train: [110/180] Step 1150/1249 Loss 2.735 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.693, lat_loss 21.806
09/28 09:32:42 PM | Train: [110/180] Step 1200/1249 Loss 2.735 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.693, lat_loss 21.806
09/28 09:32:58 PM | Train: [110/180] Step 1249/1249 Loss 2.730 Prec@(1,3) (89.2%, 99.6%), ce_loss 0.692, lat_loss 21.806
09/28 09:32:58 PM | _w_step_train: [110/180] Final Prec@1 89.2475% Time 398.97
09/28 09:32:58 PM | Start to train theta for epoch 109
09/28 09:33:12 PM | Train: [110/180] Step 050/312 Loss 3.825 Prec@(1,3) (85.3%, 99.3%), ce_loss 0.692, lat_loss 21.806
09/28 09:33:24 PM | Train: [110/180] Step 100/312 Loss 3.974 Prec@(1,3) (85.0%, 99.4%), ce_loss 0.692, lat_loss 21.806
09/28 09:33:36 PM | Train: [110/180] Step 150/312 Loss 4.056 Prec@(1,3) (84.7%, 99.4%), ce_loss 0.692, lat_loss 21.806
09/28 09:33:48 PM | Train: [110/180] Step 200/312 Loss 4.170 Prec@(1,3) (84.6%, 99.4%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:01 PM | Train: [110/180] Step 250/312 Loss 4.114 Prec@(1,3) (84.7%, 99.4%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:13 PM | Train: [110/180] Step 300/312 Loss 4.110 Prec@(1,3) (84.6%, 99.4%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:16 PM | Train: [110/180] Step 312/312 Loss 4.138 Prec@(1,3) (84.5%, 99.4%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:16 PM | _theta_step_train: [110/180] Final Prec@1 84.4800% Time 77.93
09/28 09:34:21 PM | Valid: [110/180] Step 050/312 Loss 4.395 Prec@(1,3) (83.1%, 98.3%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:26 PM | Valid: [110/180] Step 100/312 Loss 4.552 Prec@(1,3) (84.0%, 98.3%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:30 PM | Valid: [110/180] Step 150/312 Loss 4.461 Prec@(1,3) (84.3%, 98.3%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:35 PM | Valid: [110/180] Step 200/312 Loss 4.303 Prec@(1,3) (84.7%, 98.6%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:40 PM | Valid: [110/180] Step 250/312 Loss 4.172 Prec@(1,3) (85.0%, 98.7%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:44 PM | Valid: [110/180] Step 300/312 Loss 4.193 Prec@(1,3) (84.9%, 98.8%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:45 PM | Valid: [110/180] Step 312/312 Loss 4.167 Prec@(1,3) (85.0%, 98.8%), ce_loss 0.692, lat_loss 21.806
09/28 09:34:45 PM | val: [110/180] Final Prec@1 84.9700% Time 29.51
09/28 09:34:45 PM | Start to train weights for epoch 110
09/28 09:35:12 PM | Train: [111/180] Step 050/1249 Loss 2.717 Prec@(1,3) (89.1%, 99.8%), ce_loss 0.692, lat_loss 21.806
09/28 09:35:37 PM | Train: [111/180] Step 100/1249 Loss 2.697 Prec@(1,3) (89.1%, 99.8%), ce_loss 0.692, lat_loss 21.806
09/28 09:36:02 PM | Train: [111/180] Step 150/1249 Loss 2.729 Prec@(1,3) (89.3%, 99.8%), ce_loss 0.691, lat_loss 21.806
09/28 09:36:27 PM | Train: [111/180] Step 200/1249 Loss 2.780 Prec@(1,3) (89.1%, 99.7%), ce_loss 0.691, lat_loss 21.806
09/28 09:36:52 PM | Train: [111/180] Step 250/1249 Loss 2.745 Prec@(1,3) (89.3%, 99.7%), ce_loss 0.691, lat_loss 21.806
09/28 09:37:17 PM | Train: [111/180] Step 300/1249 Loss 2.722 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.691, lat_loss 21.806
09/28 09:37:42 PM | Train: [111/180] Step 350/1249 Loss 2.749 Prec@(1,3) (89.3%, 99.6%), ce_loss 0.691, lat_loss 21.806
09/28 09:38:08 PM | Train: [111/180] Step 400/1249 Loss 2.733 Prec@(1,3) (89.4%, 99.6%), ce_loss 0.691, lat_loss 21.806
09/28 09:38:33 PM | Train: [111/180] Step 450/1249 Loss 2.736 Prec@(1,3) (89.4%, 99.6%), ce_loss 0.691, lat_loss 21.806
09/28 09:38:58 PM | Train: [111/180] Step 500/1249 Loss 2.710 Prec@(1,3) (89.5%, 99.6%), ce_loss 0.691, lat_loss 21.806
09/28 09:39:23 PM | Train: [111/180] Step 550/1249 Loss 2.726 Prec@(1,3) (89.4%, 99.6%), ce_loss 0.691, lat_loss 21.806
09/28 09:39:48 PM | Train: [111/180] Step 600/1249 Loss 2.702 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.691, lat_loss 21.806
09/28 09:40:11 PM | Train: [111/180] Step 650/1249 Loss 2.714 Prec@(1,3) (89.4%, 99.7%), ce_loss 0.691, lat_loss 21.806
09/28 09:40:36 PM | Train: [111/180] Step 700/1249 Loss 2.720 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:40:59 PM | Train: [111/180] Step 750/1249 Loss 2.721 Prec@(1,3) (89.4%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:41:21 PM | Train: [111/180] Step 800/1249 Loss 2.719 Prec@(1,3) (89.4%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:41:45 PM | Train: [111/180] Step 850/1249 Loss 2.704 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:42:09 PM | Train: [111/180] Step 900/1249 Loss 2.701 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:42:33 PM | Train: [111/180] Step 950/1249 Loss 2.697 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:42:58 PM | Train: [111/180] Step 1000/1249 Loss 2.694 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:43:23 PM | Train: [111/180] Step 1050/1249 Loss 2.694 Prec@(1,3) (89.6%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:43:48 PM | Train: [111/180] Step 1100/1249 Loss 2.686 Prec@(1,3) (89.6%, 99.7%), ce_loss 0.690, lat_loss 21.806
09/28 09:44:13 PM | Train: [111/180] Step 1150/1249 Loss 2.692 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.690, lat_loss 21.805
09/28 09:44:38 PM | Train: [111/180] Step 1200/1249 Loss 2.686 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.689, lat_loss 21.805
09/28 09:45:02 PM | Train: [111/180] Step 1249/1249 Loss 2.692 Prec@(1,3) (89.5%, 99.7%), ce_loss 0.689, lat_loss 21.805
09/28 09:45:02 PM | _w_step_train: [111/180] Final Prec@1 89.5250% Time 617.08
09/28 09:45:02 PM | Start to train theta for epoch 110
09/28 09:45:24 PM | Train: [111/180] Step 050/312 Loss 4.783 Prec@(1,3) (82.9%, 99.1%), ce_loss 0.689, lat_loss 21.805
09/28 09:45:44 PM | Train: [111/180] Step 100/312 Loss 4.443 Prec@(1,3) (83.7%, 99.1%), ce_loss 0.689, lat_loss 21.805
09/28 09:46:04 PM | Train: [111/180] Step 150/312 Loss 4.266 Prec@(1,3) (84.3%, 99.3%), ce_loss 0.689, lat_loss 21.805
09/28 09:46:24 PM | Train: [111/180] Step 200/312 Loss 4.121 Prec@(1,3) (84.7%, 99.3%), ce_loss 0.689, lat_loss 21.805
09/28 09:46:43 PM | Train: [111/180] Step 250/312 Loss 4.165 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:04 PM | Train: [111/180] Step 300/312 Loss 4.160 Prec@(1,3) (84.7%, 99.2%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:09 PM | Train: [111/180] Step 312/312 Loss 4.135 Prec@(1,3) (84.8%, 99.2%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:09 PM | _theta_step_train: [111/180] Final Prec@1 84.8100% Time 126.20
09/28 09:47:14 PM | Valid: [111/180] Step 050/312 Loss 4.517 Prec@(1,3) (85.4%, 98.2%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:19 PM | Valid: [111/180] Step 100/312 Loss 4.398 Prec@(1,3) (85.4%, 98.6%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:23 PM | Valid: [111/180] Step 150/312 Loss 4.226 Prec@(1,3) (85.8%, 98.7%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:28 PM | Valid: [111/180] Step 200/312 Loss 4.107 Prec@(1,3) (85.9%, 98.8%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:32 PM | Valid: [111/180] Step 250/312 Loss 4.257 Prec@(1,3) (85.4%, 98.7%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:37 PM | Valid: [111/180] Step 300/312 Loss 4.147 Prec@(1,3) (85.5%, 98.9%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:38 PM | Valid: [111/180] Step 312/312 Loss 4.154 Prec@(1,3) (85.5%, 98.9%), ce_loss 0.689, lat_loss 21.805
09/28 09:47:38 PM | val: [111/180] Final Prec@1 85.5300% Time 29.29
09/28 09:47:38 PM | Start to train weights for epoch 111
09/28 09:48:04 PM | Train: [112/180] Step 050/1249 Loss 2.725 Prec@(1,3) (89.7%, 99.4%), ce_loss 0.689, lat_loss 21.805
09/28 09:48:28 PM | Train: [112/180] Step 100/1249 Loss 2.547 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.689, lat_loss 21.805
09/28 09:48:53 PM | Train: [112/180] Step 150/1249 Loss 2.460 Prec@(1,3) (90.5%, 99.7%), ce_loss 0.688, lat_loss 21.805
09/28 09:49:18 PM | Train: [112/180] Step 200/1249 Loss 2.484 Prec@(1,3) (90.3%, 99.7%), ce_loss 0.688, lat_loss 21.805
09/28 09:49:42 PM | Train: [112/180] Step 250/1249 Loss 2.497 Prec@(1,3) (90.3%, 99.8%), ce_loss 0.688, lat_loss 21.805
09/28 09:50:07 PM | Train: [112/180] Step 300/1249 Loss 2.543 Prec@(1,3) (90.1%, 99.8%), ce_loss 0.688, lat_loss 21.805
09/28 09:50:31 PM | Train: [112/180] Step 350/1249 Loss 2.531 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.688, lat_loss 21.805
09/28 09:50:56 PM | Train: [112/180] Step 400/1249 Loss 2.526 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.688, lat_loss 21.805
09/28 09:51:20 PM | Train: [112/180] Step 450/1249 Loss 2.506 Prec@(1,3) (90.4%, 99.7%), ce_loss 0.688, lat_loss 21.805
09/28 09:51:45 PM | Train: [112/180] Step 500/1249 Loss 2.551 Prec@(1,3) (90.3%, 99.7%), ce_loss 0.688, lat_loss 21.805
09/28 09:52:10 PM | Train: [112/180] Step 550/1249 Loss 2.575 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.688, lat_loss 21.805
09/28 09:52:34 PM | Train: [112/180] Step 600/1249 Loss 2.592 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.688, lat_loss 21.805
09/28 09:52:58 PM | Train: [112/180] Step 650/1249 Loss 2.596 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.687, lat_loss 21.805
09/28 09:53:23 PM | Train: [112/180] Step 700/1249 Loss 2.643 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.687, lat_loss 21.805
09/28 09:53:47 PM | Train: [112/180] Step 750/1249 Loss 2.637 Prec@(1,3) (90.0%, 99.6%), ce_loss 0.687, lat_loss 21.805
09/28 09:54:10 PM | Train: [112/180] Step 800/1249 Loss 2.620 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.687, lat_loss 21.805
09/28 09:54:33 PM | Train: [112/180] Step 850/1249 Loss 2.658 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.687, lat_loss 21.805
09/28 09:54:55 PM | Train: [112/180] Step 900/1249 Loss 2.656 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.687, lat_loss 21.805
09/28 09:55:19 PM | Train: [112/180] Step 950/1249 Loss 2.658 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.687, lat_loss 21.805
09/28 09:55:42 PM | Train: [112/180] Step 1000/1249 Loss 2.639 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.687, lat_loss 21.805
09/28 09:56:06 PM | Train: [112/180] Step 1050/1249 Loss 2.634 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.687, lat_loss 21.805
09/28 09:56:29 PM | Train: [112/180] Step 1100/1249 Loss 2.644 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.687, lat_loss 21.805
09/28 09:56:52 PM | Train: [112/180] Step 1150/1249 Loss 2.654 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.687, lat_loss 21.805
09/28 09:57:15 PM | Train: [112/180] Step 1200/1249 Loss 2.657 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.686, lat_loss 21.805
09/28 09:57:40 PM | Train: [112/180] Step 1249/1249 Loss 2.657 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.686, lat_loss 21.805
09/28 09:57:40 PM | _w_step_train: [112/180] Final Prec@1 89.8575% Time 602.01
09/28 09:57:40 PM | Start to train theta for epoch 111
09/28 09:57:58 PM | Train: [112/180] Step 050/312 Loss 3.897 Prec@(1,3) (85.1%, 99.4%), ce_loss 0.686, lat_loss 21.805
09/28 09:58:15 PM | Train: [112/180] Step 100/312 Loss 4.064 Prec@(1,3) (84.5%, 99.2%), ce_loss 0.686, lat_loss 21.805
09/28 09:58:33 PM | Train: [112/180] Step 150/312 Loss 4.120 Prec@(1,3) (84.4%, 99.2%), ce_loss 0.686, lat_loss 21.805
09/28 09:58:53 PM | Train: [112/180] Step 200/312 Loss 4.175 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.686, lat_loss 21.805
09/28 09:59:13 PM | Train: [112/180] Step 250/312 Loss 4.174 Prec@(1,3) (84.3%, 99.2%), ce_loss 0.686, lat_loss 21.804
09/28 09:59:33 PM | Train: [112/180] Step 300/312 Loss 4.122 Prec@(1,3) (84.6%, 99.2%), ce_loss 0.686, lat_loss 21.804
09/28 09:59:38 PM | Train: [112/180] Step 312/312 Loss 4.114 Prec@(1,3) (84.7%, 99.2%), ce_loss 0.686, lat_loss 21.804
09/28 09:59:39 PM | _theta_step_train: [112/180] Final Prec@1 84.6500% Time 118.83
09/28 09:59:44 PM | Valid: [112/180] Step 050/312 Loss 3.725 Prec@(1,3) (84.9%, 99.4%), ce_loss 0.686, lat_loss 21.804
09/28 09:59:49 PM | Valid: [112/180] Step 100/312 Loss 4.056 Prec@(1,3) (84.5%, 99.2%), ce_loss 0.686, lat_loss 21.804
09/28 09:59:53 PM | Valid: [112/180] Step 150/312 Loss 4.155 Prec@(1,3) (84.3%, 99.1%), ce_loss 0.686, lat_loss 21.804
09/28 09:59:58 PM | Valid: [112/180] Step 200/312 Loss 4.089 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.686, lat_loss 21.804
09/28 10:00:03 PM | Valid: [112/180] Step 250/312 Loss 4.093 Prec@(1,3) (84.5%, 99.1%), ce_loss 0.686, lat_loss 21.804
09/28 10:00:07 PM | Valid: [112/180] Step 300/312 Loss 4.061 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.686, lat_loss 21.804
09/28 10:00:09 PM | Valid: [112/180] Step 312/312 Loss 4.051 Prec@(1,3) (84.4%, 99.1%), ce_loss 0.686, lat_loss 21.804
09/28 10:00:09 PM | val: [112/180] Final Prec@1 84.3800% Time 29.75
09/28 10:00:09 PM | Start to train weights for epoch 112
09/28 10:00:33 PM | Train: [113/180] Step 050/1249 Loss 2.662 Prec@(1,3) (89.5%, 99.9%), ce_loss 0.686, lat_loss 21.804
09/28 10:00:58 PM | Train: [113/180] Step 100/1249 Loss 2.624 Prec@(1,3) (89.6%, 99.8%), ce_loss 0.686, lat_loss 21.804
09/28 10:01:21 PM | Train: [113/180] Step 150/1249 Loss 2.568 Prec@(1,3) (89.8%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:01:46 PM | Train: [113/180] Step 200/1249 Loss 2.528 Prec@(1,3) (90.0%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:02:10 PM | Train: [113/180] Step 250/1249 Loss 2.557 Prec@(1,3) (89.9%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:02:34 PM | Train: [113/180] Step 300/1249 Loss 2.581 Prec@(1,3) (89.7%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:02:58 PM | Train: [113/180] Step 350/1249 Loss 2.574 Prec@(1,3) (89.8%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:03:22 PM | Train: [113/180] Step 400/1249 Loss 2.560 Prec@(1,3) (89.8%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:03:45 PM | Train: [113/180] Step 450/1249 Loss 2.567 Prec@(1,3) (89.8%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:04:08 PM | Train: [113/180] Step 500/1249 Loss 2.554 Prec@(1,3) (89.9%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:04:32 PM | Train: [113/180] Step 550/1249 Loss 2.567 Prec@(1,3) (89.9%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:04:56 PM | Train: [113/180] Step 600/1249 Loss 2.567 Prec@(1,3) (89.9%, 99.8%), ce_loss 0.685, lat_loss 21.804
09/28 10:05:19 PM | Train: [113/180] Step 650/1249 Loss 2.572 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:05:43 PM | Train: [113/180] Step 700/1249 Loss 2.569 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:06:07 PM | Train: [113/180] Step 750/1249 Loss 2.588 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:06:27 PM | Train: [113/180] Step 800/1249 Loss 2.575 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:06:51 PM | Train: [113/180] Step 850/1249 Loss 2.580 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:07:14 PM | Train: [113/180] Step 900/1249 Loss 2.609 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:07:38 PM | Train: [113/180] Step 950/1249 Loss 2.601 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:08:02 PM | Train: [113/180] Step 1000/1249 Loss 2.602 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:08:26 PM | Train: [113/180] Step 1050/1249 Loss 2.601 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:08:50 PM | Train: [113/180] Step 1100/1249 Loss 2.602 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:09:14 PM | Train: [113/180] Step 1150/1249 Loss 2.619 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.684, lat_loss 21.804
09/28 10:09:37 PM | Train: [113/180] Step 1200/1249 Loss 2.615 Prec@(1,3) (89.8%, 99.7%), ce_loss 0.683, lat_loss 21.804
09/28 10:10:01 PM | Train: [113/180] Step 1249/1249 Loss 2.625 Prec@(1,3) (89.8%, 99.7%), ce_loss 0.683, lat_loss 21.804
09/28 10:10:02 PM | _w_step_train: [113/180] Final Prec@1 89.8075% Time 593.01
09/28 10:10:02 PM | Start to train theta for epoch 112
09/28 10:10:23 PM | Train: [113/180] Step 050/312 Loss 4.087 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.683, lat_loss 21.804
09/28 10:10:44 PM | Train: [113/180] Step 100/312 Loss 4.075 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.683, lat_loss 21.804
09/28 10:11:02 PM | Train: [113/180] Step 150/312 Loss 4.035 Prec@(1,3) (85.7%, 99.2%), ce_loss 0.683, lat_loss 21.804
09/28 10:11:23 PM | Train: [113/180] Step 200/312 Loss 4.037 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.683, lat_loss 21.804
09/28 10:11:44 PM | Train: [113/180] Step 250/312 Loss 4.080 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.683, lat_loss 21.804
09/28 10:12:05 PM | Train: [113/180] Step 300/312 Loss 4.204 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.683, lat_loss 21.804
09/28 10:12:10 PM | Train: [113/180] Step 312/312 Loss 4.194 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.683, lat_loss 21.804
09/28 10:12:10 PM | _theta_step_train: [113/180] Final Prec@1 85.0500% Time 128.73
09/28 10:12:16 PM | Valid: [113/180] Step 050/312 Loss 3.405 Prec@(1,3) (87.7%, 99.6%), ce_loss 0.683, lat_loss 21.804
09/28 10:12:20 PM | Valid: [113/180] Step 100/312 Loss 3.733 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.683, lat_loss 21.804
09/28 10:12:25 PM | Valid: [113/180] Step 150/312 Loss 3.847 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.683, lat_loss 21.804
09/28 10:12:29 PM | Valid: [113/180] Step 200/312 Loss 3.800 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.683, lat_loss 21.804
09/28 10:12:34 PM | Valid: [113/180] Step 250/312 Loss 3.725 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.683, lat_loss 21.804
09/28 10:12:39 PM | Valid: [113/180] Step 300/312 Loss 3.670 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.683, lat_loss 21.803
09/28 10:12:40 PM | Valid: [113/180] Step 312/312 Loss 3.661 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.683, lat_loss 21.803
09/28 10:12:40 PM | val: [113/180] Final Prec@1 86.5600% Time 29.31
09/28 10:12:40 PM | Best top1 acc by now. Save model
09/28 10:12:40 PM | Start to train weights for epoch 113
09/28 10:13:06 PM | Train: [114/180] Step 050/1249 Loss 2.642 Prec@(1,3) (90.0%, 99.8%), ce_loss 0.683, lat_loss 21.803
09/28 10:13:30 PM | Train: [114/180] Step 100/1249 Loss 2.568 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.682, lat_loss 21.803
09/28 10:13:54 PM | Train: [114/180] Step 150/1249 Loss 2.515 Prec@(1,3) (90.0%, 99.8%), ce_loss 0.682, lat_loss 21.803
09/28 10:14:16 PM | Train: [114/180] Step 200/1249 Loss 2.553 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.682, lat_loss 21.803
09/28 10:14:39 PM | Train: [114/180] Step 250/1249 Loss 2.542 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.682, lat_loss 21.803
09/28 10:15:03 PM | Train: [114/180] Step 300/1249 Loss 2.515 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.682, lat_loss 21.803
09/28 10:15:26 PM | Train: [114/180] Step 350/1249 Loss 2.547 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.682, lat_loss 21.803
09/28 10:15:50 PM | Train: [114/180] Step 400/1249 Loss 2.659 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.682, lat_loss 21.803
09/28 10:16:14 PM | Train: [114/180] Step 450/1249 Loss 2.615 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.682, lat_loss 21.803
09/28 10:16:36 PM | Train: [114/180] Step 500/1249 Loss 2.618 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.682, lat_loss 21.803
09/28 10:17:00 PM | Train: [114/180] Step 550/1249 Loss 2.619 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.682, lat_loss 21.803
09/28 10:17:24 PM | Train: [114/180] Step 600/1249 Loss 2.638 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.682, lat_loss 21.803
09/28 10:17:49 PM | Train: [114/180] Step 650/1249 Loss 2.627 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:18:13 PM | Train: [114/180] Step 700/1249 Loss 2.678 Prec@(1,3) (89.7%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:18:38 PM | Train: [114/180] Step 750/1249 Loss 2.688 Prec@(1,3) (89.6%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:19:02 PM | Train: [114/180] Step 800/1249 Loss 2.670 Prec@(1,3) (89.7%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:19:25 PM | Train: [114/180] Step 850/1249 Loss 2.660 Prec@(1,3) (89.7%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:19:49 PM | Train: [114/180] Step 900/1249 Loss 2.624 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:20:13 PM | Train: [114/180] Step 950/1249 Loss 2.629 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:20:37 PM | Train: [114/180] Step 1000/1249 Loss 2.618 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:21:01 PM | Train: [114/180] Step 1050/1249 Loss 2.613 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:21:24 PM | Train: [114/180] Step 1100/1249 Loss 2.631 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:21:47 PM | Train: [114/180] Step 1150/1249 Loss 2.634 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.681, lat_loss 21.803
09/28 10:22:12 PM | Train: [114/180] Step 1200/1249 Loss 2.630 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.680, lat_loss 21.803
09/28 10:22:37 PM | Train: [114/180] Step 1249/1249 Loss 2.627 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.680, lat_loss 21.803
09/28 10:22:37 PM | _w_step_train: [114/180] Final Prec@1 89.8275% Time 596.83
09/28 10:22:37 PM | Start to train theta for epoch 113
09/28 10:22:57 PM | Train: [114/180] Step 050/312 Loss 3.854 Prec@(1,3) (86.2%, 98.9%), ce_loss 0.680, lat_loss 21.803
09/28 10:23:13 PM | Train: [114/180] Step 100/312 Loss 3.934 Prec@(1,3) (85.6%, 99.0%), ce_loss 0.680, lat_loss 21.803
09/28 10:23:29 PM | Train: [114/180] Step 150/312 Loss 3.930 Prec@(1,3) (85.4%, 99.0%), ce_loss 0.680, lat_loss 21.803
09/28 10:23:46 PM | Train: [114/180] Step 200/312 Loss 3.952 Prec@(1,3) (85.6%, 99.0%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:03 PM | Train: [114/180] Step 250/312 Loss 3.995 Prec@(1,3) (85.5%, 99.1%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:21 PM | Train: [114/180] Step 300/312 Loss 4.024 Prec@(1,3) (85.3%, 99.1%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:26 PM | Train: [114/180] Step 312/312 Loss 3.982 Prec@(1,3) (85.5%, 99.1%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:26 PM | _theta_step_train: [114/180] Final Prec@1 85.4800% Time 109.05
09/28 10:24:31 PM | Valid: [114/180] Step 050/312 Loss 3.786 Prec@(1,3) (85.4%, 99.4%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:36 PM | Valid: [114/180] Step 100/312 Loss 3.938 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:40 PM | Valid: [114/180] Step 150/312 Loss 3.881 Prec@(1,3) (85.4%, 99.1%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:45 PM | Valid: [114/180] Step 200/312 Loss 3.766 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:50 PM | Valid: [114/180] Step 250/312 Loss 3.718 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:54 PM | Valid: [114/180] Step 300/312 Loss 3.692 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:55 PM | Valid: [114/180] Step 312/312 Loss 3.679 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.680, lat_loss 21.803
09/28 10:24:55 PM | val: [114/180] Final Prec@1 85.8300% Time 29.62
09/28 10:24:55 PM | Start to train weights for epoch 114
09/28 10:25:21 PM | Train: [115/180] Step 050/1249 Loss 2.357 Prec@(1,3) (91.0%, 99.8%), ce_loss 0.680, lat_loss 21.803
09/28 10:25:44 PM | Train: [115/180] Step 100/1249 Loss 2.420 Prec@(1,3) (90.7%, 99.7%), ce_loss 0.679, lat_loss 21.803
09/28 10:26:09 PM | Train: [115/180] Step 150/1249 Loss 2.490 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.679, lat_loss 21.803
09/28 10:26:33 PM | Train: [115/180] Step 200/1249 Loss 2.423 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.679, lat_loss 21.802
09/28 10:26:54 PM | Train: [115/180] Step 250/1249 Loss 2.661 Prec@(1,3) (90.1%, 99.6%), ce_loss 0.679, lat_loss 21.802
09/28 10:27:19 PM | Train: [115/180] Step 300/1249 Loss 2.667 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.679, lat_loss 21.802
09/28 10:27:43 PM | Train: [115/180] Step 350/1249 Loss 2.663 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.679, lat_loss 21.802
09/28 10:28:08 PM | Train: [115/180] Step 400/1249 Loss 2.614 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.679, lat_loss 21.802
09/28 10:28:32 PM | Train: [115/180] Step 450/1249 Loss 2.628 Prec@(1,3) (89.8%, 99.7%), ce_loss 0.679, lat_loss 21.802
09/28 10:28:54 PM | Train: [115/180] Step 500/1249 Loss 2.601 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.679, lat_loss 21.802
09/28 10:29:17 PM | Train: [115/180] Step 550/1249 Loss 2.598 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.679, lat_loss 21.802
09/28 10:29:40 PM | Train: [115/180] Step 600/1249 Loss 2.567 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.679, lat_loss 21.802
09/28 10:30:02 PM | Train: [115/180] Step 650/1249 Loss 2.567 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:30:25 PM | Train: [115/180] Step 700/1249 Loss 2.579 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:30:48 PM | Train: [115/180] Step 750/1249 Loss 2.574 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:31:11 PM | Train: [115/180] Step 800/1249 Loss 2.577 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:31:34 PM | Train: [115/180] Step 850/1249 Loss 2.576 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:31:56 PM | Train: [115/180] Step 900/1249 Loss 2.578 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:32:20 PM | Train: [115/180] Step 950/1249 Loss 2.569 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:32:43 PM | Train: [115/180] Step 1000/1249 Loss 2.573 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:33:06 PM | Train: [115/180] Step 1050/1249 Loss 2.581 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:33:30 PM | Train: [115/180] Step 1100/1249 Loss 2.586 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:33:55 PM | Train: [115/180] Step 1150/1249 Loss 2.584 Prec@(1,3) (89.8%, 99.7%), ce_loss 0.678, lat_loss 21.802
09/28 10:34:20 PM | Train: [115/180] Step 1200/1249 Loss 2.606 Prec@(1,3) (89.7%, 99.7%), ce_loss 0.677, lat_loss 21.802
09/28 10:34:44 PM | Train: [115/180] Step 1249/1249 Loss 2.632 Prec@(1,3) (89.7%, 99.7%), ce_loss 0.677, lat_loss 21.802
09/28 10:34:44 PM | _w_step_train: [115/180] Final Prec@1 89.7175% Time 588.78
09/28 10:34:44 PM | Start to train theta for epoch 114
09/28 10:35:05 PM | Train: [115/180] Step 050/312 Loss 3.678 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.677, lat_loss 21.802
09/28 10:35:24 PM | Train: [115/180] Step 100/312 Loss 4.044 Prec@(1,3) (84.9%, 99.2%), ce_loss 0.677, lat_loss 21.802
09/28 10:35:44 PM | Train: [115/180] Step 150/312 Loss 4.149 Prec@(1,3) (84.5%, 99.0%), ce_loss 0.677, lat_loss 21.802
09/28 10:36:00 PM | Train: [115/180] Step 200/312 Loss 4.085 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.677, lat_loss 21.802
09/28 10:36:17 PM | Train: [115/180] Step 250/312 Loss 4.095 Prec@(1,3) (84.8%, 99.0%), ce_loss 0.677, lat_loss 21.802
09/28 10:36:36 PM | Train: [115/180] Step 300/312 Loss 4.101 Prec@(1,3) (84.8%, 99.1%), ce_loss 0.677, lat_loss 21.802
09/28 10:36:41 PM | Train: [115/180] Step 312/312 Loss 4.104 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.677, lat_loss 21.802
09/28 10:36:41 PM | _theta_step_train: [115/180] Final Prec@1 84.8500% Time 116.92
09/28 10:36:46 PM | Valid: [115/180] Step 050/312 Loss 3.422 Prec@(1,3) (87.2%, 99.5%), ce_loss 0.677, lat_loss 21.802
09/28 10:36:51 PM | Valid: [115/180] Step 100/312 Loss 3.611 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.677, lat_loss 21.802
09/28 10:36:56 PM | Valid: [115/180] Step 150/312 Loss 3.649 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.677, lat_loss 21.802
09/28 10:37:00 PM | Valid: [115/180] Step 200/312 Loss 3.651 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.677, lat_loss 21.802
09/28 10:37:05 PM | Valid: [115/180] Step 250/312 Loss 3.682 Prec@(1,3) (86.0%, 99.2%), ce_loss 0.677, lat_loss 21.802
09/28 10:37:09 PM | Valid: [115/180] Step 300/312 Loss 3.611 Prec@(1,3) (86.2%, 99.2%), ce_loss 0.677, lat_loss 21.802
09/28 10:37:10 PM | Valid: [115/180] Step 312/312 Loss 3.637 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.677, lat_loss 21.802
09/28 10:37:11 PM | val: [115/180] Final Prec@1 86.0800% Time 29.37
09/28 10:37:11 PM | Start to train weights for epoch 115
09/28 10:37:37 PM | Train: [116/180] Step 050/1249 Loss 2.288 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.677, lat_loss 21.802
09/28 10:38:02 PM | Train: [116/180] Step 100/1249 Loss 2.509 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.677, lat_loss 21.802
09/28 10:38:26 PM | Train: [116/180] Step 150/1249 Loss 2.493 Prec@(1,3) (90.7%, 99.7%), ce_loss 0.676, lat_loss 21.802
09/28 10:38:51 PM | Train: [116/180] Step 200/1249 Loss 2.476 Prec@(1,3) (90.6%, 99.8%), ce_loss 0.676, lat_loss 21.802
09/28 10:39:15 PM | Train: [116/180] Step 250/1249 Loss 2.634 Prec@(1,3) (90.4%, 99.6%), ce_loss 0.676, lat_loss 21.802
09/28 10:39:39 PM | Train: [116/180] Step 300/1249 Loss 2.651 Prec@(1,3) (90.2%, 99.6%), ce_loss 0.676, lat_loss 21.802
09/28 10:40:04 PM | Train: [116/180] Step 350/1249 Loss 2.653 Prec@(1,3) (90.1%, 99.6%), ce_loss 0.676, lat_loss 21.802
09/28 10:40:23 PM | Train: [116/180] Step 400/1249 Loss 2.649 Prec@(1,3) (90.0%, 99.6%), ce_loss 0.676, lat_loss 21.801
09/28 10:40:39 PM | Train: [116/180] Step 450/1249 Loss 2.649 Prec@(1,3) (90.1%, 99.6%), ce_loss 0.676, lat_loss 21.801
09/28 10:40:55 PM | Train: [116/180] Step 500/1249 Loss 2.624 Prec@(1,3) (90.1%, 99.6%), ce_loss 0.676, lat_loss 21.801
09/28 10:41:10 PM | Train: [116/180] Step 550/1249 Loss 2.603 Prec@(1,3) (90.1%, 99.6%), ce_loss 0.676, lat_loss 21.801
09/28 10:41:26 PM | Train: [116/180] Step 600/1249 Loss 2.630 Prec@(1,3) (90.0%, 99.6%), ce_loss 0.676, lat_loss 21.801
09/28 10:41:42 PM | Train: [116/180] Step 650/1249 Loss 2.617 Prec@(1,3) (90.0%, 99.6%), ce_loss 0.676, lat_loss 21.801
09/28 10:41:58 PM | Train: [116/180] Step 700/1249 Loss 2.602 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:42:14 PM | Train: [116/180] Step 750/1249 Loss 2.601 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:42:30 PM | Train: [116/180] Step 800/1249 Loss 2.595 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:42:46 PM | Train: [116/180] Step 850/1249 Loss 2.597 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:43:02 PM | Train: [116/180] Step 900/1249 Loss 2.577 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:43:18 PM | Train: [116/180] Step 950/1249 Loss 2.574 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:43:34 PM | Train: [116/180] Step 1000/1249 Loss 2.586 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:43:52 PM | Train: [116/180] Step 1050/1249 Loss 2.602 Prec@(1,3) (90.1%, 99.6%), ce_loss 0.675, lat_loss 21.801
09/28 10:44:18 PM | Train: [116/180] Step 1100/1249 Loss 2.593 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:44:37 PM | Train: [116/180] Step 1150/1249 Loss 2.587 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:44:53 PM | Train: [116/180] Step 1200/1249 Loss 2.588 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.675, lat_loss 21.801
09/28 10:45:08 PM | Train: [116/180] Step 1249/1249 Loss 2.580 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.674, lat_loss 21.801
09/28 10:45:08 PM | _w_step_train: [116/180] Final Prec@1 90.1125% Time 477.91
09/28 10:45:08 PM | Start to train theta for epoch 115
09/28 10:45:28 PM | Train: [116/180] Step 050/312 Loss 3.816 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.674, lat_loss 21.801
09/28 10:45:46 PM | Train: [116/180] Step 100/312 Loss 3.939 Prec@(1,3) (86.0%, 99.2%), ce_loss 0.674, lat_loss 21.801
09/28 10:46:04 PM | Train: [116/180] Step 150/312 Loss 3.950 Prec@(1,3) (85.7%, 99.2%), ce_loss 0.674, lat_loss 21.801
09/28 10:46:21 PM | Train: [116/180] Step 200/312 Loss 4.049 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.674, lat_loss 21.801
09/28 10:46:37 PM | Train: [116/180] Step 250/312 Loss 4.026 Prec@(1,3) (85.5%, 99.3%), ce_loss 0.674, lat_loss 21.801
09/28 10:46:55 PM | Train: [116/180] Step 300/312 Loss 4.037 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:00 PM | Train: [116/180] Step 312/312 Loss 4.023 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:00 PM | _theta_step_train: [116/180] Final Prec@1 85.4500% Time 111.12
09/28 10:47:05 PM | Valid: [116/180] Step 050/312 Loss 3.746 Prec@(1,3) (85.8%, 99.1%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:09 PM | Valid: [116/180] Step 100/312 Loss 3.790 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:14 PM | Valid: [116/180] Step 150/312 Loss 3.773 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:19 PM | Valid: [116/180] Step 200/312 Loss 3.734 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:23 PM | Valid: [116/180] Step 250/312 Loss 3.682 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:28 PM | Valid: [116/180] Step 300/312 Loss 3.644 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:29 PM | Valid: [116/180] Step 312/312 Loss 3.650 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.674, lat_loss 21.801
09/28 10:47:29 PM | val: [116/180] Final Prec@1 86.4300% Time 29.60
09/28 10:47:29 PM | Start to train weights for epoch 116
09/28 10:47:55 PM | Train: [117/180] Step 050/1249 Loss 2.576 Prec@(1,3) (90.6%, 99.4%), ce_loss 0.674, lat_loss 21.801
09/28 10:48:19 PM | Train: [117/180] Step 100/1249 Loss 2.408 Prec@(1,3) (91.0%, 99.5%), ce_loss 0.674, lat_loss 21.801
09/28 10:48:44 PM | Train: [117/180] Step 150/1249 Loss 2.436 Prec@(1,3) (90.7%, 99.6%), ce_loss 0.673, lat_loss 21.801
09/28 10:49:10 PM | Train: [117/180] Step 200/1249 Loss 2.359 Prec@(1,3) (90.8%, 99.6%), ce_loss 0.673, lat_loss 21.801
09/28 10:49:35 PM | Train: [117/180] Step 250/1249 Loss 2.361 Prec@(1,3) (90.7%, 99.6%), ce_loss 0.673, lat_loss 21.801
09/28 10:50:00 PM | Train: [117/180] Step 300/1249 Loss 2.380 Prec@(1,3) (90.5%, 99.7%), ce_loss 0.673, lat_loss 21.801
09/28 10:50:25 PM | Train: [117/180] Step 350/1249 Loss 2.423 Prec@(1,3) (90.4%, 99.7%), ce_loss 0.673, lat_loss 21.801
09/28 10:50:50 PM | Train: [117/180] Step 400/1249 Loss 2.484 Prec@(1,3) (90.3%, 99.7%), ce_loss 0.673, lat_loss 21.801
09/28 10:51:15 PM | Train: [117/180] Step 450/1249 Loss 2.484 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.673, lat_loss 21.801
09/28 10:51:40 PM | Train: [117/180] Step 500/1249 Loss 2.491 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.673, lat_loss 21.801
09/28 10:52:05 PM | Train: [117/180] Step 550/1249 Loss 2.500 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.673, lat_loss 21.801
09/28 10:52:29 PM | Train: [117/180] Step 600/1249 Loss 2.505 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.673, lat_loss 21.801
09/28 10:52:53 PM | Train: [117/180] Step 650/1249 Loss 2.485 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.673, lat_loss 21.801
09/28 10:53:18 PM | Train: [117/180] Step 700/1249 Loss 2.487 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:53:43 PM | Train: [117/180] Step 750/1249 Loss 2.468 Prec@(1,3) (90.3%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:54:08 PM | Train: [117/180] Step 800/1249 Loss 2.477 Prec@(1,3) (90.4%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:54:33 PM | Train: [117/180] Step 850/1249 Loss 2.478 Prec@(1,3) (90.4%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:54:59 PM | Train: [117/180] Step 900/1249 Loss 2.483 Prec@(1,3) (90.4%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:55:23 PM | Train: [117/180] Step 950/1249 Loss 2.478 Prec@(1,3) (90.4%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:55:48 PM | Train: [117/180] Step 1000/1249 Loss 2.492 Prec@(1,3) (90.3%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:56:14 PM | Train: [117/180] Step 1050/1249 Loss 2.516 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:56:39 PM | Train: [117/180] Step 1100/1249 Loss 2.518 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:57:04 PM | Train: [117/180] Step 1150/1249 Loss 2.515 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:57:29 PM | Train: [117/180] Step 1200/1249 Loss 2.510 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:57:54 PM | Train: [117/180] Step 1249/1249 Loss 2.511 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.672, lat_loss 21.800
09/28 10:57:54 PM | _w_step_train: [117/180] Final Prec@1 90.2475% Time 624.72
09/28 10:57:54 PM | Start to train theta for epoch 116
09/28 10:58:15 PM | Train: [117/180] Step 050/312 Loss 3.355 Prec@(1,3) (86.9%, 99.7%), ce_loss 0.671, lat_loss 21.800
09/28 10:58:31 PM | Train: [117/180] Step 100/312 Loss 3.591 Prec@(1,3) (86.3%, 99.5%), ce_loss 0.671, lat_loss 21.800
09/28 10:58:50 PM | Train: [117/180] Step 150/312 Loss 3.705 Prec@(1,3) (85.8%, 99.5%), ce_loss 0.671, lat_loss 21.800
09/28 10:59:10 PM | Train: [117/180] Step 200/312 Loss 3.832 Prec@(1,3) (85.6%, 99.4%), ce_loss 0.671, lat_loss 21.800
09/28 10:59:29 PM | Train: [117/180] Step 250/312 Loss 3.866 Prec@(1,3) (85.4%, 99.4%), ce_loss 0.671, lat_loss 21.800
09/28 10:59:50 PM | Train: [117/180] Step 300/312 Loss 3.849 Prec@(1,3) (85.5%, 99.4%), ce_loss 0.671, lat_loss 21.800
09/28 10:59:55 PM | Train: [117/180] Step 312/312 Loss 3.839 Prec@(1,3) (85.5%, 99.4%), ce_loss 0.671, lat_loss 21.800
09/28 10:59:55 PM | _theta_step_train: [117/180] Final Prec@1 85.4900% Time 120.75
09/28 11:00:00 PM | Valid: [117/180] Step 050/312 Loss 3.699 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.671, lat_loss 21.800
09/28 11:00:05 PM | Valid: [117/180] Step 100/312 Loss 3.810 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.671, lat_loss 21.800
09/28 11:00:09 PM | Valid: [117/180] Step 150/312 Loss 3.906 Prec@(1,3) (85.9%, 99.1%), ce_loss 0.671, lat_loss 21.800
09/28 11:00:14 PM | Valid: [117/180] Step 200/312 Loss 3.810 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.671, lat_loss 21.800
09/28 11:00:18 PM | Valid: [117/180] Step 250/312 Loss 3.749 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.671, lat_loss 21.800
09/28 11:00:23 PM | Valid: [117/180] Step 300/312 Loss 3.657 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.671, lat_loss 21.800
09/28 11:00:24 PM | Valid: [117/180] Step 312/312 Loss 3.649 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.671, lat_loss 21.800
09/28 11:00:24 PM | val: [117/180] Final Prec@1 86.5600% Time 29.46
09/28 11:00:24 PM | Start to train weights for epoch 117
09/28 11:00:50 PM | Train: [118/180] Step 050/1249 Loss 3.008 Prec@(1,3) (88.6%, 99.3%), ce_loss 0.671, lat_loss 21.800
09/28 11:01:14 PM | Train: [118/180] Step 100/1249 Loss 2.579 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.671, lat_loss 21.800
09/28 11:01:38 PM | Train: [118/180] Step 150/1249 Loss 2.568 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.671, lat_loss 21.800
09/28 11:02:01 PM | Train: [118/180] Step 200/1249 Loss 2.570 Prec@(1,3) (90.0%, 99.6%), ce_loss 0.670, lat_loss 21.800
09/28 11:02:26 PM | Train: [118/180] Step 250/1249 Loss 2.507 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.670, lat_loss 21.800
09/28 11:02:47 PM | Train: [118/180] Step 300/1249 Loss 2.551 Prec@(1,3) (90.0%, 99.6%), ce_loss 0.670, lat_loss 21.800
09/28 11:03:08 PM | Train: [118/180] Step 350/1249 Loss 2.588 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.670, lat_loss 21.800
09/28 11:03:26 PM | Train: [118/180] Step 400/1249 Loss 2.583 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.670, lat_loss 21.800
09/28 11:03:42 PM | Train: [118/180] Step 450/1249 Loss 2.598 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.670, lat_loss 21.800
09/28 11:03:58 PM | Train: [118/180] Step 500/1249 Loss 2.588 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.670, lat_loss 21.800
09/28 11:04:14 PM | Train: [118/180] Step 550/1249 Loss 2.576 Prec@(1,3) (89.8%, 99.6%), ce_loss 0.670, lat_loss 21.800
09/28 11:04:30 PM | Train: [118/180] Step 600/1249 Loss 2.560 Prec@(1,3) (89.9%, 99.6%), ce_loss 0.670, lat_loss 21.800
09/28 11:04:46 PM | Train: [118/180] Step 650/1249 Loss 2.557 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.670, lat_loss 21.800
09/28 11:05:02 PM | Train: [118/180] Step 700/1249 Loss 2.552 Prec@(1,3) (89.9%, 99.7%), ce_loss 0.670, lat_loss 21.800
09/28 11:05:18 PM | Train: [118/180] Step 750/1249 Loss 2.537 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.669, lat_loss 21.800
09/28 11:05:33 PM | Train: [118/180] Step 800/1249 Loss 2.539 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.669, lat_loss 21.800
09/28 11:05:49 PM | Train: [118/180] Step 850/1249 Loss 2.522 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.669, lat_loss 21.800
09/28 11:06:05 PM | Train: [118/180] Step 900/1249 Loss 2.518 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.669, lat_loss 21.799
09/28 11:06:21 PM | Train: [118/180] Step 950/1249 Loss 2.519 Prec@(1,3) (90.0%, 99.7%), ce_loss 0.669, lat_loss 21.799
09/28 11:06:37 PM | Train: [118/180] Step 1000/1249 Loss 2.508 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.669, lat_loss 21.799
09/28 11:06:53 PM | Train: [118/180] Step 1050/1249 Loss 2.512 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.669, lat_loss 21.799
09/28 11:07:09 PM | Train: [118/180] Step 1100/1249 Loss 2.519 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.669, lat_loss 21.799
09/28 11:07:25 PM | Train: [118/180] Step 1150/1249 Loss 2.517 Prec@(1,3) (90.2%, 99.7%), ce_loss 0.669, lat_loss 21.799
09/28 11:07:40 PM | Train: [118/180] Step 1200/1249 Loss 2.524 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.669, lat_loss 21.799
09/28 11:07:56 PM | Train: [118/180] Step 1249/1249 Loss 2.514 Prec@(1,3) (90.1%, 99.7%), ce_loss 0.669, lat_loss 21.799
09/28 11:07:56 PM | _w_step_train: [118/180] Final Prec@1 90.1375% Time 451.83
09/28 11:07:56 PM | Start to train theta for epoch 117
09/28 11:08:16 PM | Train: [118/180] Step 050/312 Loss 3.895 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.669, lat_loss 21.799
09/28 11:08:33 PM | Train: [118/180] Step 100/312 Loss 3.909 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.668, lat_loss 21.799
09/28 11:08:52 PM | Train: [118/180] Step 150/312 Loss 3.842 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.668, lat_loss 21.799
09/28 11:09:12 PM | Train: [118/180] Step 200/312 Loss 3.942 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.668, lat_loss 21.799
09/28 11:09:32 PM | Train: [118/180] Step 250/312 Loss 3.930 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.668, lat_loss 21.799
09/28 11:09:52 PM | Train: [118/180] Step 300/312 Loss 4.004 Prec@(1,3) (85.8%, 99.2%), ce_loss 0.668, lat_loss 21.799
09/28 11:09:57 PM | Train: [118/180] Step 312/312 Loss 4.030 Prec@(1,3) (85.8%, 99.2%), ce_loss 0.668, lat_loss 21.799
09/28 11:09:57 PM | _theta_step_train: [118/180] Final Prec@1 85.7600% Time 121.38
09/28 11:10:03 PM | Valid: [118/180] Step 050/312 Loss 3.605 Prec@(1,3) (86.7%, 99.6%), ce_loss 0.668, lat_loss 21.799
09/28 11:10:07 PM | Valid: [118/180] Step 100/312 Loss 4.048 Prec@(1,3) (85.9%, 99.1%), ce_loss 0.668, lat_loss 21.799
09/28 11:10:12 PM | Valid: [118/180] Step 150/312 Loss 3.996 Prec@(1,3) (85.9%, 99.1%), ce_loss 0.668, lat_loss 21.799
09/28 11:10:17 PM | Valid: [118/180] Step 200/312 Loss 3.984 Prec@(1,3) (85.9%, 99.1%), ce_loss 0.668, lat_loss 21.799
09/28 11:10:21 PM | Valid: [118/180] Step 250/312 Loss 3.954 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.668, lat_loss 21.799
09/28 11:10:26 PM | Valid: [118/180] Step 300/312 Loss 3.944 Prec@(1,3) (85.9%, 99.1%), ce_loss 0.668, lat_loss 21.799
09/28 11:10:27 PM | Valid: [118/180] Step 312/312 Loss 3.923 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.668, lat_loss 21.799
09/28 11:10:27 PM | val: [118/180] Final Prec@1 85.9300% Time 29.72
09/28 11:10:27 PM | Start to train weights for epoch 118
09/28 11:10:52 PM | Train: [119/180] Step 050/1249 Loss 2.412 Prec@(1,3) (90.6%, 99.8%), ce_loss 0.668, lat_loss 21.799
09/28 11:11:15 PM | Train: [119/180] Step 100/1249 Loss 2.500 Prec@(1,3) (90.4%, 99.8%), ce_loss 0.668, lat_loss 21.799
09/28 11:11:37 PM | Train: [119/180] Step 150/1249 Loss 2.395 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.668, lat_loss 21.799
09/28 11:11:58 PM | Train: [119/180] Step 200/1249 Loss 2.350 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.668, lat_loss 21.799
09/28 11:12:21 PM | Train: [119/180] Step 250/1249 Loss 2.366 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.668, lat_loss 21.799
09/28 11:12:46 PM | Train: [119/180] Step 300/1249 Loss 2.347 Prec@(1,3) (90.9%, 99.7%), ce_loss 0.667, lat_loss 21.799
09/28 11:13:10 PM | Train: [119/180] Step 350/1249 Loss 2.365 Prec@(1,3) (90.8%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:13:34 PM | Train: [119/180] Step 400/1249 Loss 2.350 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:13:55 PM | Train: [119/180] Step 450/1249 Loss 2.331 Prec@(1,3) (91.0%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:14:19 PM | Train: [119/180] Step 500/1249 Loss 2.337 Prec@(1,3) (91.0%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:14:42 PM | Train: [119/180] Step 550/1249 Loss 2.344 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:15:05 PM | Train: [119/180] Step 600/1249 Loss 2.335 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:15:26 PM | Train: [119/180] Step 650/1249 Loss 2.320 Prec@(1,3) (91.0%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:15:51 PM | Train: [119/180] Step 700/1249 Loss 2.332 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:16:14 PM | Train: [119/180] Step 750/1249 Loss 2.335 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:16:39 PM | Train: [119/180] Step 800/1249 Loss 2.345 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.667, lat_loss 21.799
09/28 11:17:04 PM | Train: [119/180] Step 850/1249 Loss 2.347 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.666, lat_loss 21.799
09/28 11:17:27 PM | Train: [119/180] Step 900/1249 Loss 2.343 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.666, lat_loss 21.799
09/28 11:17:52 PM | Train: [119/180] Step 950/1249 Loss 2.375 Prec@(1,3) (90.8%, 99.8%), ce_loss 0.666, lat_loss 21.799
09/28 11:18:17 PM | Train: [119/180] Step 1000/1249 Loss 2.388 Prec@(1,3) (90.7%, 99.7%), ce_loss 0.666, lat_loss 21.799
09/28 11:18:42 PM | Train: [119/180] Step 1050/1249 Loss 2.374 Prec@(1,3) (90.8%, 99.8%), ce_loss 0.666, lat_loss 21.799
09/28 11:19:07 PM | Train: [119/180] Step 1100/1249 Loss 2.376 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.666, lat_loss 21.799
09/28 11:19:32 PM | Train: [119/180] Step 1150/1249 Loss 2.368 Prec@(1,3) (90.8%, 99.8%), ce_loss 0.666, lat_loss 21.799
09/28 11:19:57 PM | Train: [119/180] Step 1200/1249 Loss 2.384 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.666, lat_loss 21.799
09/28 11:20:22 PM | Train: [119/180] Step 1249/1249 Loss 2.391 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.666, lat_loss 21.799
09/28 11:20:22 PM | _w_step_train: [119/180] Final Prec@1 90.6550% Time 594.63
09/28 11:20:22 PM | Start to train theta for epoch 118
09/28 11:20:35 PM | Train: [119/180] Step 050/312 Loss 4.294 Prec@(1,3) (84.3%, 99.3%), ce_loss 0.666, lat_loss 21.799
09/28 11:20:47 PM | Train: [119/180] Step 100/312 Loss 4.018 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.666, lat_loss 21.798
09/28 11:21:00 PM | Train: [119/180] Step 150/312 Loss 3.963 Prec@(1,3) (85.3%, 99.3%), ce_loss 0.666, lat_loss 21.798
09/28 11:21:12 PM | Train: [119/180] Step 200/312 Loss 3.907 Prec@(1,3) (85.6%, 99.3%), ce_loss 0.666, lat_loss 21.798
09/28 11:21:24 PM | Train: [119/180] Step 250/312 Loss 3.935 Prec@(1,3) (85.5%, 99.4%), ce_loss 0.665, lat_loss 21.798
09/28 11:21:36 PM | Train: [119/180] Step 300/312 Loss 4.025 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.665, lat_loss 21.798
09/28 11:21:39 PM | Train: [119/180] Step 312/312 Loss 3.977 Prec@(1,3) (85.2%, 99.3%), ce_loss 0.665, lat_loss 21.798
09/28 11:21:39 PM | _theta_step_train: [119/180] Final Prec@1 85.2500% Time 77.54
09/28 11:21:45 PM | Valid: [119/180] Step 050/312 Loss 3.672 Prec@(1,3) (86.0%, 99.4%), ce_loss 0.665, lat_loss 21.798
09/28 11:21:49 PM | Valid: [119/180] Step 100/312 Loss 3.843 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.665, lat_loss 21.798
09/28 11:21:54 PM | Valid: [119/180] Step 150/312 Loss 3.829 Prec@(1,3) (85.5%, 99.1%), ce_loss 0.665, lat_loss 21.798
09/28 11:21:58 PM | Valid: [119/180] Step 200/312 Loss 3.852 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.665, lat_loss 21.798
09/28 11:22:03 PM | Valid: [119/180] Step 250/312 Loss 3.766 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.665, lat_loss 21.798
09/28 11:22:07 PM | Valid: [119/180] Step 300/312 Loss 3.717 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.665, lat_loss 21.798
09/28 11:22:08 PM | Valid: [119/180] Step 312/312 Loss 3.718 Prec@(1,3) (85.7%, 99.3%), ce_loss 0.665, lat_loss 21.798
09/28 11:22:09 PM | val: [119/180] Final Prec@1 85.7400% Time 29.19
09/28 11:22:09 PM | Start to train weights for epoch 119
09/28 11:22:26 PM | Train: [120/180] Step 050/1249 Loss 2.318 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.665, lat_loss 21.798
09/28 11:22:42 PM | Train: [120/180] Step 100/1249 Loss 2.237 Prec@(1,3) (91.1%, 99.8%), ce_loss 0.665, lat_loss 21.798
09/28 11:22:58 PM | Train: [120/180] Step 150/1249 Loss 2.296 Prec@(1,3) (91.1%, 99.8%), ce_loss 0.665, lat_loss 21.798
09/28 11:23:14 PM | Train: [120/180] Step 200/1249 Loss 2.336 Prec@(1,3) (91.0%, 99.8%), ce_loss 0.665, lat_loss 21.798
09/28 11:23:31 PM | Train: [120/180] Step 250/1249 Loss 2.362 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.665, lat_loss 21.798
09/28 11:23:47 PM | Train: [120/180] Step 300/1249 Loss 2.354 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.665, lat_loss 21.798
09/28 11:24:03 PM | Train: [120/180] Step 350/1249 Loss 2.363 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:24:19 PM | Train: [120/180] Step 400/1249 Loss 2.363 Prec@(1,3) (90.6%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:24:35 PM | Train: [120/180] Step 450/1249 Loss 2.342 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:24:51 PM | Train: [120/180] Step 500/1249 Loss 2.333 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:25:06 PM | Train: [120/180] Step 550/1249 Loss 2.333 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:25:22 PM | Train: [120/180] Step 600/1249 Loss 2.383 Prec@(1,3) (90.6%, 99.7%), ce_loss 0.664, lat_loss 21.798
09/28 11:25:38 PM | Train: [120/180] Step 650/1249 Loss 2.387 Prec@(1,3) (90.6%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:25:54 PM | Train: [120/180] Step 700/1249 Loss 2.379 Prec@(1,3) (90.6%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:26:10 PM | Train: [120/180] Step 750/1249 Loss 2.366 Prec@(1,3) (90.7%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:26:26 PM | Train: [120/180] Step 800/1249 Loss 2.384 Prec@(1,3) (90.6%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:26:42 PM | Train: [120/180] Step 850/1249 Loss 2.401 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.664, lat_loss 21.798
09/28 11:26:58 PM | Train: [120/180] Step 900/1249 Loss 2.404 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.663, lat_loss 21.798
09/28 11:27:14 PM | Train: [120/180] Step 950/1249 Loss 2.409 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.663, lat_loss 21.798
09/28 11:27:30 PM | Train: [120/180] Step 1000/1249 Loss 2.398 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.663, lat_loss 21.798
09/28 11:27:47 PM | Train: [120/180] Step 1050/1249 Loss 2.407 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.663, lat_loss 21.798
09/28 11:28:02 PM | Train: [120/180] Step 1100/1249 Loss 2.402 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.663, lat_loss 21.798
09/28 11:28:19 PM | Train: [120/180] Step 1150/1249 Loss 2.386 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.663, lat_loss 21.798
09/28 11:28:35 PM | Train: [120/180] Step 1200/1249 Loss 2.411 Prec@(1,3) (90.5%, 99.8%), ce_loss 0.663, lat_loss 21.798
09/28 11:28:50 PM | Train: [120/180] Step 1249/1249 Loss 2.416 Prec@(1,3) (90.4%, 99.8%), ce_loss 0.663, lat_loss 21.798
09/28 11:28:50 PM | _w_step_train: [120/180] Final Prec@1 90.4150% Time 401.62
09/28 11:28:50 PM | Start to train theta for epoch 119
09/28 11:29:11 PM | Train: [120/180] Step 050/312 Loss 3.895 Prec@(1,3) (85.1%, 99.0%), ce_loss 0.663, lat_loss 21.798
09/28 11:29:32 PM | Train: [120/180] Step 100/312 Loss 4.076 Prec@(1,3) (84.9%, 99.1%), ce_loss 0.663, lat_loss 21.798
09/28 11:29:53 PM | Train: [120/180] Step 150/312 Loss 4.103 Prec@(1,3) (84.6%, 99.1%), ce_loss 0.663, lat_loss 21.798
09/28 11:30:13 PM | Train: [120/180] Step 200/312 Loss 4.031 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.663, lat_loss 21.798
09/28 11:30:33 PM | Train: [120/180] Step 250/312 Loss 3.913 Prec@(1,3) (85.2%, 99.1%), ce_loss 0.663, lat_loss 21.798
09/28 11:30:54 PM | Train: [120/180] Step 300/312 Loss 3.902 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.663, lat_loss 21.798
09/28 11:30:59 PM | Train: [120/180] Step 312/312 Loss 3.914 Prec@(1,3) (85.4%, 99.1%), ce_loss 0.663, lat_loss 21.798
09/28 11:30:59 PM | _theta_step_train: [120/180] Final Prec@1 85.3700% Time 128.71
09/28 11:31:04 PM | Valid: [120/180] Step 050/312 Loss 3.382 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.662, lat_loss 21.798
09/28 11:31:09 PM | Valid: [120/180] Step 100/312 Loss 3.655 Prec@(1,3) (86.7%, 99.0%), ce_loss 0.662, lat_loss 21.798
09/28 11:31:14 PM | Valid: [120/180] Step 150/312 Loss 3.799 Prec@(1,3) (86.2%, 98.9%), ce_loss 0.662, lat_loss 21.798
09/28 11:31:18 PM | Valid: [120/180] Step 200/312 Loss 3.824 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.662, lat_loss 21.798
09/28 11:31:23 PM | Valid: [120/180] Step 250/312 Loss 3.848 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.662, lat_loss 21.798
09/28 11:31:27 PM | Valid: [120/180] Step 300/312 Loss 3.759 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.662, lat_loss 21.798
09/28 11:31:28 PM | Valid: [120/180] Step 312/312 Loss 3.751 Prec@(1,3) (86.2%, 99.2%), ce_loss 0.662, lat_loss 21.798
09/28 11:31:28 PM | val: [120/180] Final Prec@1 86.2200% Time 29.49
09/28 11:31:28 PM | Start to train weights for epoch 120
09/28 11:31:55 PM | Train: [121/180] Step 050/1249 Loss 2.402 Prec@(1,3) (90.2%, 99.5%), ce_loss 0.662, lat_loss 21.798
09/28 11:32:20 PM | Train: [121/180] Step 100/1249 Loss 2.662 Prec@(1,3) (89.3%, 99.5%), ce_loss 0.662, lat_loss 21.797
09/28 11:32:44 PM | Train: [121/180] Step 150/1249 Loss 2.619 Prec@(1,3) (89.5%, 99.5%), ce_loss 0.662, lat_loss 21.797
09/28 11:33:09 PM | Train: [121/180] Step 200/1249 Loss 2.570 Prec@(1,3) (89.8%, 99.5%), ce_loss 0.662, lat_loss 21.797
09/28 11:33:33 PM | Train: [121/180] Step 250/1249 Loss 2.467 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.662, lat_loss 21.797
09/28 11:33:58 PM | Train: [121/180] Step 300/1249 Loss 2.427 Prec@(1,3) (90.4%, 99.6%), ce_loss 0.662, lat_loss 21.797
09/28 11:34:23 PM | Train: [121/180] Step 350/1249 Loss 2.391 Prec@(1,3) (90.5%, 99.6%), ce_loss 0.662, lat_loss 21.797
09/28 11:34:48 PM | Train: [121/180] Step 400/1249 Loss 2.393 Prec@(1,3) (90.5%, 99.6%), ce_loss 0.662, lat_loss 21.797
09/28 11:35:13 PM | Train: [121/180] Step 450/1249 Loss 2.360 Prec@(1,3) (90.5%, 99.7%), ce_loss 0.661, lat_loss 21.797
09/28 11:35:35 PM | Train: [121/180] Step 500/1249 Loss 2.384 Prec@(1,3) (90.5%, 99.7%), ce_loss 0.661, lat_loss 21.797
09/28 11:36:00 PM | Train: [121/180] Step 550/1249 Loss 2.366 Prec@(1,3) (90.6%, 99.7%), ce_loss 0.661, lat_loss 21.797
09/28 11:36:23 PM | Train: [121/180] Step 600/1249 Loss 2.366 Prec@(1,3) (90.6%, 99.7%), ce_loss 0.661, lat_loss 21.797
09/28 11:36:48 PM | Train: [121/180] Step 650/1249 Loss 2.365 Prec@(1,3) (90.5%, 99.7%), ce_loss 0.661, lat_loss 21.797
09/28 11:37:13 PM | Train: [121/180] Step 700/1249 Loss 2.361 Prec@(1,3) (90.6%, 99.7%), ce_loss 0.661, lat_loss 21.797
09/28 11:37:38 PM | Train: [121/180] Step 750/1249 Loss 2.432 Prec@(1,3) (90.3%, 99.7%), ce_loss 0.661, lat_loss 21.797
09/28 11:38:01 PM | Train: [121/180] Step 800/1249 Loss 2.450 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.661, lat_loss 21.797
09/28 11:38:26 PM | Train: [121/180] Step 850/1249 Loss 2.453 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.661, lat_loss 21.797
09/28 11:38:51 PM | Train: [121/180] Step 900/1249 Loss 2.435 Prec@(1,3) (90.4%, 99.6%), ce_loss 0.661, lat_loss 21.797
09/28 11:39:16 PM | Train: [121/180] Step 950/1249 Loss 2.413 Prec@(1,3) (90.5%, 99.7%), ce_loss 0.661, lat_loss 21.797
09/28 11:39:41 PM | Train: [121/180] Step 1000/1249 Loss 2.463 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.660, lat_loss 21.797
09/28 11:40:06 PM | Train: [121/180] Step 1050/1249 Loss 2.482 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.660, lat_loss 21.797
09/28 11:40:31 PM | Train: [121/180] Step 1100/1249 Loss 2.483 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.660, lat_loss 21.797
09/28 11:40:56 PM | Train: [121/180] Step 1150/1249 Loss 2.474 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.660, lat_loss 21.797
09/28 11:41:21 PM | Train: [121/180] Step 1200/1249 Loss 2.478 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.660, lat_loss 21.797
09/28 11:41:45 PM | Train: [121/180] Step 1249/1249 Loss 2.461 Prec@(1,3) (90.3%, 99.6%), ce_loss 0.660, lat_loss 21.797
09/28 11:41:45 PM | _w_step_train: [121/180] Final Prec@1 90.3350% Time 616.50
09/28 11:41:45 PM | Start to train theta for epoch 120
09/28 11:42:06 PM | Train: [121/180] Step 050/312 Loss 4.496 Prec@(1,3) (84.8%, 99.3%), ce_loss 0.660, lat_loss 21.797
09/28 11:42:27 PM | Train: [121/180] Step 100/312 Loss 4.270 Prec@(1,3) (85.3%, 99.4%), ce_loss 0.660, lat_loss 21.797
09/28 11:42:48 PM | Train: [121/180] Step 150/312 Loss 4.187 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.660, lat_loss 21.797
09/28 11:43:09 PM | Train: [121/180] Step 200/312 Loss 4.189 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.660, lat_loss 21.797
09/28 11:43:23 PM | Train: [121/180] Step 250/312 Loss 4.140 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.660, lat_loss 21.797
09/28 11:43:35 PM | Train: [121/180] Step 300/312 Loss 4.121 Prec@(1,3) (85.3%, 99.2%), ce_loss 0.660, lat_loss 21.797
09/28 11:43:38 PM | Train: [121/180] Step 312/312 Loss 4.088 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.660, lat_loss 21.797
09/28 11:43:38 PM | _theta_step_train: [121/180] Final Prec@1 85.3800% Time 113.53
09/28 11:43:44 PM | Valid: [121/180] Step 050/312 Loss 3.317 Prec@(1,3) (87.5%, 99.7%), ce_loss 0.660, lat_loss 21.797
09/28 11:43:48 PM | Valid: [121/180] Step 100/312 Loss 3.623 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.660, lat_loss 21.797
09/28 11:43:53 PM | Valid: [121/180] Step 150/312 Loss 3.704 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.660, lat_loss 21.797
09/28 11:43:57 PM | Valid: [121/180] Step 200/312 Loss 3.666 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.660, lat_loss 21.797
09/28 11:44:02 PM | Valid: [121/180] Step 250/312 Loss 3.725 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.660, lat_loss 21.797
09/28 11:44:07 PM | Valid: [121/180] Step 300/312 Loss 3.701 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.659, lat_loss 21.797
09/28 11:44:08 PM | Valid: [121/180] Step 312/312 Loss 3.698 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.659, lat_loss 21.797
09/28 11:44:08 PM | val: [121/180] Final Prec@1 86.0800% Time 29.26
09/28 11:44:08 PM | Start to train weights for epoch 121
09/28 11:44:34 PM | Train: [122/180] Step 050/1249 Loss 2.108 Prec@(1,3) (91.2%, 99.9%), ce_loss 0.659, lat_loss 21.797
09/28 11:44:59 PM | Train: [122/180] Step 100/1249 Loss 2.072 Prec@(1,3) (91.6%, 99.9%), ce_loss 0.659, lat_loss 21.797
09/28 11:45:24 PM | Train: [122/180] Step 150/1249 Loss 2.108 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.659, lat_loss 21.797
09/28 11:45:48 PM | Train: [122/180] Step 200/1249 Loss 2.199 Prec@(1,3) (91.4%, 99.8%), ce_loss 0.659, lat_loss 21.797
09/28 11:46:13 PM | Train: [122/180] Step 250/1249 Loss 2.270 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.659, lat_loss 21.797
09/28 11:46:38 PM | Train: [122/180] Step 300/1249 Loss 2.320 Prec@(1,3) (90.9%, 99.8%), ce_loss 0.659, lat_loss 21.797
09/28 11:47:02 PM | Train: [122/180] Step 350/1249 Loss 2.310 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.659, lat_loss 21.797
09/28 11:47:27 PM | Train: [122/180] Step 400/1249 Loss 2.296 Prec@(1,3) (91.2%, 99.7%), ce_loss 0.659, lat_loss 21.797
09/28 11:47:51 PM | Train: [122/180] Step 450/1249 Loss 2.267 Prec@(1,3) (91.3%, 99.7%), ce_loss 0.659, lat_loss 21.797
09/28 11:48:16 PM | Train: [122/180] Step 500/1249 Loss 2.288 Prec@(1,3) (91.2%, 99.7%), ce_loss 0.659, lat_loss 21.797
09/28 11:48:41 PM | Train: [122/180] Step 550/1249 Loss 2.271 Prec@(1,3) (91.3%, 99.7%), ce_loss 0.658, lat_loss 21.797
09/28 11:49:06 PM | Train: [122/180] Step 600/1249 Loss 2.243 Prec@(1,3) (91.3%, 99.7%), ce_loss 0.658, lat_loss 21.796
09/28 11:49:31 PM | Train: [122/180] Step 650/1249 Loss 2.236 Prec@(1,3) (91.4%, 99.8%), ce_loss 0.658, lat_loss 21.796
09/28 11:49:55 PM | Train: [122/180] Step 700/1249 Loss 2.232 Prec@(1,3) (91.4%, 99.8%), ce_loss 0.658, lat_loss 21.796
09/28 11:50:18 PM | Train: [122/180] Step 750/1249 Loss 2.220 Prec@(1,3) (91.4%, 99.8%), ce_loss 0.658, lat_loss 21.796
09/28 11:50:41 PM | Train: [122/180] Step 800/1249 Loss 2.229 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.658, lat_loss 21.796
09/28 11:51:04 PM | Train: [122/180] Step 850/1249 Loss 2.240 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.658, lat_loss 21.796
09/28 11:51:26 PM | Train: [122/180] Step 900/1249 Loss 2.262 Prec@(1,3) (91.1%, 99.8%), ce_loss 0.658, lat_loss 21.796
09/28 11:51:47 PM | Train: [122/180] Step 950/1249 Loss 2.288 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.658, lat_loss 21.796
09/28 11:52:10 PM | Train: [122/180] Step 1000/1249 Loss 2.320 Prec@(1,3) (90.9%, 99.7%), ce_loss 0.658, lat_loss 21.796
09/28 11:52:30 PM | Train: [122/180] Step 1050/1249 Loss 2.334 Prec@(1,3) (90.8%, 99.7%), ce_loss 0.658, lat_loss 21.796
09/28 11:52:46 PM | Train: [122/180] Step 1100/1249 Loss 2.368 Prec@(1,3) (90.7%, 99.7%), ce_loss 0.658, lat_loss 21.796
09/28 11:53:06 PM | Train: [122/180] Step 1150/1249 Loss 2.376 Prec@(1,3) (90.7%, 99.7%), ce_loss 0.657, lat_loss 21.796
09/28 11:53:31 PM | Train: [122/180] Step 1200/1249 Loss 2.376 Prec@(1,3) (90.7%, 99.7%), ce_loss 0.657, lat_loss 21.796
09/28 11:53:55 PM | Train: [122/180] Step 1249/1249 Loss 2.381 Prec@(1,3) (90.7%, 99.7%), ce_loss 0.657, lat_loss 21.796
09/28 11:53:55 PM | _w_step_train: [122/180] Final Prec@1 90.6650% Time 587.72
09/28 11:53:55 PM | Start to train theta for epoch 121
09/28 11:54:15 PM | Train: [122/180] Step 050/312 Loss 3.817 Prec@(1,3) (85.7%, 99.3%), ce_loss 0.657, lat_loss 21.796
09/28 11:54:36 PM | Train: [122/180] Step 100/312 Loss 4.051 Prec@(1,3) (85.3%, 99.0%), ce_loss 0.657, lat_loss 21.796
09/28 11:54:57 PM | Train: [122/180] Step 150/312 Loss 4.147 Prec@(1,3) (85.1%, 99.2%), ce_loss 0.657, lat_loss 21.796
09/28 11:55:17 PM | Train: [122/180] Step 200/312 Loss 4.006 Prec@(1,3) (85.2%, 99.3%), ce_loss 0.657, lat_loss 21.796
09/28 11:55:38 PM | Train: [122/180] Step 250/312 Loss 3.947 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.657, lat_loss 21.796
09/28 11:55:58 PM | Train: [122/180] Step 300/312 Loss 3.927 Prec@(1,3) (85.5%, 99.3%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:03 PM | Train: [122/180] Step 312/312 Loss 3.916 Prec@(1,3) (85.6%, 99.3%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:03 PM | _theta_step_train: [122/180] Final Prec@1 85.5900% Time 127.95
09/28 11:56:09 PM | Valid: [122/180] Step 050/312 Loss 3.946 Prec@(1,3) (86.7%, 99.0%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:14 PM | Valid: [122/180] Step 100/312 Loss 3.894 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:18 PM | Valid: [122/180] Step 150/312 Loss 4.130 Prec@(1,3) (85.8%, 98.8%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:23 PM | Valid: [122/180] Step 200/312 Loss 4.126 Prec@(1,3) (85.6%, 98.9%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:27 PM | Valid: [122/180] Step 250/312 Loss 3.992 Prec@(1,3) (85.6%, 99.0%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:32 PM | Valid: [122/180] Step 300/312 Loss 3.885 Prec@(1,3) (86.0%, 99.1%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:33 PM | Valid: [122/180] Step 312/312 Loss 3.915 Prec@(1,3) (85.7%, 99.0%), ce_loss 0.657, lat_loss 21.796
09/28 11:56:33 PM | val: [122/180] Final Prec@1 85.7400% Time 29.93
09/28 11:56:33 PM | Start to train weights for epoch 122
09/28 11:56:58 PM | Train: [123/180] Step 050/1249 Loss 2.247 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.657, lat_loss 21.796
09/28 11:57:20 PM | Train: [123/180] Step 100/1249 Loss 2.334 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.657, lat_loss 21.796
09/28 11:57:41 PM | Train: [123/180] Step 150/1249 Loss 2.348 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/28 11:58:05 PM | Train: [123/180] Step 200/1249 Loss 2.380 Prec@(1,3) (91.0%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/28 11:58:28 PM | Train: [123/180] Step 250/1249 Loss 2.297 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/28 11:58:50 PM | Train: [123/180] Step 300/1249 Loss 2.285 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/28 11:59:11 PM | Train: [123/180] Step 350/1249 Loss 2.259 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/28 11:59:33 PM | Train: [123/180] Step 400/1249 Loss 2.245 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/28 11:59:55 PM | Train: [123/180] Step 450/1249 Loss 2.262 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/29 12:00:16 AM | Train: [123/180] Step 500/1249 Loss 2.224 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/29 12:00:38 AM | Train: [123/180] Step 550/1249 Loss 2.272 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/29 12:01:01 AM | Train: [123/180] Step 600/1249 Loss 2.291 Prec@(1,3) (91.1%, 99.8%), ce_loss 0.656, lat_loss 21.796
09/29 12:01:22 AM | Train: [123/180] Step 650/1249 Loss 2.351 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.656, lat_loss 21.796
09/29 12:01:43 AM | Train: [123/180] Step 700/1249 Loss 2.340 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.656, lat_loss 21.796
09/29 12:02:05 AM | Train: [123/180] Step 750/1249 Loss 2.355 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.655, lat_loss 21.796
09/29 12:02:28 AM | Train: [123/180] Step 800/1249 Loss 2.349 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.655, lat_loss 21.796
09/29 12:02:51 AM | Train: [123/180] Step 850/1249 Loss 2.334 Prec@(1,3) (91.1%, 99.7%), ce_loss 0.655, lat_loss 21.796
09/29 12:03:14 AM | Train: [123/180] Step 900/1249 Loss 2.347 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.655, lat_loss 21.796
09/29 12:03:36 AM | Train: [123/180] Step 950/1249 Loss 2.369 Prec@(1,3) (90.9%, 99.7%), ce_loss 0.655, lat_loss 21.796
09/29 12:03:58 AM | Train: [123/180] Step 1000/1249 Loss 2.369 Prec@(1,3) (90.8%, 99.7%), ce_loss 0.655, lat_loss 21.795
09/29 12:04:20 AM | Train: [123/180] Step 1050/1249 Loss 2.366 Prec@(1,3) (90.8%, 99.7%), ce_loss 0.655, lat_loss 21.795
09/29 12:04:43 AM | Train: [123/180] Step 1100/1249 Loss 2.350 Prec@(1,3) (90.9%, 99.7%), ce_loss 0.655, lat_loss 21.795
09/29 12:05:05 AM | Train: [123/180] Step 1150/1249 Loss 2.351 Prec@(1,3) (90.9%, 99.7%), ce_loss 0.655, lat_loss 21.795
09/29 12:05:28 AM | Train: [123/180] Step 1200/1249 Loss 2.358 Prec@(1,3) (90.8%, 99.7%), ce_loss 0.655, lat_loss 21.795
09/29 12:05:53 AM | Train: [123/180] Step 1249/1249 Loss 2.371 Prec@(1,3) (90.8%, 99.7%), ce_loss 0.655, lat_loss 21.795
09/29 12:05:53 AM | _w_step_train: [123/180] Final Prec@1 90.7900% Time 559.43
09/29 12:05:53 AM | Start to train theta for epoch 122
09/29 12:06:14 AM | Train: [123/180] Step 050/312 Loss 4.571 Prec@(1,3) (83.1%, 99.2%), ce_loss 0.655, lat_loss 21.795
09/29 12:06:35 AM | Train: [123/180] Step 100/312 Loss 4.220 Prec@(1,3) (84.0%, 99.3%), ce_loss 0.654, lat_loss 21.795
09/29 12:06:56 AM | Train: [123/180] Step 150/312 Loss 4.148 Prec@(1,3) (84.2%, 99.3%), ce_loss 0.654, lat_loss 21.795
09/29 12:07:17 AM | Train: [123/180] Step 200/312 Loss 4.003 Prec@(1,3) (84.7%, 99.3%), ce_loss 0.654, lat_loss 21.795
09/29 12:07:38 AM | Train: [123/180] Step 250/312 Loss 3.951 Prec@(1,3) (85.0%, 99.3%), ce_loss 0.654, lat_loss 21.795
09/29 12:07:59 AM | Train: [123/180] Step 300/312 Loss 3.966 Prec@(1,3) (85.1%, 99.3%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:05 AM | Train: [123/180] Step 312/312 Loss 3.950 Prec@(1,3) (85.2%, 99.3%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:05 AM | _theta_step_train: [123/180] Final Prec@1 85.1600% Time 131.75
09/29 12:08:10 AM | Valid: [123/180] Step 050/312 Loss 4.015 Prec@(1,3) (85.5%, 99.4%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:15 AM | Valid: [123/180] Step 100/312 Loss 4.138 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:19 AM | Valid: [123/180] Step 150/312 Loss 3.971 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:24 AM | Valid: [123/180] Step 200/312 Loss 3.866 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:28 AM | Valid: [123/180] Step 250/312 Loss 3.783 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:33 AM | Valid: [123/180] Step 300/312 Loss 3.697 Prec@(1,3) (86.0%, 99.4%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:34 AM | Valid: [123/180] Step 312/312 Loss 3.694 Prec@(1,3) (86.0%, 99.4%), ce_loss 0.654, lat_loss 21.795
09/29 12:08:34 AM | val: [123/180] Final Prec@1 85.9900% Time 29.64
09/29 12:08:34 AM | Start to train weights for epoch 123
09/29 12:08:57 AM | Train: [124/180] Step 050/1249 Loss 2.103 Prec@(1,3) (91.3%, 100.0%), ce_loss 0.654, lat_loss 21.795
09/29 12:09:21 AM | Train: [124/180] Step 100/1249 Loss 2.044 Prec@(1,3) (91.4%, 99.8%), ce_loss 0.654, lat_loss 21.795
09/29 12:09:46 AM | Train: [124/180] Step 150/1249 Loss 2.060 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.654, lat_loss 21.795
09/29 12:10:11 AM | Train: [124/180] Step 200/1249 Loss 2.033 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.654, lat_loss 21.795
09/29 12:10:36 AM | Train: [124/180] Step 250/1249 Loss 2.139 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.654, lat_loss 21.795
09/29 12:11:01 AM | Train: [124/180] Step 300/1249 Loss 2.125 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.653, lat_loss 21.795
09/29 12:11:25 AM | Train: [124/180] Step 350/1249 Loss 2.119 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.653, lat_loss 21.795
09/29 12:11:45 AM | Train: [124/180] Step 400/1249 Loss 2.145 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.653, lat_loss 21.795
09/29 12:12:05 AM | Train: [124/180] Step 450/1249 Loss 2.169 Prec@(1,3) (91.4%, 99.7%), ce_loss 0.653, lat_loss 21.795
09/29 12:12:23 AM | Train: [124/180] Step 500/1249 Loss 2.222 Prec@(1,3) (91.2%, 99.7%), ce_loss 0.653, lat_loss 21.795
09/29 12:12:42 AM | Train: [124/180] Step 550/1249 Loss 2.224 Prec@(1,3) (91.1%, 99.7%), ce_loss 0.653, lat_loss 21.795
09/29 12:13:01 AM | Train: [124/180] Step 600/1249 Loss 2.223 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.653, lat_loss 21.795
09/29 12:13:20 AM | Train: [124/180] Step 650/1249 Loss 2.247 Prec@(1,3) (91.1%, 99.8%), ce_loss 0.653, lat_loss 21.795
09/29 12:13:39 AM | Train: [124/180] Step 700/1249 Loss 2.234 Prec@(1,3) (91.1%, 99.8%), ce_loss 0.653, lat_loss 21.795
09/29 12:13:58 AM | Train: [124/180] Step 750/1249 Loss 2.292 Prec@(1,3) (91.1%, 99.7%), ce_loss 0.653, lat_loss 21.795
09/29 12:14:18 AM | Train: [124/180] Step 800/1249 Loss 2.292 Prec@(1,3) (91.1%, 99.7%), ce_loss 0.653, lat_loss 21.795
09/29 12:14:43 AM | Train: [124/180] Step 850/1249 Loss 2.292 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.653, lat_loss 21.795
09/29 12:15:08 AM | Train: [124/180] Step 900/1249 Loss 2.301 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.652, lat_loss 21.795
09/29 12:15:33 AM | Train: [124/180] Step 950/1249 Loss 2.295 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.652, lat_loss 21.795
09/29 12:15:57 AM | Train: [124/180] Step 1000/1249 Loss 2.289 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.652, lat_loss 21.795
09/29 12:16:22 AM | Train: [124/180] Step 1050/1249 Loss 2.287 Prec@(1,3) (91.1%, 99.7%), ce_loss 0.652, lat_loss 21.795
09/29 12:16:47 AM | Train: [124/180] Step 1100/1249 Loss 2.310 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.652, lat_loss 21.795
09/29 12:17:11 AM | Train: [124/180] Step 1150/1249 Loss 2.307 Prec@(1,3) (91.0%, 99.7%), ce_loss 0.652, lat_loss 21.795
09/29 12:17:36 AM | Train: [124/180] Step 1200/1249 Loss 2.312 Prec@(1,3) (90.9%, 99.7%), ce_loss 0.652, lat_loss 21.795
09/29 12:18:00 AM | Train: [124/180] Step 1249/1249 Loss 2.308 Prec@(1,3) (90.9%, 99.7%), ce_loss 0.652, lat_loss 21.795
09/29 12:18:00 AM | _w_step_train: [124/180] Final Prec@1 90.9300% Time 566.03
09/29 12:18:00 AM | Start to train theta for epoch 123
09/29 12:18:22 AM | Train: [124/180] Step 050/312 Loss 4.327 Prec@(1,3) (84.5%, 99.2%), ce_loss 0.652, lat_loss 21.795
09/29 12:18:43 AM | Train: [124/180] Step 100/312 Loss 3.874 Prec@(1,3) (84.9%, 99.5%), ce_loss 0.652, lat_loss 21.795
09/29 12:19:04 AM | Train: [124/180] Step 150/312 Loss 4.042 Prec@(1,3) (84.8%, 99.5%), ce_loss 0.652, lat_loss 21.795
09/29 12:19:24 AM | Train: [124/180] Step 200/312 Loss 4.011 Prec@(1,3) (85.1%, 99.4%), ce_loss 0.652, lat_loss 21.795
09/29 12:19:45 AM | Train: [124/180] Step 250/312 Loss 3.966 Prec@(1,3) (85.3%, 99.4%), ce_loss 0.652, lat_loss 21.795
09/29 12:20:06 AM | Train: [124/180] Step 300/312 Loss 3.888 Prec@(1,3) (85.6%, 99.4%), ce_loss 0.652, lat_loss 21.794
09/29 12:20:11 AM | Train: [124/180] Step 312/312 Loss 3.869 Prec@(1,3) (85.7%, 99.4%), ce_loss 0.652, lat_loss 21.794
09/29 12:20:11 AM | _theta_step_train: [124/180] Final Prec@1 85.7300% Time 130.41
09/29 12:20:16 AM | Valid: [124/180] Step 050/312 Loss 3.243 Prec@(1,3) (87.7%, 99.5%), ce_loss 0.651, lat_loss 21.794
09/29 12:20:20 AM | Valid: [124/180] Step 100/312 Loss 3.721 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.651, lat_loss 21.794
09/29 12:20:24 AM | Valid: [124/180] Step 150/312 Loss 3.694 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.651, lat_loss 21.794
09/29 12:20:28 AM | Valid: [124/180] Step 200/312 Loss 3.785 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.651, lat_loss 21.794
09/29 12:20:33 AM | Valid: [124/180] Step 250/312 Loss 3.698 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.651, lat_loss 21.794
09/29 12:20:37 AM | Valid: [124/180] Step 300/312 Loss 3.693 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.651, lat_loss 21.794
09/29 12:20:38 AM | Valid: [124/180] Step 312/312 Loss 3.695 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.651, lat_loss 21.794
09/29 12:20:38 AM | val: [124/180] Final Prec@1 86.2800% Time 27.06
09/29 12:20:38 AM | Start to train weights for epoch 124
09/29 12:21:04 AM | Train: [125/180] Step 050/1249 Loss 1.990 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.651, lat_loss 21.794
09/29 12:21:27 AM | Train: [125/180] Step 100/1249 Loss 1.997 Prec@(1,3) (91.8%, 99.9%), ce_loss 0.651, lat_loss 21.794
09/29 12:21:52 AM | Train: [125/180] Step 150/1249 Loss 1.969 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.651, lat_loss 21.794
09/29 12:22:17 AM | Train: [125/180] Step 200/1249 Loss 1.995 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.651, lat_loss 21.794
09/29 12:22:42 AM | Train: [125/180] Step 250/1249 Loss 2.080 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.651, lat_loss 21.794
09/29 12:23:07 AM | Train: [125/180] Step 300/1249 Loss 2.108 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.651, lat_loss 21.794
09/29 12:23:31 AM | Train: [125/180] Step 350/1249 Loss 2.138 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.651, lat_loss 21.794
09/29 12:23:56 AM | Train: [125/180] Step 400/1249 Loss 2.128 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.651, lat_loss 21.794
09/29 12:24:21 AM | Train: [125/180] Step 450/1249 Loss 2.166 Prec@(1,3) (91.4%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:24:46 AM | Train: [125/180] Step 500/1249 Loss 2.157 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:25:11 AM | Train: [125/180] Step 550/1249 Loss 2.143 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:25:35 AM | Train: [125/180] Step 600/1249 Loss 2.169 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:25:59 AM | Train: [125/180] Step 650/1249 Loss 2.176 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:26:24 AM | Train: [125/180] Step 700/1249 Loss 2.193 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:26:49 AM | Train: [125/180] Step 750/1249 Loss 2.196 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:27:12 AM | Train: [125/180] Step 800/1249 Loss 2.176 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:27:37 AM | Train: [125/180] Step 850/1249 Loss 2.184 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:28:02 AM | Train: [125/180] Step 900/1249 Loss 2.180 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:28:26 AM | Train: [125/180] Step 950/1249 Loss 2.177 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.650, lat_loss 21.794
09/29 12:28:51 AM | Train: [125/180] Step 1000/1249 Loss 2.180 Prec@(1,3) (91.3%, 99.8%), ce_loss 0.649, lat_loss 21.794
09/29 12:29:16 AM | Train: [125/180] Step 1050/1249 Loss 2.194 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.649, lat_loss 21.794
09/29 12:29:41 AM | Train: [125/180] Step 1100/1249 Loss 2.230 Prec@(1,3) (91.1%, 99.8%), ce_loss 0.649, lat_loss 21.794
09/29 12:30:06 AM | Train: [125/180] Step 1150/1249 Loss 2.230 Prec@(1,3) (91.1%, 99.8%), ce_loss 0.649, lat_loss 21.794
09/29 12:30:25 AM | Train: [125/180] Step 1200/1249 Loss 2.218 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.649, lat_loss 21.794
09/29 12:30:42 AM | Train: [125/180] Step 1249/1249 Loss 2.214 Prec@(1,3) (91.2%, 99.8%), ce_loss 0.649, lat_loss 21.794
09/29 12:30:42 AM | _w_step_train: [125/180] Final Prec@1 91.1800% Time 604.27
09/29 12:30:42 AM | Start to train theta for epoch 124
09/29 12:31:04 AM | Train: [125/180] Step 050/312 Loss 3.759 Prec@(1,3) (85.1%, 99.1%), ce_loss 0.649, lat_loss 21.794
09/29 12:31:17 AM | Train: [125/180] Step 100/312 Loss 3.943 Prec@(1,3) (85.0%, 99.2%), ce_loss 0.649, lat_loss 21.794
09/29 12:31:30 AM | Train: [125/180] Step 150/312 Loss 3.872 Prec@(1,3) (85.5%, 99.3%), ce_loss 0.649, lat_loss 21.794
09/29 12:31:43 AM | Train: [125/180] Step 200/312 Loss 3.891 Prec@(1,3) (85.4%, 99.3%), ce_loss 0.649, lat_loss 21.794
09/29 12:31:56 AM | Train: [125/180] Step 250/312 Loss 3.877 Prec@(1,3) (85.6%, 99.3%), ce_loss 0.649, lat_loss 21.794
09/29 12:32:10 AM | Train: [125/180] Step 300/312 Loss 3.895 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.649, lat_loss 21.794
09/29 12:32:13 AM | Train: [125/180] Step 312/312 Loss 3.902 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.649, lat_loss 21.794
09/29 12:32:13 AM | _theta_step_train: [125/180] Final Prec@1 85.5000% Time 90.77
09/29 12:32:18 AM | Valid: [125/180] Step 050/312 Loss 3.801 Prec@(1,3) (86.8%, 99.1%), ce_loss 0.649, lat_loss 21.794
09/29 12:32:23 AM | Valid: [125/180] Step 100/312 Loss 4.323 Prec@(1,3) (84.6%, 98.6%), ce_loss 0.649, lat_loss 21.794
09/29 12:32:28 AM | Valid: [125/180] Step 150/312 Loss 4.091 Prec@(1,3) (85.3%, 98.8%), ce_loss 0.649, lat_loss 21.794
09/29 12:32:33 AM | Valid: [125/180] Step 200/312 Loss 3.937 Prec@(1,3) (85.7%, 99.0%), ce_loss 0.649, lat_loss 21.794
09/29 12:32:37 AM | Valid: [125/180] Step 250/312 Loss 3.836 Prec@(1,3) (86.0%, 99.1%), ce_loss 0.649, lat_loss 21.794
09/29 12:32:42 AM | Valid: [125/180] Step 300/312 Loss 3.774 Prec@(1,3) (86.0%, 99.1%), ce_loss 0.648, lat_loss 21.794
09/29 12:32:43 AM | Valid: [125/180] Step 312/312 Loss 3.751 Prec@(1,3) (86.0%, 99.2%), ce_loss 0.648, lat_loss 21.794
09/29 12:32:43 AM | val: [125/180] Final Prec@1 86.0100% Time 30.43
09/29 12:32:43 AM | Start to train weights for epoch 125
09/29 12:33:06 AM | Train: [126/180] Step 050/1249 Loss 1.957 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.648, lat_loss 21.794
09/29 12:33:24 AM | Train: [126/180] Step 100/1249 Loss 1.973 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.648, lat_loss 21.794
09/29 12:33:44 AM | Train: [126/180] Step 150/1249 Loss 2.062 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.648, lat_loss 21.794
09/29 12:34:06 AM | Train: [126/180] Step 200/1249 Loss 2.043 Prec@(1,3) (92.4%, 99.7%), ce_loss 0.648, lat_loss 21.794
09/29 12:34:23 AM | Train: [126/180] Step 250/1249 Loss 2.074 Prec@(1,3) (92.4%, 99.7%), ce_loss 0.648, lat_loss 21.794
09/29 12:34:41 AM | Train: [126/180] Step 300/1249 Loss 2.085 Prec@(1,3) (92.2%, 99.7%), ce_loss 0.648, lat_loss 21.794
09/29 12:34:59 AM | Train: [126/180] Step 350/1249 Loss 2.053 Prec@(1,3) (92.2%, 99.7%), ce_loss 0.648, lat_loss 21.794
09/29 12:35:24 AM | Train: [126/180] Step 400/1249 Loss 2.029 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.648, lat_loss 21.794
09/29 12:35:51 AM | Train: [126/180] Step 450/1249 Loss 2.050 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.648, lat_loss 21.793
09/29 12:36:18 AM | Train: [126/180] Step 500/1249 Loss 2.081 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.648, lat_loss 21.793
09/29 12:36:42 AM | Train: [126/180] Step 550/1249 Loss 2.097 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.648, lat_loss 21.793
09/29 12:37:07 AM | Train: [126/180] Step 600/1249 Loss 2.165 Prec@(1,3) (91.7%, 99.7%), ce_loss 0.647, lat_loss 21.793
09/29 12:37:31 AM | Train: [126/180] Step 650/1249 Loss 2.177 Prec@(1,3) (91.6%, 99.7%), ce_loss 0.647, lat_loss 21.793
09/29 12:37:58 AM | Train: [126/180] Step 700/1249 Loss 2.163 Prec@(1,3) (91.7%, 99.7%), ce_loss 0.647, lat_loss 21.793
09/29 12:38:26 AM | Train: [126/180] Step 750/1249 Loss 2.150 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.647, lat_loss 21.793
09/29 12:38:53 AM | Train: [126/180] Step 800/1249 Loss 2.162 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.647, lat_loss 21.793
09/29 12:39:22 AM | Train: [126/180] Step 850/1249 Loss 2.164 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.647, lat_loss 21.793
09/29 12:39:48 AM | Train: [126/180] Step 900/1249 Loss 2.174 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.647, lat_loss 21.793
09/29 12:40:14 AM | Train: [126/180] Step 950/1249 Loss 2.166 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.647, lat_loss 21.793
09/29 12:40:41 AM | Train: [126/180] Step 1000/1249 Loss 2.160 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.647, lat_loss 21.793
09/29 12:41:07 AM | Train: [126/180] Step 1050/1249 Loss 2.162 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.647, lat_loss 21.793
09/29 12:41:32 AM | Train: [126/180] Step 1100/1249 Loss 2.167 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.647, lat_loss 21.793
09/29 12:42:00 AM | Train: [126/180] Step 1150/1249 Loss 2.162 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.646, lat_loss 21.793
09/29 12:42:28 AM | Train: [126/180] Step 1200/1249 Loss 2.166 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.646, lat_loss 21.793
09/29 12:42:55 AM | Train: [126/180] Step 1249/1249 Loss 2.158 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.646, lat_loss 21.793
09/29 12:42:55 AM | _w_step_train: [126/180] Final Prec@1 91.5825% Time 611.61
09/29 12:42:55 AM | Start to train theta for epoch 125
09/29 12:43:21 AM | Train: [126/180] Step 050/312 Loss 3.751 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.646, lat_loss 21.793
09/29 12:43:45 AM | Train: [126/180] Step 100/312 Loss 4.315 Prec@(1,3) (85.2%, 98.9%), ce_loss 0.646, lat_loss 21.793
09/29 12:44:08 AM | Train: [126/180] Step 150/312 Loss 4.256 Prec@(1,3) (85.2%, 99.0%), ce_loss 0.646, lat_loss 21.793
09/29 12:44:31 AM | Train: [126/180] Step 200/312 Loss 4.212 Prec@(1,3) (85.3%, 99.1%), ce_loss 0.646, lat_loss 21.793
09/29 12:44:55 AM | Train: [126/180] Step 250/312 Loss 4.120 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:18 AM | Train: [126/180] Step 300/312 Loss 4.069 Prec@(1,3) (85.7%, 99.2%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:22 AM | Train: [126/180] Step 312/312 Loss 4.068 Prec@(1,3) (85.7%, 99.2%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:22 AM | _theta_step_train: [126/180] Final Prec@1 85.6600% Time 147.42
09/29 12:45:28 AM | Valid: [126/180] Step 050/312 Loss 3.645 Prec@(1,3) (87.2%, 99.6%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:34 AM | Valid: [126/180] Step 100/312 Loss 3.850 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:39 AM | Valid: [126/180] Step 150/312 Loss 3.979 Prec@(1,3) (86.0%, 99.1%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:45 AM | Valid: [126/180] Step 200/312 Loss 3.932 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:50 AM | Valid: [126/180] Step 250/312 Loss 3.839 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:56 AM | Valid: [126/180] Step 300/312 Loss 3.817 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:57 AM | Valid: [126/180] Step 312/312 Loss 3.808 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.646, lat_loss 21.793
09/29 12:45:57 AM | val: [126/180] Final Prec@1 86.2800% Time 34.90
09/29 12:45:57 AM | Start to train weights for epoch 126
09/29 12:46:28 AM | Train: [127/180] Step 050/1249 Loss 1.871 Prec@(1,3) (92.3%, 99.9%), ce_loss 0.646, lat_loss 21.793
09/29 12:46:56 AM | Train: [127/180] Step 100/1249 Loss 1.865 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.646, lat_loss 21.793
09/29 12:47:23 AM | Train: [127/180] Step 150/1249 Loss 1.884 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:47:50 AM | Train: [127/180] Step 200/1249 Loss 1.933 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:48:21 AM | Train: [127/180] Step 250/1249 Loss 1.994 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:48:52 AM | Train: [127/180] Step 300/1249 Loss 2.059 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:49:18 AM | Train: [127/180] Step 350/1249 Loss 2.094 Prec@(1,3) (91.8%, 99.7%), ce_loss 0.645, lat_loss 21.793
09/29 12:49:46 AM | Train: [127/180] Step 400/1249 Loss 2.090 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:50:14 AM | Train: [127/180] Step 450/1249 Loss 2.099 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:50:42 AM | Train: [127/180] Step 500/1249 Loss 2.088 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:51:09 AM | Train: [127/180] Step 550/1249 Loss 2.123 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:51:36 AM | Train: [127/180] Step 600/1249 Loss 2.110 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:52:05 AM | Train: [127/180] Step 650/1249 Loss 2.095 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:52:34 AM | Train: [127/180] Step 700/1249 Loss 2.086 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.645, lat_loss 21.793
09/29 12:53:06 AM | Train: [127/180] Step 750/1249 Loss 2.115 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:53:34 AM | Train: [127/180] Step 800/1249 Loss 2.122 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:54:07 AM | Train: [127/180] Step 850/1249 Loss 2.164 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:54:42 AM | Train: [127/180] Step 900/1249 Loss 2.167 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:55:17 AM | Train: [127/180] Step 950/1249 Loss 2.161 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:55:53 AM | Train: [127/180] Step 1000/1249 Loss 2.165 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:56:26 AM | Train: [127/180] Step 1050/1249 Loss 2.169 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:57:03 AM | Train: [127/180] Step 1100/1249 Loss 2.177 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:57:44 AM | Train: [127/180] Step 1150/1249 Loss 2.179 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:58:15 AM | Train: [127/180] Step 1200/1249 Loss 2.180 Prec@(1,3) (91.5%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:58:51 AM | Train: [127/180] Step 1249/1249 Loss 2.181 Prec@(1,3) (91.4%, 99.8%), ce_loss 0.644, lat_loss 21.793
09/29 12:58:51 AM | _w_step_train: [127/180] Final Prec@1 91.4250% Time 773.85
09/29 12:58:51 AM | Start to train theta for epoch 126
09/29 12:59:25 AM | Train: [127/180] Step 050/312 Loss 3.840 Prec@(1,3) (85.2%, 99.5%), ce_loss 0.644, lat_loss 21.793
09/29 12:59:59 AM | Train: [127/180] Step 100/312 Loss 3.812 Prec@(1,3) (85.4%, 99.4%), ce_loss 0.644, lat_loss 21.793
09/29 01:00:35 AM | Train: [127/180] Step 150/312 Loss 3.849 Prec@(1,3) (85.6%, 99.3%), ce_loss 0.644, lat_loss 21.793
09/29 01:01:14 AM | Train: [127/180] Step 200/312 Loss 3.791 Prec@(1,3) (85.8%, 99.5%), ce_loss 0.643, lat_loss 21.793
09/29 01:01:54 AM | Train: [127/180] Step 250/312 Loss 4.006 Prec@(1,3) (85.5%, 99.3%), ce_loss 0.643, lat_loss 21.793
09/29 01:02:34 AM | Train: [127/180] Step 300/312 Loss 3.938 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.643, lat_loss 21.793
09/29 01:02:43 AM | Train: [127/180] Step 312/312 Loss 3.983 Prec@(1,3) (85.5%, 99.3%), ce_loss 0.643, lat_loss 21.793
09/29 01:02:43 AM | _theta_step_train: [127/180] Final Prec@1 85.5500% Time 232.15
09/29 01:02:53 AM | Valid: [127/180] Step 050/312 Loss 3.309 Prec@(1,3) (87.8%, 99.7%), ce_loss 0.643, lat_loss 21.792
09/29 01:03:00 AM | Valid: [127/180] Step 100/312 Loss 3.594 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.643, lat_loss 21.792
09/29 01:03:06 AM | Valid: [127/180] Step 150/312 Loss 3.662 Prec@(1,3) (86.6%, 99.1%), ce_loss 0.643, lat_loss 21.792
09/29 01:03:13 AM | Valid: [127/180] Step 200/312 Loss 3.635 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.643, lat_loss 21.792
09/29 01:03:20 AM | Valid: [127/180] Step 250/312 Loss 3.606 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.643, lat_loss 21.792
09/29 01:03:28 AM | Valid: [127/180] Step 300/312 Loss 3.613 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.643, lat_loss 21.792
09/29 01:03:30 AM | Valid: [127/180] Step 312/312 Loss 3.599 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.643, lat_loss 21.792
09/29 01:03:30 AM | val: [127/180] Final Prec@1 86.5500% Time 46.83
09/29 01:03:30 AM | Start to train weights for epoch 127
09/29 01:04:18 AM | Train: [128/180] Step 050/1249 Loss 2.070 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.643, lat_loss 21.792
09/29 01:05:02 AM | Train: [128/180] Step 100/1249 Loss 2.123 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.643, lat_loss 21.792
09/29 01:05:46 AM | Train: [128/180] Step 150/1249 Loss 2.106 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.643, lat_loss 21.792
09/29 01:06:31 AM | Train: [128/180] Step 200/1249 Loss 2.040 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.643, lat_loss 21.792
09/29 01:07:17 AM | Train: [128/180] Step 250/1249 Loss 2.040 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.643, lat_loss 21.792
09/29 01:08:04 AM | Train: [128/180] Step 300/1249 Loss 2.052 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.643, lat_loss 21.792
09/29 01:08:46 AM | Train: [128/180] Step 350/1249 Loss 2.069 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:09:32 AM | Train: [128/180] Step 400/1249 Loss 2.071 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:10:20 AM | Train: [128/180] Step 450/1249 Loss 2.075 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:11:08 AM | Train: [128/180] Step 500/1249 Loss 2.067 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:11:56 AM | Train: [128/180] Step 550/1249 Loss 2.075 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:12:45 AM | Train: [128/180] Step 600/1249 Loss 2.069 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:13:34 AM | Train: [128/180] Step 650/1249 Loss 2.080 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:14:16 AM | Train: [128/180] Step 700/1249 Loss 2.079 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:15:05 AM | Train: [128/180] Step 750/1249 Loss 2.083 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:15:54 AM | Train: [128/180] Step 800/1249 Loss 2.091 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:16:43 AM | Train: [128/180] Step 850/1249 Loss 2.085 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:17:32 AM | Train: [128/180] Step 900/1249 Loss 2.098 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.642, lat_loss 21.792
09/29 01:18:21 AM | Train: [128/180] Step 950/1249 Loss 2.104 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.641, lat_loss 21.792
09/29 01:19:11 AM | Train: [128/180] Step 1000/1249 Loss 2.099 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.641, lat_loss 21.792
09/29 01:19:53 AM | Train: [128/180] Step 1050/1249 Loss 2.097 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.641, lat_loss 21.792
09/29 01:20:42 AM | Train: [128/180] Step 1100/1249 Loss 2.086 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.641, lat_loss 21.792
09/29 01:21:31 AM | Train: [128/180] Step 1150/1249 Loss 2.085 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.641, lat_loss 21.792
09/29 01:22:20 AM | Train: [128/180] Step 1200/1249 Loss 2.107 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.641, lat_loss 21.792
09/29 01:23:08 AM | Train: [128/180] Step 1249/1249 Loss 2.115 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.641, lat_loss 21.792
09/29 01:23:08 AM | _w_step_train: [128/180] Final Prec@1 91.6550% Time 1178.18
09/29 01:23:08 AM | Start to train theta for epoch 127
09/29 01:23:54 AM | Train: [128/180] Step 050/312 Loss 3.723 Prec@(1,3) (86.5%, 99.6%), ce_loss 0.641, lat_loss 21.792
09/29 01:24:37 AM | Train: [128/180] Step 100/312 Loss 3.672 Prec@(1,3) (86.7%, 99.5%), ce_loss 0.641, lat_loss 21.792
09/29 01:25:16 AM | Train: [128/180] Step 150/312 Loss 3.670 Prec@(1,3) (86.4%, 99.5%), ce_loss 0.641, lat_loss 21.792
09/29 01:25:58 AM | Train: [128/180] Step 200/312 Loss 3.746 Prec@(1,3) (86.0%, 99.5%), ce_loss 0.641, lat_loss 21.792
09/29 01:26:41 AM | Train: [128/180] Step 250/312 Loss 3.878 Prec@(1,3) (85.8%, 99.4%), ce_loss 0.641, lat_loss 21.792
09/29 01:27:23 AM | Train: [128/180] Step 300/312 Loss 3.893 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.641, lat_loss 21.792
09/29 01:27:33 AM | Train: [128/180] Step 312/312 Loss 3.920 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.641, lat_loss 21.792
09/29 01:27:34 AM | _theta_step_train: [128/180] Final Prec@1 85.7900% Time 265.19
09/29 01:27:44 AM | Valid: [128/180] Step 050/312 Loss 3.669 Prec@(1,3) (86.8%, 99.1%), ce_loss 0.641, lat_loss 21.792
09/29 01:27:54 AM | Valid: [128/180] Step 100/312 Loss 3.844 Prec@(1,3) (86.2%, 99.0%), ce_loss 0.641, lat_loss 21.792
09/29 01:28:04 AM | Valid: [128/180] Step 150/312 Loss 3.877 Prec@(1,3) (86.0%, 98.9%), ce_loss 0.641, lat_loss 21.792
09/29 01:28:13 AM | Valid: [128/180] Step 200/312 Loss 3.844 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.641, lat_loss 21.792
09/29 01:28:23 AM | Valid: [128/180] Step 250/312 Loss 3.796 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.640, lat_loss 21.792
09/29 01:28:33 AM | Valid: [128/180] Step 300/312 Loss 3.708 Prec@(1,3) (86.2%, 99.2%), ce_loss 0.640, lat_loss 21.792
09/29 01:28:35 AM | Valid: [128/180] Step 312/312 Loss 3.713 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.640, lat_loss 21.792
09/29 01:28:35 AM | val: [128/180] Final Prec@1 86.1200% Time 61.65
09/29 01:28:35 AM | Start to train weights for epoch 128
09/29 01:29:28 AM | Train: [129/180] Step 050/1249 Loss 1.968 Prec@(1,3) (91.7%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:30:18 AM | Train: [129/180] Step 100/1249 Loss 1.929 Prec@(1,3) (92.3%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:31:04 AM | Train: [129/180] Step 150/1249 Loss 1.983 Prec@(1,3) (92.1%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:31:54 AM | Train: [129/180] Step 200/1249 Loss 1.946 Prec@(1,3) (92.3%, 100.0%), ce_loss 0.640, lat_loss 21.792
09/29 01:32:43 AM | Train: [129/180] Step 250/1249 Loss 1.970 Prec@(1,3) (92.1%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:33:31 AM | Train: [129/180] Step 300/1249 Loss 1.984 Prec@(1,3) (91.9%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:34:20 AM | Train: [129/180] Step 350/1249 Loss 2.018 Prec@(1,3) (91.7%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:35:08 AM | Train: [129/180] Step 400/1249 Loss 2.048 Prec@(1,3) (91.6%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:35:57 AM | Train: [129/180] Step 450/1249 Loss 2.050 Prec@(1,3) (91.6%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:36:39 AM | Train: [129/180] Step 500/1249 Loss 2.051 Prec@(1,3) (91.5%, 99.9%), ce_loss 0.640, lat_loss 21.792
09/29 01:37:28 AM | Train: [129/180] Step 550/1249 Loss 2.046 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.639, lat_loss 21.792
09/29 01:38:16 AM | Train: [129/180] Step 600/1249 Loss 2.062 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.639, lat_loss 21.792
09/29 01:39:05 AM | Train: [129/180] Step 650/1249 Loss 2.051 Prec@(1,3) (91.6%, 99.9%), ce_loss 0.639, lat_loss 21.792
09/29 01:39:54 AM | Train: [129/180] Step 700/1249 Loss 2.037 Prec@(1,3) (91.7%, 99.9%), ce_loss 0.639, lat_loss 21.792
09/29 01:40:43 AM | Train: [129/180] Step 750/1249 Loss 2.051 Prec@(1,3) (91.6%, 99.8%), ce_loss 0.639, lat_loss 21.792
09/29 01:41:32 AM | Train: [129/180] Step 800/1249 Loss 2.046 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.639, lat_loss 21.792
09/29 01:42:17 AM | Train: [129/180] Step 850/1249 Loss 2.027 Prec@(1,3) (91.8%, 99.9%), ce_loss 0.639, lat_loss 21.792
09/29 01:43:05 AM | Train: [129/180] Step 900/1249 Loss 2.038 Prec@(1,3) (91.8%, 99.9%), ce_loss 0.639, lat_loss 21.792
09/29 01:43:54 AM | Train: [129/180] Step 950/1249 Loss 2.040 Prec@(1,3) (91.8%, 99.9%), ce_loss 0.639, lat_loss 21.792
09/29 01:44:44 AM | Train: [129/180] Step 1000/1249 Loss 2.040 Prec@(1,3) (91.8%, 99.9%), ce_loss 0.639, lat_loss 21.792
09/29 01:45:34 AM | Train: [129/180] Step 1050/1249 Loss 2.046 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.639, lat_loss 21.792
09/29 01:46:24 AM | Train: [129/180] Step 1100/1249 Loss 2.049 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.639, lat_loss 21.792
09/29 01:47:13 AM | Train: [129/180] Step 1150/1249 Loss 2.049 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.638, lat_loss 21.792
09/29 01:47:58 AM | Train: [129/180] Step 1200/1249 Loss 2.064 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.638, lat_loss 21.792
09/29 01:48:45 AM | Train: [129/180] Step 1249/1249 Loss 2.067 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.638, lat_loss 21.792
09/29 01:48:45 AM | _w_step_train: [129/180] Final Prec@1 91.7150% Time 1209.76
09/29 01:48:45 AM | Start to train theta for epoch 128
09/29 01:49:30 AM | Train: [129/180] Step 050/312 Loss 3.364 Prec@(1,3) (87.5%, 99.5%), ce_loss 0.638, lat_loss 21.792
09/29 01:50:12 AM | Train: [129/180] Step 100/312 Loss 3.663 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.638, lat_loss 21.792
09/29 01:50:56 AM | Train: [129/180] Step 150/312 Loss 3.862 Prec@(1,3) (85.9%, 99.4%), ce_loss 0.638, lat_loss 21.792
09/29 01:51:40 AM | Train: [129/180] Step 200/312 Loss 3.808 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.638, lat_loss 21.791
09/29 01:52:22 AM | Train: [129/180] Step 250/312 Loss 3.841 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.638, lat_loss 21.791
09/29 01:53:04 AM | Train: [129/180] Step 300/312 Loss 3.892 Prec@(1,3) (85.9%, 99.4%), ce_loss 0.638, lat_loss 21.791
09/29 01:53:12 AM | Train: [129/180] Step 312/312 Loss 3.893 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.638, lat_loss 21.791
09/29 01:53:12 AM | _theta_step_train: [129/180] Final Prec@1 85.9800% Time 266.99
09/29 01:53:21 AM | Valid: [129/180] Step 050/312 Loss 3.337 Prec@(1,3) (87.8%, 99.5%), ce_loss 0.638, lat_loss 21.791
09/29 01:53:31 AM | Valid: [129/180] Step 100/312 Loss 3.834 Prec@(1,3) (86.5%, 99.0%), ce_loss 0.638, lat_loss 21.791
09/29 01:53:40 AM | Valid: [129/180] Step 150/312 Loss 3.819 Prec@(1,3) (86.7%, 98.9%), ce_loss 0.638, lat_loss 21.791
09/29 01:53:50 AM | Valid: [129/180] Step 200/312 Loss 3.818 Prec@(1,3) (86.4%, 99.0%), ce_loss 0.638, lat_loss 21.791
09/29 01:54:00 AM | Valid: [129/180] Step 250/312 Loss 3.795 Prec@(1,3) (86.3%, 99.1%), ce_loss 0.638, lat_loss 21.791
09/29 01:54:10 AM | Valid: [129/180] Step 300/312 Loss 3.706 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.638, lat_loss 21.791
09/29 01:54:12 AM | Valid: [129/180] Step 312/312 Loss 3.690 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.638, lat_loss 21.791
09/29 01:54:12 AM | val: [129/180] Final Prec@1 86.5500% Time 59.86
09/29 01:54:12 AM | Start to train weights for epoch 129
09/29 01:55:04 AM | Train: [130/180] Step 050/1249 Loss 2.036 Prec@(1,3) (91.7%, 99.9%), ce_loss 0.638, lat_loss 21.791
09/29 01:55:53 AM | Train: [130/180] Step 100/1249 Loss 1.938 Prec@(1,3) (92.1%, 99.9%), ce_loss 0.638, lat_loss 21.791
09/29 01:56:43 AM | Train: [130/180] Step 150/1249 Loss 2.008 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.637, lat_loss 21.791
09/29 01:57:32 AM | Train: [130/180] Step 200/1249 Loss 2.050 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.637, lat_loss 21.791
09/29 01:58:21 AM | Train: [130/180] Step 250/1249 Loss 1.989 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.637, lat_loss 21.791
09/29 01:59:03 AM | Train: [130/180] Step 300/1249 Loss 2.208 Prec@(1,3) (91.6%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 01:59:51 AM | Train: [130/180] Step 350/1249 Loss 2.183 Prec@(1,3) (91.7%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:00:40 AM | Train: [130/180] Step 400/1249 Loss 2.152 Prec@(1,3) (91.8%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:01:30 AM | Train: [130/180] Step 450/1249 Loss 2.184 Prec@(1,3) (91.6%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:02:20 AM | Train: [130/180] Step 500/1249 Loss 2.199 Prec@(1,3) (91.5%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:03:10 AM | Train: [130/180] Step 550/1249 Loss 2.198 Prec@(1,3) (91.5%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:03:59 AM | Train: [130/180] Step 600/1249 Loss 2.232 Prec@(1,3) (91.5%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:04:42 AM | Train: [130/180] Step 650/1249 Loss 2.215 Prec@(1,3) (91.5%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:05:32 AM | Train: [130/180] Step 700/1249 Loss 2.200 Prec@(1,3) (91.6%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:06:22 AM | Train: [130/180] Step 750/1249 Loss 2.209 Prec@(1,3) (91.5%, 99.7%), ce_loss 0.637, lat_loss 21.791
09/29 02:07:12 AM | Train: [130/180] Step 800/1249 Loss 2.181 Prec@(1,3) (91.6%, 99.7%), ce_loss 0.636, lat_loss 21.791
09/29 02:08:02 AM | Train: [130/180] Step 850/1249 Loss 2.165 Prec@(1,3) (91.7%, 99.7%), ce_loss 0.636, lat_loss 21.791
09/29 02:08:51 AM | Train: [130/180] Step 900/1249 Loss 2.149 Prec@(1,3) (91.7%, 99.7%), ce_loss 0.636, lat_loss 21.791
09/29 02:09:39 AM | Train: [130/180] Step 950/1249 Loss 2.143 Prec@(1,3) (91.7%, 99.7%), ce_loss 0.636, lat_loss 21.791
09/29 02:10:22 AM | Train: [130/180] Step 1000/1249 Loss 2.142 Prec@(1,3) (91.7%, 99.7%), ce_loss 0.636, lat_loss 21.791
09/29 02:11:11 AM | Train: [130/180] Step 1050/1249 Loss 2.144 Prec@(1,3) (91.7%, 99.7%), ce_loss 0.636, lat_loss 21.791
09/29 02:12:00 AM | Train: [130/180] Step 1100/1249 Loss 2.142 Prec@(1,3) (91.6%, 99.7%), ce_loss 0.636, lat_loss 21.791
09/29 02:12:48 AM | Train: [130/180] Step 1150/1249 Loss 2.143 Prec@(1,3) (91.6%, 99.7%), ce_loss 0.636, lat_loss 21.791
09/29 02:13:38 AM | Train: [130/180] Step 1200/1249 Loss 2.137 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.636, lat_loss 21.791
09/29 02:14:26 AM | Train: [130/180] Step 1249/1249 Loss 2.125 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.636, lat_loss 21.791
09/29 02:14:26 AM | _w_step_train: [130/180] Final Prec@1 91.6750% Time 1214.12
09/29 02:14:26 AM | Start to train theta for epoch 129
09/29 02:15:10 AM | Train: [130/180] Step 050/312 Loss 3.780 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.636, lat_loss 21.791
09/29 02:15:48 AM | Train: [130/180] Step 100/312 Loss 3.816 Prec@(1,3) (86.4%, 99.5%), ce_loss 0.636, lat_loss 21.791
09/29 02:16:32 AM | Train: [130/180] Step 150/312 Loss 3.880 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.636, lat_loss 21.791
09/29 02:17:15 AM | Train: [130/180] Step 200/312 Loss 3.951 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.635, lat_loss 21.791
09/29 02:17:58 AM | Train: [130/180] Step 250/312 Loss 3.931 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.635, lat_loss 21.791
09/29 02:18:41 AM | Train: [130/180] Step 300/312 Loss 3.901 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.635, lat_loss 21.791
09/29 02:18:51 AM | Train: [130/180] Step 312/312 Loss 3.895 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.635, lat_loss 21.791
09/29 02:18:51 AM | _theta_step_train: [130/180] Final Prec@1 86.2600% Time 264.58
09/29 02:19:01 AM | Valid: [130/180] Step 050/312 Loss 3.792 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.635, lat_loss 21.791
09/29 02:19:11 AM | Valid: [130/180] Step 100/312 Loss 3.909 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.635, lat_loss 21.791
09/29 02:19:21 AM | Valid: [130/180] Step 150/312 Loss 4.028 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.635, lat_loss 21.791
09/29 02:19:30 AM | Valid: [130/180] Step 200/312 Loss 3.886 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.635, lat_loss 21.791
09/29 02:19:40 AM | Valid: [130/180] Step 250/312 Loss 3.743 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.635, lat_loss 21.791
09/29 02:19:50 AM | Valid: [130/180] Step 300/312 Loss 3.731 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.635, lat_loss 21.791
09/29 02:19:52 AM | Valid: [130/180] Step 312/312 Loss 3.720 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.635, lat_loss 21.791
09/29 02:19:52 AM | val: [130/180] Final Prec@1 87.0300% Time 61.50
09/29 02:19:52 AM | Best top1 acc by now. Save model
09/29 02:19:52 AM | Start to train weights for epoch 130
09/29 02:20:44 AM | Train: [131/180] Step 050/1249 Loss 1.737 Prec@(1,3) (92.8%, 99.9%), ce_loss 0.635, lat_loss 21.791
09/29 02:21:26 AM | Train: [131/180] Step 100/1249 Loss 1.720 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.635, lat_loss 21.791
09/29 02:22:15 AM | Train: [131/180] Step 150/1249 Loss 1.723 Prec@(1,3) (92.9%, 100.0%), ce_loss 0.635, lat_loss 21.791
09/29 02:23:04 AM | Train: [131/180] Step 200/1249 Loss 1.781 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.635, lat_loss 21.791
09/29 02:23:53 AM | Train: [131/180] Step 250/1249 Loss 1.977 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.635, lat_loss 21.791
09/29 02:24:43 AM | Train: [131/180] Step 300/1249 Loss 1.999 Prec@(1,3) (92.0%, 99.9%), ce_loss 0.635, lat_loss 21.791
09/29 02:25:33 AM | Train: [131/180] Step 350/1249 Loss 2.080 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.635, lat_loss 21.791
09/29 02:26:21 AM | Train: [131/180] Step 400/1249 Loss 2.072 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:27:03 AM | Train: [131/180] Step 450/1249 Loss 2.109 Prec@(1,3) (91.7%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:27:52 AM | Train: [131/180] Step 500/1249 Loss 2.104 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:28:41 AM | Train: [131/180] Step 550/1249 Loss 2.097 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:29:30 AM | Train: [131/180] Step 600/1249 Loss 2.108 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:30:19 AM | Train: [131/180] Step 650/1249 Loss 2.108 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:31:08 AM | Train: [131/180] Step 700/1249 Loss 2.113 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:31:57 AM | Train: [131/180] Step 750/1249 Loss 2.106 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:32:39 AM | Train: [131/180] Step 800/1249 Loss 2.092 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:33:27 AM | Train: [131/180] Step 850/1249 Loss 2.101 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:34:16 AM | Train: [131/180] Step 900/1249 Loss 2.102 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:35:06 AM | Train: [131/180] Step 950/1249 Loss 2.097 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.634, lat_loss 21.791
09/29 02:35:55 AM | Train: [131/180] Step 1000/1249 Loss 2.102 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.633, lat_loss 21.791
09/29 02:36:44 AM | Train: [131/180] Step 1050/1249 Loss 2.090 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.633, lat_loss 21.791
09/29 02:37:33 AM | Train: [131/180] Step 1100/1249 Loss 2.086 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.633, lat_loss 21.791
09/29 02:38:18 AM | Train: [131/180] Step 1150/1249 Loss 2.091 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.633, lat_loss 21.791
09/29 02:39:07 AM | Train: [131/180] Step 1200/1249 Loss 2.085 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.633, lat_loss 21.791
09/29 02:39:55 AM | Train: [131/180] Step 1249/1249 Loss 2.077 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.633, lat_loss 21.791
09/29 02:39:55 AM | _w_step_train: [131/180] Final Prec@1 91.8500% Time 1202.87
09/29 02:39:55 AM | Start to train theta for epoch 130
09/29 02:40:39 AM | Train: [131/180] Step 050/312 Loss 3.764 Prec@(1,3) (86.2%, 99.6%), ce_loss 0.633, lat_loss 21.791
09/29 02:41:22 AM | Train: [131/180] Step 100/312 Loss 3.787 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.633, lat_loss 21.791
09/29 02:42:06 AM | Train: [131/180] Step 150/312 Loss 3.772 Prec@(1,3) (86.2%, 99.5%), ce_loss 0.633, lat_loss 21.791
09/29 02:42:50 AM | Train: [131/180] Step 200/312 Loss 3.808 Prec@(1,3) (86.2%, 99.5%), ce_loss 0.633, lat_loss 21.791
09/29 02:43:30 AM | Train: [131/180] Step 250/312 Loss 3.747 Prec@(1,3) (86.3%, 99.5%), ce_loss 0.633, lat_loss 21.791
09/29 02:44:12 AM | Train: [131/180] Step 300/312 Loss 3.760 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.633, lat_loss 21.791
09/29 02:44:22 AM | Train: [131/180] Step 312/312 Loss 3.880 Prec@(1,3) (86.0%, 99.4%), ce_loss 0.633, lat_loss 21.791
09/29 02:44:23 AM | _theta_step_train: [131/180] Final Prec@1 86.0400% Time 267.31
09/29 02:44:33 AM | Valid: [131/180] Step 050/312 Loss 3.905 Prec@(1,3) (87.0%, 99.6%), ce_loss 0.633, lat_loss 21.791
09/29 02:44:43 AM | Valid: [131/180] Step 100/312 Loss 4.048 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.633, lat_loss 21.791
09/29 02:44:53 AM | Valid: [131/180] Step 150/312 Loss 3.943 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.633, lat_loss 21.791
09/29 02:45:02 AM | Valid: [131/180] Step 200/312 Loss 4.028 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.633, lat_loss 21.791
09/29 02:45:12 AM | Valid: [131/180] Step 250/312 Loss 3.870 Prec@(1,3) (86.9%, 99.1%), ce_loss 0.633, lat_loss 21.791
09/29 02:45:22 AM | Valid: [131/180] Step 300/312 Loss 3.762 Prec@(1,3) (87.0%, 99.2%), ce_loss 0.633, lat_loss 21.791
09/29 02:45:24 AM | Valid: [131/180] Step 312/312 Loss 3.743 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.633, lat_loss 21.791
09/29 02:45:24 AM | val: [131/180] Final Prec@1 86.9200% Time 61.44
09/29 02:45:24 AM | Start to train weights for epoch 131
09/29 02:46:16 AM | Train: [132/180] Step 050/1249 Loss 1.975 Prec@(1,3) (92.0%, 100.0%), ce_loss 0.632, lat_loss 21.791
09/29 02:47:06 AM | Train: [132/180] Step 100/1249 Loss 1.991 Prec@(1,3) (92.1%, 99.9%), ce_loss 0.632, lat_loss 21.791
09/29 02:47:56 AM | Train: [132/180] Step 150/1249 Loss 2.074 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:48:45 AM | Train: [132/180] Step 200/1249 Loss 2.044 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:49:29 AM | Train: [132/180] Step 250/1249 Loss 2.035 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:50:18 AM | Train: [132/180] Step 300/1249 Loss 2.036 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:51:07 AM | Train: [132/180] Step 350/1249 Loss 2.013 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:51:56 AM | Train: [132/180] Step 400/1249 Loss 1.998 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:52:46 AM | Train: [132/180] Step 450/1249 Loss 1.983 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:53:36 AM | Train: [132/180] Step 500/1249 Loss 1.978 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:54:26 AM | Train: [132/180] Step 550/1249 Loss 1.963 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.632, lat_loss 21.791
09/29 02:55:14 AM | Train: [132/180] Step 600/1249 Loss 1.993 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.632, lat_loss 21.790
09/29 02:56:03 AM | Train: [132/180] Step 650/1249 Loss 1.986 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 02:56:53 AM | Train: [132/180] Step 700/1249 Loss 1.986 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 02:57:44 AM | Train: [132/180] Step 750/1249 Loss 2.002 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 02:58:33 AM | Train: [132/180] Step 800/1249 Loss 2.000 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 02:59:22 AM | Train: [132/180] Step 850/1249 Loss 2.006 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 03:00:08 AM | Train: [132/180] Step 900/1249 Loss 2.009 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 03:00:54 AM | Train: [132/180] Step 950/1249 Loss 2.005 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 03:01:43 AM | Train: [132/180] Step 1000/1249 Loss 2.001 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 03:02:31 AM | Train: [132/180] Step 1050/1249 Loss 2.001 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 03:03:20 AM | Train: [132/180] Step 1100/1249 Loss 2.005 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 03:04:09 AM | Train: [132/180] Step 1150/1249 Loss 2.015 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 03:04:58 AM | Train: [132/180] Step 1200/1249 Loss 2.007 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.631, lat_loss 21.790
09/29 03:05:42 AM | Train: [132/180] Step 1249/1249 Loss 2.017 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.630, lat_loss 21.790
09/29 03:05:42 AM | _w_step_train: [132/180] Final Prec@1 92.1450% Time 1218.41
09/29 03:05:42 AM | Start to train theta for epoch 131
09/29 03:06:25 AM | Train: [132/180] Step 050/312 Loss 3.525 Prec@(1,3) (87.6%, 99.0%), ce_loss 0.630, lat_loss 21.790
09/29 03:07:08 AM | Train: [132/180] Step 100/312 Loss 3.641 Prec@(1,3) (87.0%, 99.1%), ce_loss 0.630, lat_loss 21.790
09/29 03:07:51 AM | Train: [132/180] Step 150/312 Loss 3.616 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.630, lat_loss 21.790
09/29 03:08:34 AM | Train: [132/180] Step 200/312 Loss 3.595 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.630, lat_loss 21.790
09/29 03:09:17 AM | Train: [132/180] Step 250/312 Loss 3.648 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.630, lat_loss 21.790
09/29 03:09:59 AM | Train: [132/180] Step 300/312 Loss 3.716 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.630, lat_loss 21.790
09/29 03:10:09 AM | Train: [132/180] Step 312/312 Loss 3.764 Prec@(1,3) (86.2%, 99.2%), ce_loss 0.630, lat_loss 21.790
09/29 03:10:09 AM | _theta_step_train: [132/180] Final Prec@1 86.2100% Time 267.00
09/29 03:10:20 AM | Valid: [132/180] Step 050/312 Loss 3.304 Prec@(1,3) (87.8%, 99.6%), ce_loss 0.630, lat_loss 21.790
09/29 03:10:30 AM | Valid: [132/180] Step 100/312 Loss 3.697 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.630, lat_loss 21.790
09/29 03:10:39 AM | Valid: [132/180] Step 150/312 Loss 3.730 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.630, lat_loss 21.790
09/29 03:10:49 AM | Valid: [132/180] Step 200/312 Loss 3.675 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.630, lat_loss 21.790
09/29 03:10:59 AM | Valid: [132/180] Step 250/312 Loss 3.634 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.630, lat_loss 21.790
09/29 03:11:08 AM | Valid: [132/180] Step 300/312 Loss 3.635 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.630, lat_loss 21.790
09/29 03:11:11 AM | Valid: [132/180] Step 312/312 Loss 3.744 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.630, lat_loss 21.790
09/29 03:11:11 AM | val: [132/180] Final Prec@1 86.2500% Time 61.35
09/29 03:11:11 AM | Start to train weights for epoch 132
09/29 03:11:55 AM | Train: [133/180] Step 050/1249 Loss 1.666 Prec@(1,3) (93.5%, 100.0%), ce_loss 0.630, lat_loss 21.790
09/29 03:12:44 AM | Train: [133/180] Step 100/1249 Loss 1.838 Prec@(1,3) (93.2%, 100.0%), ce_loss 0.630, lat_loss 21.790
09/29 03:13:33 AM | Train: [133/180] Step 150/1249 Loss 1.870 Prec@(1,3) (92.9%, 100.0%), ce_loss 0.630, lat_loss 21.790
09/29 03:14:22 AM | Train: [133/180] Step 200/1249 Loss 2.064 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.630, lat_loss 21.790
09/29 03:15:10 AM | Train: [133/180] Step 250/1249 Loss 2.020 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.630, lat_loss 21.790
09/29 03:15:59 AM | Train: [133/180] Step 300/1249 Loss 2.112 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:16:48 AM | Train: [133/180] Step 350/1249 Loss 2.103 Prec@(1,3) (91.9%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:17:32 AM | Train: [133/180] Step 400/1249 Loss 2.076 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:18:21 AM | Train: [133/180] Step 450/1249 Loss 2.058 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:19:11 AM | Train: [133/180] Step 500/1249 Loss 2.055 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:20:00 AM | Train: [133/180] Step 550/1249 Loss 2.042 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:20:50 AM | Train: [133/180] Step 600/1249 Loss 2.071 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:21:39 AM | Train: [133/180] Step 650/1249 Loss 2.077 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:22:27 AM | Train: [133/180] Step 700/1249 Loss 2.073 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:23:12 AM | Train: [133/180] Step 750/1249 Loss 2.065 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:24:02 AM | Train: [133/180] Step 800/1249 Loss 2.059 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:24:53 AM | Train: [133/180] Step 850/1249 Loss 2.040 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.629, lat_loss 21.790
09/29 03:25:42 AM | Train: [133/180] Step 900/1249 Loss 2.039 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:26:32 AM | Train: [133/180] Step 950/1249 Loss 2.047 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:27:21 AM | Train: [133/180] Step 1000/1249 Loss 2.034 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:28:07 AM | Train: [133/180] Step 1050/1249 Loss 2.035 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:28:53 AM | Train: [133/180] Step 1100/1249 Loss 2.041 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:29:42 AM | Train: [133/180] Step 1150/1249 Loss 2.046 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:30:31 AM | Train: [133/180] Step 1200/1249 Loss 2.044 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:31:20 AM | Train: [133/180] Step 1249/1249 Loss 2.040 Prec@(1,3) (92.0%, 99.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:31:20 AM | _w_step_train: [133/180] Final Prec@1 92.0500% Time 1209.60
09/29 03:31:20 AM | Start to train theta for epoch 132
09/29 03:32:05 AM | Train: [133/180] Step 050/312 Loss 4.047 Prec@(1,3) (84.2%, 99.7%), ce_loss 0.628, lat_loss 21.790
09/29 03:32:48 AM | Train: [133/180] Step 100/312 Loss 4.071 Prec@(1,3) (84.8%, 99.5%), ce_loss 0.628, lat_loss 21.790
09/29 03:33:31 AM | Train: [133/180] Step 150/312 Loss 4.149 Prec@(1,3) (84.6%, 99.4%), ce_loss 0.628, lat_loss 21.790
09/29 03:34:10 AM | Train: [133/180] Step 200/312 Loss 4.176 Prec@(1,3) (84.7%, 99.3%), ce_loss 0.628, lat_loss 21.790
09/29 03:34:53 AM | Train: [133/180] Step 250/312 Loss 4.138 Prec@(1,3) (85.0%, 99.3%), ce_loss 0.628, lat_loss 21.790
09/29 03:35:35 AM | Train: [133/180] Step 300/312 Loss 4.193 Prec@(1,3) (85.0%, 99.2%), ce_loss 0.628, lat_loss 21.790
09/29 03:35:45 AM | Train: [133/180] Step 312/312 Loss 4.173 Prec@(1,3) (85.1%, 99.2%), ce_loss 0.628, lat_loss 21.790
09/29 03:35:45 AM | _theta_step_train: [133/180] Final Prec@1 85.0700% Time 264.94
09/29 03:35:56 AM | Valid: [133/180] Step 050/312 Loss 4.335 Prec@(1,3) (85.4%, 99.0%), ce_loss 0.628, lat_loss 21.790
09/29 03:36:05 AM | Valid: [133/180] Step 100/312 Loss 4.158 Prec@(1,3) (85.8%, 98.9%), ce_loss 0.628, lat_loss 21.790
09/29 03:36:15 AM | Valid: [133/180] Step 150/312 Loss 4.055 Prec@(1,3) (86.0%, 98.9%), ce_loss 0.628, lat_loss 21.790
09/29 03:36:25 AM | Valid: [133/180] Step 200/312 Loss 4.165 Prec@(1,3) (85.3%, 98.8%), ce_loss 0.628, lat_loss 21.790
09/29 03:36:34 AM | Valid: [133/180] Step 250/312 Loss 4.096 Prec@(1,3) (85.5%, 98.9%), ce_loss 0.628, lat_loss 21.790
09/29 03:36:44 AM | Valid: [133/180] Step 300/312 Loss 4.034 Prec@(1,3) (85.6%, 99.0%), ce_loss 0.628, lat_loss 21.790
09/29 03:36:47 AM | Valid: [133/180] Step 312/312 Loss 4.011 Prec@(1,3) (85.7%, 99.0%), ce_loss 0.628, lat_loss 21.790
09/29 03:36:47 AM | val: [133/180] Final Prec@1 85.6900% Time 61.19
09/29 03:36:47 AM | Start to train weights for epoch 133
09/29 03:37:39 AM | Train: [134/180] Step 050/1249 Loss 1.703 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.627, lat_loss 21.790
09/29 03:38:28 AM | Train: [134/180] Step 100/1249 Loss 1.676 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.627, lat_loss 21.790
09/29 03:39:17 AM | Train: [134/180] Step 150/1249 Loss 1.776 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.627, lat_loss 21.790
09/29 03:40:00 AM | Train: [134/180] Step 200/1249 Loss 1.800 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.627, lat_loss 21.790
09/29 03:40:51 AM | Train: [134/180] Step 250/1249 Loss 1.842 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.627, lat_loss 21.790
09/29 03:41:41 AM | Train: [134/180] Step 300/1249 Loss 1.852 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.627, lat_loss 21.790
09/29 03:42:31 AM | Train: [134/180] Step 350/1249 Loss 1.853 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.627, lat_loss 21.790
09/29 03:43:20 AM | Train: [134/180] Step 400/1249 Loss 1.823 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.627, lat_loss 21.790
09/29 03:44:09 AM | Train: [134/180] Step 450/1249 Loss 1.870 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.627, lat_loss 21.790
09/29 03:44:57 AM | Train: [134/180] Step 500/1249 Loss 1.874 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.627, lat_loss 21.790
09/29 03:45:42 AM | Train: [134/180] Step 550/1249 Loss 1.878 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.627, lat_loss 21.790
09/29 03:46:32 AM | Train: [134/180] Step 600/1249 Loss 1.872 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:47:22 AM | Train: [134/180] Step 650/1249 Loss 1.865 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:48:11 AM | Train: [134/180] Step 700/1249 Loss 1.899 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:49:00 AM | Train: [134/180] Step 750/1249 Loss 1.903 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:49:50 AM | Train: [134/180] Step 800/1249 Loss 1.913 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:50:37 AM | Train: [134/180] Step 850/1249 Loss 1.920 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:51:23 AM | Train: [134/180] Step 900/1249 Loss 1.912 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:52:12 AM | Train: [134/180] Step 950/1249 Loss 1.917 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:53:00 AM | Train: [134/180] Step 1000/1249 Loss 1.925 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:53:49 AM | Train: [134/180] Step 1050/1249 Loss 1.922 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:54:38 AM | Train: [134/180] Step 1100/1249 Loss 1.928 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:55:27 AM | Train: [134/180] Step 1150/1249 Loss 1.934 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:56:11 AM | Train: [134/180] Step 1200/1249 Loss 1.935 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.626, lat_loss 21.790
09/29 03:56:56 AM | Train: [134/180] Step 1249/1249 Loss 1.955 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.625, lat_loss 21.790
09/29 03:56:56 AM | _w_step_train: [134/180] Final Prec@1 92.3425% Time 1209.62
09/29 03:56:56 AM | Start to train theta for epoch 133
09/29 03:57:41 AM | Train: [134/180] Step 050/312 Loss 4.025 Prec@(1,3) (85.4%, 99.2%), ce_loss 0.625, lat_loss 21.789
09/29 03:58:23 AM | Train: [134/180] Step 100/312 Loss 3.972 Prec@(1,3) (85.4%, 99.4%), ce_loss 0.625, lat_loss 21.789
09/29 03:59:06 AM | Train: [134/180] Step 150/312 Loss 3.825 Prec@(1,3) (85.8%, 99.4%), ce_loss 0.625, lat_loss 21.789
09/29 03:59:50 AM | Train: [134/180] Step 200/312 Loss 3.789 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.625, lat_loss 21.789
09/29 04:00:33 AM | Train: [134/180] Step 250/312 Loss 3.802 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.625, lat_loss 21.789
09/29 04:01:17 AM | Train: [134/180] Step 300/312 Loss 3.837 Prec@(1,3) (86.0%, 99.4%), ce_loss 0.625, lat_loss 21.789
09/29 04:01:27 AM | Train: [134/180] Step 312/312 Loss 3.835 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.625, lat_loss 21.789
09/29 04:01:27 AM | _theta_step_train: [134/180] Final Prec@1 86.0600% Time 271.07
09/29 04:01:38 AM | Valid: [134/180] Step 050/312 Loss 3.425 Prec@(1,3) (87.0%, 99.2%), ce_loss 0.625, lat_loss 21.789
09/29 04:01:46 AM | Valid: [134/180] Step 100/312 Loss 3.690 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.625, lat_loss 21.789
09/29 04:01:54 AM | Valid: [134/180] Step 150/312 Loss 3.829 Prec@(1,3) (86.3%, 98.9%), ce_loss 0.625, lat_loss 21.789
09/29 04:02:02 AM | Valid: [134/180] Step 200/312 Loss 3.733 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.625, lat_loss 21.789
09/29 04:02:11 AM | Valid: [134/180] Step 250/312 Loss 3.687 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.625, lat_loss 21.789
09/29 04:02:20 AM | Valid: [134/180] Step 300/312 Loss 3.594 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.625, lat_loss 21.789
09/29 04:02:23 AM | Valid: [134/180] Step 312/312 Loss 3.612 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.625, lat_loss 21.789
09/29 04:02:23 AM | val: [134/180] Final Prec@1 86.6600% Time 55.39
09/29 04:02:23 AM | Start to train weights for epoch 134
09/29 04:03:15 AM | Train: [135/180] Step 050/1249 Loss 1.948 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.625, lat_loss 21.789
09/29 04:04:04 AM | Train: [135/180] Step 100/1249 Loss 1.891 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.625, lat_loss 21.789
09/29 04:04:52 AM | Train: [135/180] Step 150/1249 Loss 1.923 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.625, lat_loss 21.789
09/29 04:05:42 AM | Train: [135/180] Step 200/1249 Loss 1.985 Prec@(1,3) (92.3%, 99.9%), ce_loss 0.625, lat_loss 21.789
09/29 04:06:31 AM | Train: [135/180] Step 250/1249 Loss 1.932 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.625, lat_loss 21.789
09/29 04:07:20 AM | Train: [135/180] Step 300/1249 Loss 1.921 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:08:03 AM | Train: [135/180] Step 350/1249 Loss 1.912 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:08:53 AM | Train: [135/180] Step 400/1249 Loss 1.880 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:09:42 AM | Train: [135/180] Step 450/1249 Loss 1.861 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:10:31 AM | Train: [135/180] Step 500/1249 Loss 1.836 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:11:21 AM | Train: [135/180] Step 550/1249 Loss 1.859 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:12:10 AM | Train: [135/180] Step 600/1249 Loss 1.865 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:12:58 AM | Train: [135/180] Step 650/1249 Loss 1.866 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:13:44 AM | Train: [135/180] Step 700/1249 Loss 1.853 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:14:32 AM | Train: [135/180] Step 750/1249 Loss 1.869 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:15:22 AM | Train: [135/180] Step 800/1249 Loss 1.862 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:16:12 AM | Train: [135/180] Step 850/1249 Loss 1.871 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.624, lat_loss 21.789
09/29 04:17:02 AM | Train: [135/180] Step 900/1249 Loss 1.875 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:17:51 AM | Train: [135/180] Step 950/1249 Loss 1.873 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:18:37 AM | Train: [135/180] Step 1000/1249 Loss 1.879 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:19:23 AM | Train: [135/180] Step 1050/1249 Loss 1.885 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:20:11 AM | Train: [135/180] Step 1100/1249 Loss 1.916 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:21:00 AM | Train: [135/180] Step 1150/1249 Loss 1.919 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:21:50 AM | Train: [135/180] Step 1200/1249 Loss 1.931 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:22:38 AM | Train: [135/180] Step 1249/1249 Loss 1.950 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:22:38 AM | _w_step_train: [135/180] Final Prec@1 92.2975% Time 1215.10
09/29 04:22:38 AM | Start to train theta for epoch 134
09/29 04:23:22 AM | Train: [135/180] Step 050/312 Loss 4.029 Prec@(1,3) (85.0%, 99.1%), ce_loss 0.623, lat_loss 21.789
09/29 04:24:05 AM | Train: [135/180] Step 100/312 Loss 3.948 Prec@(1,3) (85.6%, 99.1%), ce_loss 0.623, lat_loss 21.789
09/29 04:24:43 AM | Train: [135/180] Step 150/312 Loss 3.928 Prec@(1,3) (85.7%, 99.2%), ce_loss 0.623, lat_loss 21.789
09/29 04:25:24 AM | Train: [135/180] Step 200/312 Loss 3.838 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.623, lat_loss 21.789
09/29 04:26:07 AM | Train: [135/180] Step 250/312 Loss 3.780 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.623, lat_loss 21.789
09/29 04:26:48 AM | Train: [135/180] Step 300/312 Loss 3.749 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.623, lat_loss 21.789
09/29 04:26:58 AM | Train: [135/180] Step 312/312 Loss 3.743 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.623, lat_loss 21.789
09/29 04:26:58 AM | _theta_step_train: [135/180] Final Prec@1 86.1100% Time 260.55
09/29 04:27:09 AM | Valid: [135/180] Step 050/312 Loss 3.207 Prec@(1,3) (87.8%, 99.8%), ce_loss 0.623, lat_loss 21.789
09/29 04:27:18 AM | Valid: [135/180] Step 100/312 Loss 3.473 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.623, lat_loss 21.789
09/29 04:27:28 AM | Valid: [135/180] Step 150/312 Loss 3.529 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.623, lat_loss 21.789
09/29 04:27:37 AM | Valid: [135/180] Step 200/312 Loss 3.526 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.622, lat_loss 21.789
09/29 04:27:46 AM | Valid: [135/180] Step 250/312 Loss 3.681 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.622, lat_loss 21.789
09/29 04:27:56 AM | Valid: [135/180] Step 300/312 Loss 3.670 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.622, lat_loss 21.789
09/29 04:27:58 AM | Valid: [135/180] Step 312/312 Loss 3.652 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.622, lat_loss 21.789
09/29 04:27:58 AM | val: [135/180] Final Prec@1 86.3900% Time 59.49
09/29 04:27:58 AM | Start to train weights for epoch 135
09/29 04:28:47 AM | Train: [136/180] Step 050/1249 Loss 1.923 Prec@(1,3) (91.8%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:29:32 AM | Train: [136/180] Step 100/1249 Loss 1.844 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:30:15 AM | Train: [136/180] Step 150/1249 Loss 1.833 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:31:04 AM | Train: [136/180] Step 200/1249 Loss 1.851 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:31:52 AM | Train: [136/180] Step 250/1249 Loss 1.828 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:32:40 AM | Train: [136/180] Step 300/1249 Loss 1.835 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:33:28 AM | Train: [136/180] Step 350/1249 Loss 1.856 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:34:15 AM | Train: [136/180] Step 400/1249 Loss 1.915 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:35:03 AM | Train: [136/180] Step 450/1249 Loss 1.885 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:35:47 AM | Train: [136/180] Step 500/1249 Loss 1.883 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:36:36 AM | Train: [136/180] Step 550/1249 Loss 1.873 Prec@(1,3) (92.4%, 99.8%), ce_loss 0.622, lat_loss 21.789
09/29 04:37:25 AM | Train: [136/180] Step 600/1249 Loss 1.923 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:38:13 AM | Train: [136/180] Step 650/1249 Loss 1.936 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:39:02 AM | Train: [136/180] Step 700/1249 Loss 1.946 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:39:52 AM | Train: [136/180] Step 750/1249 Loss 1.957 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:40:39 AM | Train: [136/180] Step 800/1249 Loss 1.939 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:41:25 AM | Train: [136/180] Step 850/1249 Loss 1.940 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:42:16 AM | Train: [136/180] Step 900/1249 Loss 1.950 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:43:06 AM | Train: [136/180] Step 950/1249 Loss 1.943 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:43:56 AM | Train: [136/180] Step 1000/1249 Loss 1.943 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:44:44 AM | Train: [136/180] Step 1050/1249 Loss 1.944 Prec@(1,3) (92.3%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:45:34 AM | Train: [136/180] Step 1100/1249 Loss 1.948 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:46:19 AM | Train: [136/180] Step 1150/1249 Loss 1.955 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.621, lat_loss 21.789
09/29 04:47:05 AM | Train: [136/180] Step 1200/1249 Loss 1.968 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:47:53 AM | Train: [136/180] Step 1249/1249 Loss 1.981 Prec@(1,3) (92.2%, 99.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:47:53 AM | _w_step_train: [136/180] Final Prec@1 92.1600% Time 1195.14
09/29 04:47:53 AM | Start to train theta for epoch 135
09/29 04:48:38 AM | Train: [136/180] Step 050/312 Loss 3.614 Prec@(1,3) (87.6%, 99.4%), ce_loss 0.620, lat_loss 21.789
09/29 04:49:20 AM | Train: [136/180] Step 100/312 Loss 3.598 Prec@(1,3) (87.7%, 99.4%), ce_loss 0.620, lat_loss 21.789
09/29 04:50:03 AM | Train: [136/180] Step 150/312 Loss 3.579 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.620, lat_loss 21.789
09/29 04:50:46 AM | Train: [136/180] Step 200/312 Loss 3.680 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.620, lat_loss 21.789
09/29 04:51:29 AM | Train: [136/180] Step 250/312 Loss 3.677 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.620, lat_loss 21.789
09/29 04:52:10 AM | Train: [136/180] Step 300/312 Loss 3.653 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.620, lat_loss 21.789
09/29 04:52:19 AM | Train: [136/180] Step 312/312 Loss 3.652 Prec@(1,3) (87.0%, 99.5%), ce_loss 0.620, lat_loss 21.789
09/29 04:52:19 AM | _theta_step_train: [136/180] Final Prec@1 86.9600% Time 265.91
09/29 04:52:29 AM | Valid: [136/180] Step 050/312 Loss 4.649 Prec@(1,3) (85.6%, 98.3%), ce_loss 0.620, lat_loss 21.789
09/29 04:52:39 AM | Valid: [136/180] Step 100/312 Loss 4.261 Prec@(1,3) (85.4%, 98.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:52:49 AM | Valid: [136/180] Step 150/312 Loss 4.021 Prec@(1,3) (85.9%, 98.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:52:58 AM | Valid: [136/180] Step 200/312 Loss 3.960 Prec@(1,3) (86.0%, 98.9%), ce_loss 0.620, lat_loss 21.789
09/29 04:53:08 AM | Valid: [136/180] Step 250/312 Loss 3.832 Prec@(1,3) (86.2%, 99.0%), ce_loss 0.620, lat_loss 21.789
09/29 04:53:18 AM | Valid: [136/180] Step 300/312 Loss 3.848 Prec@(1,3) (86.1%, 98.9%), ce_loss 0.620, lat_loss 21.789
09/29 04:53:20 AM | Valid: [136/180] Step 312/312 Loss 3.828 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.620, lat_loss 21.789
09/29 04:53:20 AM | val: [136/180] Final Prec@1 86.1200% Time 61.05
09/29 04:53:20 AM | Start to train weights for epoch 136
09/29 04:54:11 AM | Train: [137/180] Step 050/1249 Loss 1.676 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:55:00 AM | Train: [137/180] Step 100/1249 Loss 1.660 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:55:48 AM | Train: [137/180] Step 150/1249 Loss 1.708 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:56:37 AM | Train: [137/180] Step 200/1249 Loss 1.717 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:57:26 AM | Train: [137/180] Step 250/1249 Loss 1.725 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.620, lat_loss 21.789
09/29 04:58:09 AM | Train: [137/180] Step 300/1249 Loss 1.747 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 04:58:58 AM | Train: [137/180] Step 350/1249 Loss 1.759 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 04:59:46 AM | Train: [137/180] Step 400/1249 Loss 1.768 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:00:35 AM | Train: [137/180] Step 450/1249 Loss 1.770 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:01:24 AM | Train: [137/180] Step 500/1249 Loss 1.794 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:02:13 AM | Train: [137/180] Step 550/1249 Loss 1.799 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:03:01 AM | Train: [137/180] Step 600/1249 Loss 1.785 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:03:45 AM | Train: [137/180] Step 650/1249 Loss 1.797 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:04:34 AM | Train: [137/180] Step 700/1249 Loss 1.810 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:05:24 AM | Train: [137/180] Step 750/1249 Loss 1.810 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:06:14 AM | Train: [137/180] Step 800/1249 Loss 1.806 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:07:04 AM | Train: [137/180] Step 850/1249 Loss 1.816 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.619, lat_loss 21.789
09/29 05:07:55 AM | Train: [137/180] Step 900/1249 Loss 1.819 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.618, lat_loss 21.789
09/29 05:08:41 AM | Train: [137/180] Step 950/1249 Loss 1.870 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.618, lat_loss 21.789
09/29 05:09:27 AM | Train: [137/180] Step 1000/1249 Loss 1.899 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.618, lat_loss 21.789
09/29 05:10:15 AM | Train: [137/180] Step 1050/1249 Loss 1.888 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.618, lat_loss 21.789
09/29 05:11:04 AM | Train: [137/180] Step 1100/1249 Loss 1.905 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.618, lat_loss 21.789
09/29 05:11:53 AM | Train: [137/180] Step 1150/1249 Loss 1.900 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.618, lat_loss 21.789
09/29 05:12:42 AM | Train: [137/180] Step 1200/1249 Loss 1.891 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.618, lat_loss 21.789
09/29 05:13:31 AM | Train: [137/180] Step 1249/1249 Loss 1.886 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.618, lat_loss 21.789
09/29 05:13:31 AM | _w_step_train: [137/180] Final Prec@1 92.5825% Time 1211.39
09/29 05:13:31 AM | Start to train theta for epoch 136
09/29 05:14:13 AM | Train: [137/180] Step 050/312 Loss 3.921 Prec@(1,3) (85.7%, 99.6%), ce_loss 0.618, lat_loss 21.789
09/29 05:14:52 AM | Train: [137/180] Step 100/312 Loss 3.878 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.618, lat_loss 21.789
09/29 05:15:34 AM | Train: [137/180] Step 150/312 Loss 3.696 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.618, lat_loss 21.788
09/29 05:16:17 AM | Train: [137/180] Step 200/312 Loss 3.752 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.618, lat_loss 21.788
09/29 05:17:00 AM | Train: [137/180] Step 250/312 Loss 3.879 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.618, lat_loss 21.788
09/29 05:17:42 AM | Train: [137/180] Step 300/312 Loss 3.915 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.618, lat_loss 21.788
09/29 05:17:52 AM | Train: [137/180] Step 312/312 Loss 3.927 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.618, lat_loss 21.788
09/29 05:17:52 AM | _theta_step_train: [137/180] Final Prec@1 85.9900% Time 260.91
09/29 05:18:03 AM | Valid: [137/180] Step 050/312 Loss 4.136 Prec@(1,3) (85.5%, 99.3%), ce_loss 0.618, lat_loss 21.788
09/29 05:18:13 AM | Valid: [137/180] Step 100/312 Loss 4.128 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.618, lat_loss 21.788
09/29 05:18:22 AM | Valid: [137/180] Step 150/312 Loss 3.938 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.618, lat_loss 21.788
09/29 05:18:32 AM | Valid: [137/180] Step 200/312 Loss 3.871 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.618, lat_loss 21.788
09/29 05:18:42 AM | Valid: [137/180] Step 250/312 Loss 3.770 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.618, lat_loss 21.788
09/29 05:18:51 AM | Valid: [137/180] Step 300/312 Loss 3.772 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.618, lat_loss 21.788
09/29 05:18:54 AM | Valid: [137/180] Step 312/312 Loss 3.790 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.617, lat_loss 21.788
09/29 05:18:54 AM | val: [137/180] Final Prec@1 86.3700% Time 61.28
09/29 05:18:54 AM | Start to train weights for epoch 137
09/29 05:19:46 AM | Train: [138/180] Step 050/1249 Loss 1.567 Prec@(1,3) (94.1%, 100.0%), ce_loss 0.617, lat_loss 21.788
09/29 05:20:30 AM | Train: [138/180] Step 100/1249 Loss 1.687 Prec@(1,3) (93.2%, 100.0%), ce_loss 0.617, lat_loss 21.788
09/29 05:21:19 AM | Train: [138/180] Step 150/1249 Loss 1.784 Prec@(1,3) (93.0%, 100.0%), ce_loss 0.617, lat_loss 21.788
09/29 05:22:09 AM | Train: [138/180] Step 200/1249 Loss 1.790 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:23:00 AM | Train: [138/180] Step 250/1249 Loss 1.786 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:23:49 AM | Train: [138/180] Step 300/1249 Loss 1.727 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:24:38 AM | Train: [138/180] Step 350/1249 Loss 1.752 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:25:29 AM | Train: [138/180] Step 400/1249 Loss 1.794 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:26:13 AM | Train: [138/180] Step 450/1249 Loss 1.807 Prec@(1,3) (92.9%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:27:02 AM | Train: [138/180] Step 500/1249 Loss 1.814 Prec@(1,3) (92.8%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:27:51 AM | Train: [138/180] Step 550/1249 Loss 1.841 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:28:40 AM | Train: [138/180] Step 600/1249 Loss 1.870 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.617, lat_loss 21.788
09/29 05:29:29 AM | Train: [138/180] Step 650/1249 Loss 1.866 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:30:18 AM | Train: [138/180] Step 700/1249 Loss 1.859 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:31:05 AM | Train: [138/180] Step 750/1249 Loss 1.860 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:31:51 AM | Train: [138/180] Step 800/1249 Loss 1.847 Prec@(1,3) (92.8%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:32:40 AM | Train: [138/180] Step 850/1249 Loss 1.841 Prec@(1,3) (92.8%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:33:29 AM | Train: [138/180] Step 900/1249 Loss 1.843 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:34:18 AM | Train: [138/180] Step 950/1249 Loss 1.840 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:35:06 AM | Train: [138/180] Step 1000/1249 Loss 1.851 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:35:55 AM | Train: [138/180] Step 1050/1249 Loss 1.858 Prec@(1,3) (92.6%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:36:41 AM | Train: [138/180] Step 1100/1249 Loss 1.852 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:37:27 AM | Train: [138/180] Step 1150/1249 Loss 1.845 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:38:16 AM | Train: [138/180] Step 1200/1249 Loss 1.850 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.616, lat_loss 21.788
09/29 05:39:04 AM | Train: [138/180] Step 1249/1249 Loss 1.854 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.615, lat_loss 21.788
09/29 05:39:04 AM | _w_step_train: [138/180] Final Prec@1 92.6575% Time 1210.10
09/29 05:39:04 AM | Start to train theta for epoch 137
09/29 05:39:48 AM | Train: [138/180] Step 050/312 Loss 4.004 Prec@(1,3) (86.0%, 99.5%), ce_loss 0.615, lat_loss 21.788
09/29 05:40:30 AM | Train: [138/180] Step 100/312 Loss 3.910 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.615, lat_loss 21.788
09/29 05:41:14 AM | Train: [138/180] Step 150/312 Loss 3.755 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.615, lat_loss 21.788
09/29 05:41:56 AM | Train: [138/180] Step 200/312 Loss 3.828 Prec@(1,3) (86.1%, 99.4%), ce_loss 0.615, lat_loss 21.788
09/29 05:42:34 AM | Train: [138/180] Step 250/312 Loss 3.811 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.615, lat_loss 21.788
09/29 05:43:17 AM | Train: [138/180] Step 300/312 Loss 3.811 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.615, lat_loss 21.788
09/29 05:43:27 AM | Train: [138/180] Step 312/312 Loss 3.787 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.615, lat_loss 21.788
09/29 05:43:28 AM | _theta_step_train: [138/180] Final Prec@1 86.4400% Time 263.81
09/29 05:43:38 AM | Valid: [138/180] Step 050/312 Loss 3.585 Prec@(1,3) (86.9%, 99.6%), ce_loss 0.615, lat_loss 21.788
09/29 05:43:48 AM | Valid: [138/180] Step 100/312 Loss 3.778 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.615, lat_loss 21.788
09/29 05:43:57 AM | Valid: [138/180] Step 150/312 Loss 3.790 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.615, lat_loss 21.788
09/29 05:44:07 AM | Valid: [138/180] Step 200/312 Loss 3.894 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.615, lat_loss 21.788
09/29 05:44:17 AM | Valid: [138/180] Step 250/312 Loss 3.784 Prec@(1,3) (87.0%, 99.2%), ce_loss 0.615, lat_loss 21.788
09/29 05:44:26 AM | Valid: [138/180] Step 300/312 Loss 3.719 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.615, lat_loss 21.788
09/29 05:44:29 AM | Valid: [138/180] Step 312/312 Loss 3.713 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.615, lat_loss 21.788
09/29 05:44:29 AM | val: [138/180] Final Prec@1 87.0000% Time 61.19
09/29 05:44:29 AM | Start to train weights for epoch 138
09/29 05:45:20 AM | Train: [139/180] Step 050/1249 Loss 1.866 Prec@(1,3) (92.5%, 99.9%), ce_loss 0.615, lat_loss 21.788
09/29 05:46:09 AM | Train: [139/180] Step 100/1249 Loss 1.823 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.615, lat_loss 21.788
09/29 05:46:58 AM | Train: [139/180] Step 150/1249 Loss 1.796 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.615, lat_loss 21.788
09/29 05:47:46 AM | Train: [139/180] Step 200/1249 Loss 1.794 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.615, lat_loss 21.788
09/29 05:48:31 AM | Train: [139/180] Step 250/1249 Loss 1.754 Prec@(1,3) (92.9%, 99.9%), ce_loss 0.615, lat_loss 21.788
09/29 05:49:20 AM | Train: [139/180] Step 300/1249 Loss 1.720 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.615, lat_loss 21.788
09/29 05:50:10 AM | Train: [139/180] Step 350/1249 Loss 1.765 Prec@(1,3) (92.9%, 99.9%), ce_loss 0.614, lat_loss 21.788
09/29 05:51:00 AM | Train: [139/180] Step 400/1249 Loss 1.809 Prec@(1,3) (92.8%, 99.9%), ce_loss 0.614, lat_loss 21.788
09/29 05:51:49 AM | Train: [139/180] Step 450/1249 Loss 1.791 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 05:52:37 AM | Train: [139/180] Step 500/1249 Loss 1.811 Prec@(1,3) (92.8%, 99.9%), ce_loss 0.614, lat_loss 21.788
09/29 05:53:24 AM | Train: [139/180] Step 550/1249 Loss 1.817 Prec@(1,3) (92.8%, 99.9%), ce_loss 0.614, lat_loss 21.788
09/29 05:54:08 AM | Train: [139/180] Step 600/1249 Loss 1.834 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 05:54:57 AM | Train: [139/180] Step 650/1249 Loss 1.836 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 05:55:46 AM | Train: [139/180] Step 700/1249 Loss 1.868 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 05:56:35 AM | Train: [139/180] Step 750/1249 Loss 1.870 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 05:57:24 AM | Train: [139/180] Step 800/1249 Loss 1.864 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 05:58:13 AM | Train: [139/180] Step 850/1249 Loss 1.855 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 05:59:00 AM | Train: [139/180] Step 900/1249 Loss 1.855 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 05:59:46 AM | Train: [139/180] Step 950/1249 Loss 1.869 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.614, lat_loss 21.788
09/29 06:00:35 AM | Train: [139/180] Step 1000/1249 Loss 1.869 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:01:24 AM | Train: [139/180] Step 1050/1249 Loss 1.866 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:02:13 AM | Train: [139/180] Step 1100/1249 Loss 1.860 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:03:01 AM | Train: [139/180] Step 1150/1249 Loss 1.847 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:03:51 AM | Train: [139/180] Step 1200/1249 Loss 1.849 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:04:35 AM | Train: [139/180] Step 1249/1249 Loss 1.852 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:04:36 AM | _w_step_train: [139/180] Final Prec@1 92.6900% Time 1206.77
09/29 06:04:36 AM | Start to train theta for epoch 138
09/29 06:05:18 AM | Train: [139/180] Step 050/312 Loss 4.109 Prec@(1,3) (86.0%, 99.1%), ce_loss 0.613, lat_loss 21.788
09/29 06:06:00 AM | Train: [139/180] Step 100/312 Loss 4.087 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.613, lat_loss 21.788
09/29 06:06:44 AM | Train: [139/180] Step 150/312 Loss 4.067 Prec@(1,3) (86.2%, 99.2%), ce_loss 0.613, lat_loss 21.788
09/29 06:07:27 AM | Train: [139/180] Step 200/312 Loss 4.044 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.613, lat_loss 21.788
09/29 06:08:10 AM | Train: [139/180] Step 250/312 Loss 4.116 Prec@(1,3) (86.0%, 99.2%), ce_loss 0.613, lat_loss 21.788
09/29 06:08:52 AM | Train: [139/180] Step 300/312 Loss 4.140 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.613, lat_loss 21.788
09/29 06:09:03 AM | Train: [139/180] Step 312/312 Loss 4.169 Prec@(1,3) (85.7%, 99.2%), ce_loss 0.613, lat_loss 21.788
09/29 06:09:03 AM | _theta_step_train: [139/180] Final Prec@1 85.7400% Time 267.07
09/29 06:09:13 AM | Valid: [139/180] Step 050/312 Loss 3.944 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.613, lat_loss 21.788
09/29 06:09:23 AM | Valid: [139/180] Step 100/312 Loss 4.733 Prec@(1,3) (84.9%, 98.6%), ce_loss 0.613, lat_loss 21.788
09/29 06:09:32 AM | Valid: [139/180] Step 150/312 Loss 4.394 Prec@(1,3) (85.7%, 98.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:09:42 AM | Valid: [139/180] Step 200/312 Loss 4.191 Prec@(1,3) (86.1%, 98.9%), ce_loss 0.613, lat_loss 21.788
09/29 06:09:52 AM | Valid: [139/180] Step 250/312 Loss 4.126 Prec@(1,3) (86.0%, 98.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:10:01 AM | Valid: [139/180] Step 300/312 Loss 4.026 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.613, lat_loss 21.788
09/29 06:10:04 AM | Valid: [139/180] Step 312/312 Loss 4.013 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.613, lat_loss 21.788
09/29 06:10:04 AM | val: [139/180] Final Prec@1 86.0600% Time 61.03
09/29 06:10:04 AM | Start to train weights for epoch 139
09/29 06:10:52 AM | Train: [140/180] Step 050/1249 Loss 1.842 Prec@(1,3) (92.1%, 99.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:11:42 AM | Train: [140/180] Step 100/1249 Loss 1.779 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.613, lat_loss 21.788
09/29 06:12:31 AM | Train: [140/180] Step 150/1249 Loss 1.892 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:13:20 AM | Train: [140/180] Step 200/1249 Loss 1.842 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:14:08 AM | Train: [140/180] Step 250/1249 Loss 1.853 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:14:57 AM | Train: [140/180] Step 300/1249 Loss 1.833 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:15:44 AM | Train: [140/180] Step 350/1249 Loss 1.787 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:16:27 AM | Train: [140/180] Step 400/1249 Loss 1.775 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:17:15 AM | Train: [140/180] Step 450/1249 Loss 1.803 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:18:04 AM | Train: [140/180] Step 500/1249 Loss 1.854 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:18:53 AM | Train: [140/180] Step 550/1249 Loss 1.844 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:19:41 AM | Train: [140/180] Step 600/1249 Loss 1.840 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:20:29 AM | Train: [140/180] Step 650/1249 Loss 1.855 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:21:17 AM | Train: [140/180] Step 700/1249 Loss 1.848 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:22:00 AM | Train: [140/180] Step 750/1249 Loss 1.860 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.612, lat_loss 21.788
09/29 06:22:49 AM | Train: [140/180] Step 800/1249 Loss 1.851 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:23:37 AM | Train: [140/180] Step 850/1249 Loss 1.838 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:24:26 AM | Train: [140/180] Step 900/1249 Loss 1.829 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:25:13 AM | Train: [140/180] Step 950/1249 Loss 1.823 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:26:01 AM | Train: [140/180] Step 1000/1249 Loss 1.864 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:26:48 AM | Train: [140/180] Step 1050/1249 Loss 1.864 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:27:30 AM | Train: [140/180] Step 1100/1249 Loss 1.850 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:28:19 AM | Train: [140/180] Step 1150/1249 Loss 1.846 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:29:08 AM | Train: [140/180] Step 1200/1249 Loss 1.843 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:29:55 AM | Train: [140/180] Step 1249/1249 Loss 1.843 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.611, lat_loss 21.788
09/29 06:29:55 AM | _w_step_train: [140/180] Final Prec@1 92.8325% Time 1191.73
09/29 06:29:55 AM | Start to train theta for epoch 139
09/29 06:30:41 AM | Train: [140/180] Step 050/312 Loss 3.870 Prec@(1,3) (86.2%, 99.0%), ce_loss 0.611, lat_loss 21.788
09/29 06:31:23 AM | Train: [140/180] Step 100/312 Loss 4.153 Prec@(1,3) (85.3%, 99.3%), ce_loss 0.611, lat_loss 21.788
09/29 06:32:06 AM | Train: [140/180] Step 150/312 Loss 4.134 Prec@(1,3) (85.5%, 99.3%), ce_loss 0.611, lat_loss 21.788
09/29 06:32:45 AM | Train: [140/180] Step 200/312 Loss 4.001 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.611, lat_loss 21.788
09/29 06:33:27 AM | Train: [140/180] Step 250/312 Loss 3.973 Prec@(1,3) (86.0%, 99.4%), ce_loss 0.611, lat_loss 21.788
09/29 06:34:10 AM | Train: [140/180] Step 300/312 Loss 3.915 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.611, lat_loss 21.788
09/29 06:34:20 AM | Train: [140/180] Step 312/312 Loss 3.995 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.611, lat_loss 21.788
09/29 06:34:20 AM | _theta_step_train: [140/180] Final Prec@1 86.0400% Time 264.21
09/29 06:34:30 AM | Valid: [140/180] Step 050/312 Loss 3.793 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.611, lat_loss 21.788
09/29 06:34:40 AM | Valid: [140/180] Step 100/312 Loss 3.868 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.610, lat_loss 21.788
09/29 06:34:49 AM | Valid: [140/180] Step 150/312 Loss 3.935 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.610, lat_loss 21.788
09/29 06:34:59 AM | Valid: [140/180] Step 200/312 Loss 4.058 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.610, lat_loss 21.788
09/29 06:35:09 AM | Valid: [140/180] Step 250/312 Loss 3.918 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.610, lat_loss 21.788
09/29 06:35:19 AM | Valid: [140/180] Step 300/312 Loss 3.852 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.610, lat_loss 21.788
09/29 06:35:21 AM | Valid: [140/180] Step 312/312 Loss 3.854 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.610, lat_loss 21.788
09/29 06:35:21 AM | val: [140/180] Final Prec@1 86.5200% Time 61.36
09/29 06:35:21 AM | Start to train weights for epoch 140
09/29 06:36:11 AM | Train: [141/180] Step 050/1249 Loss 1.638 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.610, lat_loss 21.788
09/29 06:37:02 AM | Train: [141/180] Step 100/1249 Loss 1.782 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.610, lat_loss 21.788
09/29 06:37:52 AM | Train: [141/180] Step 150/1249 Loss 1.762 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.610, lat_loss 21.787
09/29 06:38:37 AM | Train: [141/180] Step 200/1249 Loss 1.720 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.610, lat_loss 21.787
09/29 06:39:27 AM | Train: [141/180] Step 250/1249 Loss 1.765 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.610, lat_loss 21.787
09/29 06:40:17 AM | Train: [141/180] Step 300/1249 Loss 1.761 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.610, lat_loss 21.787
09/29 06:41:08 AM | Train: [141/180] Step 350/1249 Loss 1.752 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.610, lat_loss 21.787
09/29 06:41:58 AM | Train: [141/180] Step 400/1249 Loss 1.760 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.610, lat_loss 21.787
09/29 06:42:48 AM | Train: [141/180] Step 450/1249 Loss 1.771 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.610, lat_loss 21.787
09/29 06:43:37 AM | Train: [141/180] Step 500/1249 Loss 1.741 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.610, lat_loss 21.787
09/29 06:44:20 AM | Train: [141/180] Step 550/1249 Loss 1.742 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:45:09 AM | Train: [141/180] Step 600/1249 Loss 1.741 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:45:58 AM | Train: [141/180] Step 650/1249 Loss 1.747 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:46:47 AM | Train: [141/180] Step 700/1249 Loss 1.734 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:47:35 AM | Train: [141/180] Step 750/1249 Loss 1.747 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:48:25 AM | Train: [141/180] Step 800/1249 Loss 1.738 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:49:15 AM | Train: [141/180] Step 850/1249 Loss 1.743 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:50:02 AM | Train: [141/180] Step 900/1249 Loss 1.765 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:50:52 AM | Train: [141/180] Step 950/1249 Loss 1.754 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:51:41 AM | Train: [141/180] Step 1000/1249 Loss 1.756 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:52:30 AM | Train: [141/180] Step 1050/1249 Loss 1.753 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:53:19 AM | Train: [141/180] Step 1100/1249 Loss 1.756 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:54:08 AM | Train: [141/180] Step 1150/1249 Loss 1.758 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.609, lat_loss 21.787
09/29 06:54:54 AM | Train: [141/180] Step 1200/1249 Loss 1.760 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.608, lat_loss 21.787
09/29 06:55:38 AM | Train: [141/180] Step 1249/1249 Loss 1.761 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.608, lat_loss 21.787
09/29 06:55:38 AM | _w_step_train: [141/180] Final Prec@1 93.0525% Time 1216.97
09/29 06:55:38 AM | Start to train theta for epoch 140
09/29 06:56:22 AM | Train: [141/180] Step 050/312 Loss 3.759 Prec@(1,3) (87.9%, 99.2%), ce_loss 0.608, lat_loss 21.787
09/29 06:57:05 AM | Train: [141/180] Step 100/312 Loss 3.881 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.608, lat_loss 21.787
09/29 06:57:48 AM | Train: [141/180] Step 150/312 Loss 3.910 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.608, lat_loss 21.787
09/29 06:58:31 AM | Train: [141/180] Step 200/312 Loss 3.950 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.608, lat_loss 21.787
09/29 06:59:14 AM | Train: [141/180] Step 250/312 Loss 3.947 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.608, lat_loss 21.787
09/29 06:59:57 AM | Train: [141/180] Step 300/312 Loss 3.989 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.608, lat_loss 21.787
09/29 07:00:07 AM | Train: [141/180] Step 312/312 Loss 4.005 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.608, lat_loss 21.787
09/29 07:00:08 AM | _theta_step_train: [141/180] Final Prec@1 86.3400% Time 269.56
09/29 07:00:18 AM | Valid: [141/180] Step 050/312 Loss 3.804 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.608, lat_loss 21.787
09/29 07:00:28 AM | Valid: [141/180] Step 100/312 Loss 4.011 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.608, lat_loss 21.787
09/29 07:00:36 AM | Valid: [141/180] Step 150/312 Loss 4.373 Prec@(1,3) (85.4%, 98.5%), ce_loss 0.608, lat_loss 21.787
09/29 07:00:44 AM | Valid: [141/180] Step 200/312 Loss 4.170 Prec@(1,3) (85.9%, 98.7%), ce_loss 0.608, lat_loss 21.787
09/29 07:00:52 AM | Valid: [141/180] Step 250/312 Loss 4.146 Prec@(1,3) (85.8%, 98.8%), ce_loss 0.608, lat_loss 21.787
09/29 07:01:01 AM | Valid: [141/180] Step 300/312 Loss 4.149 Prec@(1,3) (85.8%, 98.9%), ce_loss 0.608, lat_loss 21.787
09/29 07:01:04 AM | Valid: [141/180] Step 312/312 Loss 4.115 Prec@(1,3) (85.8%, 98.9%), ce_loss 0.608, lat_loss 21.787
09/29 07:01:04 AM | val: [141/180] Final Prec@1 85.7900% Time 56.04
09/29 07:01:04 AM | Start to train weights for epoch 141
09/29 07:01:55 AM | Train: [142/180] Step 050/1249 Loss 1.842 Prec@(1,3) (92.3%, 99.9%), ce_loss 0.608, lat_loss 21.787
09/29 07:02:44 AM | Train: [142/180] Step 100/1249 Loss 1.731 Prec@(1,3) (92.9%, 100.0%), ce_loss 0.608, lat_loss 21.787
09/29 07:03:33 AM | Train: [142/180] Step 150/1249 Loss 1.791 Prec@(1,3) (92.9%, 99.9%), ce_loss 0.608, lat_loss 21.787
09/29 07:04:21 AM | Train: [142/180] Step 200/1249 Loss 1.819 Prec@(1,3) (92.7%, 99.9%), ce_loss 0.608, lat_loss 21.787
09/29 07:05:10 AM | Train: [142/180] Step 250/1249 Loss 1.763 Prec@(1,3) (92.9%, 99.9%), ce_loss 0.608, lat_loss 21.787
09/29 07:05:58 AM | Train: [142/180] Step 300/1249 Loss 1.765 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.608, lat_loss 21.787
09/29 07:06:39 AM | Train: [142/180] Step 350/1249 Loss 1.748 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.607, lat_loss 21.787
09/29 07:07:28 AM | Train: [142/180] Step 400/1249 Loss 1.756 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:08:16 AM | Train: [142/180] Step 450/1249 Loss 1.777 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:09:05 AM | Train: [142/180] Step 500/1249 Loss 1.774 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:09:54 AM | Train: [142/180] Step 550/1249 Loss 1.765 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.607, lat_loss 21.787
09/29 07:10:43 AM | Train: [142/180] Step 600/1249 Loss 1.788 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.607, lat_loss 21.787
09/29 07:11:32 AM | Train: [142/180] Step 650/1249 Loss 1.774 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.607, lat_loss 21.787
09/29 07:12:15 AM | Train: [142/180] Step 700/1249 Loss 1.830 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:13:03 AM | Train: [142/180] Step 750/1249 Loss 1.819 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:13:51 AM | Train: [142/180] Step 800/1249 Loss 1.841 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:14:42 AM | Train: [142/180] Step 850/1249 Loss 1.841 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:15:31 AM | Train: [142/180] Step 900/1249 Loss 1.832 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:16:20 AM | Train: [142/180] Step 950/1249 Loss 1.831 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.607, lat_loss 21.787
09/29 07:17:09 AM | Train: [142/180] Step 1000/1249 Loss 1.823 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.606, lat_loss 21.787
09/29 07:17:51 AM | Train: [142/180] Step 1050/1249 Loss 1.844 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.606, lat_loss 21.787
09/29 07:18:41 AM | Train: [142/180] Step 1100/1249 Loss 1.847 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.606, lat_loss 21.787
09/29 07:19:31 AM | Train: [142/180] Step 1150/1249 Loss 1.844 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.606, lat_loss 21.787
09/29 07:20:22 AM | Train: [142/180] Step 1200/1249 Loss 1.840 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.606, lat_loss 21.787
09/29 07:21:11 AM | Train: [142/180] Step 1249/1249 Loss 1.835 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.606, lat_loss 21.787
09/29 07:21:11 AM | _w_step_train: [142/180] Final Prec@1 92.8050% Time 1207.50
09/29 07:21:11 AM | Start to train theta for epoch 141
09/29 07:21:56 AM | Train: [142/180] Step 050/312 Loss 3.745 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.606, lat_loss 21.787
09/29 07:22:38 AM | Train: [142/180] Step 100/312 Loss 3.851 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.606, lat_loss 21.787
09/29 07:23:16 AM | Train: [142/180] Step 150/312 Loss 3.726 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.606, lat_loss 21.787
09/29 07:23:59 AM | Train: [142/180] Step 200/312 Loss 3.840 Prec@(1,3) (86.5%, 99.5%), ce_loss 0.606, lat_loss 21.787
09/29 07:24:41 AM | Train: [142/180] Step 250/312 Loss 3.878 Prec@(1,3) (86.3%, 99.5%), ce_loss 0.606, lat_loss 21.787
09/29 07:25:23 AM | Train: [142/180] Step 300/312 Loss 3.908 Prec@(1,3) (86.2%, 99.5%), ce_loss 0.606, lat_loss 21.787
09/29 07:25:33 AM | Train: [142/180] Step 312/312 Loss 3.915 Prec@(1,3) (86.2%, 99.5%), ce_loss 0.606, lat_loss 21.787
09/29 07:25:33 AM | _theta_step_train: [142/180] Final Prec@1 86.1500% Time 262.38
09/29 07:25:44 AM | Valid: [142/180] Step 050/312 Loss 3.546 Prec@(1,3) (87.3%, 99.8%), ce_loss 0.606, lat_loss 21.787
09/29 07:25:54 AM | Valid: [142/180] Step 100/312 Loss 3.794 Prec@(1,3) (86.7%, 99.5%), ce_loss 0.606, lat_loss 21.787
09/29 07:26:03 AM | Valid: [142/180] Step 150/312 Loss 3.868 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.606, lat_loss 21.787
09/29 07:26:13 AM | Valid: [142/180] Step 200/312 Loss 3.904 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.606, lat_loss 21.787
09/29 07:26:23 AM | Valid: [142/180] Step 250/312 Loss 3.921 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.606, lat_loss 21.787
09/29 07:26:33 AM | Valid: [142/180] Step 300/312 Loss 3.869 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.606, lat_loss 21.787
09/29 07:26:35 AM | Valid: [142/180] Step 312/312 Loss 3.861 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.606, lat_loss 21.787
09/29 07:26:35 AM | val: [142/180] Final Prec@1 86.3500% Time 61.65
09/29 07:26:35 AM | Start to train weights for epoch 142
09/29 07:27:28 AM | Train: [143/180] Step 050/1249 Loss 1.684 Prec@(1,3) (93.3%, 100.0%), ce_loss 0.606, lat_loss 21.787
09/29 07:28:17 AM | Train: [143/180] Step 100/1249 Loss 1.765 Prec@(1,3) (92.8%, 100.0%), ce_loss 0.606, lat_loss 21.787
09/29 07:28:59 AM | Train: [143/180] Step 150/1249 Loss 1.744 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.606, lat_loss 21.787
09/29 07:29:48 AM | Train: [143/180] Step 200/1249 Loss 1.746 Prec@(1,3) (92.9%, 99.9%), ce_loss 0.605, lat_loss 21.787
09/29 07:30:37 AM | Train: [143/180] Step 250/1249 Loss 1.723 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.605, lat_loss 21.787
09/29 07:31:26 AM | Train: [143/180] Step 300/1249 Loss 1.727 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.605, lat_loss 21.787
09/29 07:32:15 AM | Train: [143/180] Step 350/1249 Loss 1.778 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:33:05 AM | Train: [143/180] Step 400/1249 Loss 1.777 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:33:55 AM | Train: [143/180] Step 450/1249 Loss 1.780 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:34:38 AM | Train: [143/180] Step 500/1249 Loss 1.781 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:35:27 AM | Train: [143/180] Step 550/1249 Loss 1.800 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:36:15 AM | Train: [143/180] Step 600/1249 Loss 1.815 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:37:05 AM | Train: [143/180] Step 650/1249 Loss 1.831 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:37:54 AM | Train: [143/180] Step 700/1249 Loss 1.814 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:38:42 AM | Train: [143/180] Step 750/1249 Loss 1.803 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:39:31 AM | Train: [143/180] Step 800/1249 Loss 1.791 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.605, lat_loss 21.787
09/29 07:40:14 AM | Train: [143/180] Step 850/1249 Loss 1.780 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.604, lat_loss 21.787
09/29 07:41:03 AM | Train: [143/180] Step 900/1249 Loss 1.786 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.604, lat_loss 21.787
09/29 07:41:52 AM | Train: [143/180] Step 950/1249 Loss 1.790 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.604, lat_loss 21.787
09/29 07:42:41 AM | Train: [143/180] Step 1000/1249 Loss 1.798 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.604, lat_loss 21.787
09/29 07:43:30 AM | Train: [143/180] Step 1050/1249 Loss 1.807 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.604, lat_loss 21.787
09/29 07:44:19 AM | Train: [143/180] Step 1100/1249 Loss 1.811 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.604, lat_loss 21.787
09/29 07:45:07 AM | Train: [143/180] Step 1150/1249 Loss 1.806 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.604, lat_loss 21.787
09/29 07:45:49 AM | Train: [143/180] Step 1200/1249 Loss 1.808 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.604, lat_loss 21.787
09/29 07:46:37 AM | Train: [143/180] Step 1249/1249 Loss 1.795 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.604, lat_loss 21.787
09/29 07:46:37 AM | _w_step_train: [143/180] Final Prec@1 92.9900% Time 1202.25
09/29 07:46:37 AM | Start to train theta for epoch 142
09/29 07:47:22 AM | Train: [143/180] Step 050/312 Loss 3.469 Prec@(1,3) (87.3%, 99.3%), ce_loss 0.604, lat_loss 21.787
09/29 07:48:05 AM | Train: [143/180] Step 100/312 Loss 3.788 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.604, lat_loss 21.786
09/29 07:48:48 AM | Train: [143/180] Step 150/312 Loss 3.875 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.604, lat_loss 21.786
09/29 07:49:32 AM | Train: [143/180] Step 200/312 Loss 3.718 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.604, lat_loss 21.786
09/29 07:50:15 AM | Train: [143/180] Step 250/312 Loss 3.801 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.604, lat_loss 21.786
09/29 07:50:57 AM | Train: [143/180] Step 300/312 Loss 3.818 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.604, lat_loss 21.786
09/29 07:51:06 AM | Train: [143/180] Step 312/312 Loss 3.831 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.604, lat_loss 21.786
09/29 07:51:06 AM | _theta_step_train: [143/180] Final Prec@1 86.4100% Time 268.55
09/29 07:51:15 AM | Valid: [143/180] Step 050/312 Loss 3.329 Prec@(1,3) (88.0%, 99.6%), ce_loss 0.604, lat_loss 21.786
09/29 07:51:24 AM | Valid: [143/180] Step 100/312 Loss 3.851 Prec@(1,3) (86.6%, 99.0%), ce_loss 0.604, lat_loss 21.786
09/29 07:51:33 AM | Valid: [143/180] Step 150/312 Loss 3.859 Prec@(1,3) (86.4%, 99.0%), ce_loss 0.604, lat_loss 21.786
09/29 07:51:43 AM | Valid: [143/180] Step 200/312 Loss 3.799 Prec@(1,3) (86.4%, 99.0%), ce_loss 0.604, lat_loss 21.786
09/29 07:51:53 AM | Valid: [143/180] Step 250/312 Loss 4.048 Prec@(1,3) (85.8%, 98.8%), ce_loss 0.604, lat_loss 21.786
09/29 07:52:03 AM | Valid: [143/180] Step 300/312 Loss 3.995 Prec@(1,3) (85.8%, 98.9%), ce_loss 0.603, lat_loss 21.786
09/29 07:52:05 AM | Valid: [143/180] Step 312/312 Loss 3.994 Prec@(1,3) (85.8%, 98.9%), ce_loss 0.603, lat_loss 21.786
09/29 07:52:05 AM | val: [143/180] Final Prec@1 85.7500% Time 59.02
09/29 07:52:05 AM | Start to train weights for epoch 143
09/29 07:52:57 AM | Train: [144/180] Step 050/1249 Loss 1.522 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 07:53:48 AM | Train: [144/180] Step 100/1249 Loss 1.609 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 07:54:37 AM | Train: [144/180] Step 150/1249 Loss 1.586 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.603, lat_loss 21.786
09/29 07:55:25 AM | Train: [144/180] Step 200/1249 Loss 1.610 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.603, lat_loss 21.786
09/29 07:56:14 AM | Train: [144/180] Step 250/1249 Loss 1.671 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.603, lat_loss 21.786
09/29 07:56:57 AM | Train: [144/180] Step 300/1249 Loss 1.713 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 07:57:46 AM | Train: [144/180] Step 350/1249 Loss 1.745 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 07:58:35 AM | Train: [144/180] Step 400/1249 Loss 1.733 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 07:59:25 AM | Train: [144/180] Step 450/1249 Loss 1.725 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 08:00:14 AM | Train: [144/180] Step 500/1249 Loss 1.734 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 08:01:03 AM | Train: [144/180] Step 550/1249 Loss 1.745 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 08:01:51 AM | Train: [144/180] Step 600/1249 Loss 1.752 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.603, lat_loss 21.786
09/29 08:02:34 AM | Train: [144/180] Step 650/1249 Loss 1.758 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:03:24 AM | Train: [144/180] Step 700/1249 Loss 1.744 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:04:14 AM | Train: [144/180] Step 750/1249 Loss 1.746 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:05:03 AM | Train: [144/180] Step 800/1249 Loss 1.776 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:05:52 AM | Train: [144/180] Step 850/1249 Loss 1.765 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:06:41 AM | Train: [144/180] Step 900/1249 Loss 1.774 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:07:29 AM | Train: [144/180] Step 950/1249 Loss 1.796 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:08:11 AM | Train: [144/180] Step 1000/1249 Loss 1.792 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:09:00 AM | Train: [144/180] Step 1050/1249 Loss 1.778 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:09:50 AM | Train: [144/180] Step 1100/1249 Loss 1.776 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:10:40 AM | Train: [144/180] Step 1150/1249 Loss 1.797 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:11:30 AM | Train: [144/180] Step 1200/1249 Loss 1.797 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:12:19 AM | Train: [144/180] Step 1249/1249 Loss 1.795 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.602, lat_loss 21.786
09/29 08:12:19 AM | _w_step_train: [144/180] Final Prec@1 92.9975% Time 1213.93
09/29 08:12:19 AM | Start to train theta for epoch 143
09/29 08:13:03 AM | Train: [144/180] Step 050/312 Loss 3.731 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.602, lat_loss 21.786
09/29 08:13:40 AM | Train: [144/180] Step 100/312 Loss 3.786 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.602, lat_loss 21.786
09/29 08:14:22 AM | Train: [144/180] Step 150/312 Loss 3.887 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.601, lat_loss 21.786
09/29 08:15:05 AM | Train: [144/180] Step 200/312 Loss 3.907 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.601, lat_loss 21.786
09/29 08:15:49 AM | Train: [144/180] Step 250/312 Loss 3.868 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.601, lat_loss 21.786
09/29 08:16:32 AM | Train: [144/180] Step 300/312 Loss 3.836 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.601, lat_loss 21.786
09/29 08:16:43 AM | Train: [144/180] Step 312/312 Loss 3.800 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.601, lat_loss 21.786
09/29 08:16:43 AM | _theta_step_train: [144/180] Final Prec@1 86.6400% Time 263.78
09/29 08:16:53 AM | Valid: [144/180] Step 050/312 Loss 3.621 Prec@(1,3) (86.7%, 99.8%), ce_loss 0.601, lat_loss 21.786
09/29 08:17:03 AM | Valid: [144/180] Step 100/312 Loss 4.020 Prec@(1,3) (86.0%, 98.9%), ce_loss 0.601, lat_loss 21.786
09/29 08:17:13 AM | Valid: [144/180] Step 150/312 Loss 4.110 Prec@(1,3) (85.9%, 98.9%), ce_loss 0.601, lat_loss 21.786
09/29 08:17:23 AM | Valid: [144/180] Step 200/312 Loss 4.050 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.601, lat_loss 21.786
09/29 08:17:32 AM | Valid: [144/180] Step 250/312 Loss 3.998 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.601, lat_loss 21.786
09/29 08:17:42 AM | Valid: [144/180] Step 300/312 Loss 3.943 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.601, lat_loss 21.786
09/29 08:17:44 AM | Valid: [144/180] Step 312/312 Loss 3.971 Prec@(1,3) (86.0%, 99.1%), ce_loss 0.601, lat_loss 21.786
09/29 08:17:44 AM | val: [144/180] Final Prec@1 86.0100% Time 61.51
09/29 08:17:44 AM | Start to train weights for epoch 144
09/29 08:18:36 AM | Train: [145/180] Step 050/1249 Loss 1.621 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.601, lat_loss 21.786
09/29 08:19:22 AM | Train: [145/180] Step 100/1249 Loss 1.569 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.601, lat_loss 21.786
09/29 08:20:11 AM | Train: [145/180] Step 150/1249 Loss 1.584 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.601, lat_loss 21.786
09/29 08:21:00 AM | Train: [145/180] Step 200/1249 Loss 1.590 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.601, lat_loss 21.786
09/29 08:21:49 AM | Train: [145/180] Step 250/1249 Loss 1.641 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.601, lat_loss 21.786
09/29 08:22:38 AM | Train: [145/180] Step 300/1249 Loss 1.632 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.601, lat_loss 21.786
09/29 08:23:28 AM | Train: [145/180] Step 350/1249 Loss 1.643 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.601, lat_loss 21.786
09/29 08:24:18 AM | Train: [145/180] Step 400/1249 Loss 1.739 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.601, lat_loss 21.786
09/29 08:25:01 AM | Train: [145/180] Step 450/1249 Loss 1.757 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.601, lat_loss 21.786
09/29 08:25:51 AM | Train: [145/180] Step 500/1249 Loss 1.761 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:26:40 AM | Train: [145/180] Step 550/1249 Loss 1.750 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:27:29 AM | Train: [145/180] Step 600/1249 Loss 1.738 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:28:18 AM | Train: [145/180] Step 650/1249 Loss 1.735 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:29:07 AM | Train: [145/180] Step 700/1249 Loss 1.734 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:29:55 AM | Train: [145/180] Step 750/1249 Loss 1.743 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:30:37 AM | Train: [145/180] Step 800/1249 Loss 1.759 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:31:26 AM | Train: [145/180] Step 850/1249 Loss 1.759 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:32:15 AM | Train: [145/180] Step 900/1249 Loss 1.760 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:33:04 AM | Train: [145/180] Step 950/1249 Loss 1.744 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:33:53 AM | Train: [145/180] Step 1000/1249 Loss 1.753 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.600, lat_loss 21.786
09/29 08:34:41 AM | Train: [145/180] Step 1050/1249 Loss 1.746 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.600, lat_loss 21.786
09/29 08:35:30 AM | Train: [145/180] Step 1100/1249 Loss 1.748 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.600, lat_loss 21.786
09/29 08:36:13 AM | Train: [145/180] Step 1150/1249 Loss 1.749 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.599, lat_loss 21.786
09/29 08:37:02 AM | Train: [145/180] Step 1200/1249 Loss 1.738 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.599, lat_loss 21.786
09/29 08:37:49 AM | Train: [145/180] Step 1249/1249 Loss 1.741 Prec@(1,3) (93.1%, 99.9%), ce_loss 0.599, lat_loss 21.786
09/29 08:37:49 AM | _w_step_train: [145/180] Final Prec@1 93.0575% Time 1205.12
09/29 08:37:49 AM | Start to train theta for epoch 144
09/29 08:38:34 AM | Train: [145/180] Step 050/312 Loss 4.038 Prec@(1,3) (87.5%, 99.2%), ce_loss 0.599, lat_loss 21.786
09/29 08:39:17 AM | Train: [145/180] Step 100/312 Loss 4.114 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.599, lat_loss 21.786
09/29 08:39:59 AM | Train: [145/180] Step 150/312 Loss 4.026 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.599, lat_loss 21.786
09/29 08:40:42 AM | Train: [145/180] Step 200/312 Loss 4.023 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.599, lat_loss 21.786
09/29 08:41:24 AM | Train: [145/180] Step 250/312 Loss 4.077 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.599, lat_loss 21.786
09/29 08:42:03 AM | Train: [145/180] Step 300/312 Loss 4.118 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.599, lat_loss 21.786
09/29 08:42:14 AM | Train: [145/180] Step 312/312 Loss 4.075 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.599, lat_loss 21.786
09/29 08:42:14 AM | _theta_step_train: [145/180] Final Prec@1 86.1500% Time 264.23
09/29 08:42:24 AM | Valid: [145/180] Step 050/312 Loss 3.467 Prec@(1,3) (87.5%, 99.6%), ce_loss 0.599, lat_loss 21.786
09/29 08:42:34 AM | Valid: [145/180] Step 100/312 Loss 3.906 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.599, lat_loss 21.786
09/29 08:42:44 AM | Valid: [145/180] Step 150/312 Loss 4.194 Prec@(1,3) (85.7%, 99.1%), ce_loss 0.599, lat_loss 21.786
09/29 08:42:53 AM | Valid: [145/180] Step 200/312 Loss 4.050 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.599, lat_loss 21.786
09/29 08:43:03 AM | Valid: [145/180] Step 250/312 Loss 3.976 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.599, lat_loss 21.786
09/29 08:43:13 AM | Valid: [145/180] Step 300/312 Loss 4.091 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.599, lat_loss 21.786
09/29 08:43:15 AM | Valid: [145/180] Step 312/312 Loss 4.064 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.599, lat_loss 21.786
09/29 08:43:15 AM | val: [145/180] Final Prec@1 86.0700% Time 61.44
09/29 08:43:15 AM | Start to train weights for epoch 145
09/29 08:44:07 AM | Train: [146/180] Step 050/1249 Loss 2.096 Prec@(1,3) (92.6%, 99.6%), ce_loss 0.599, lat_loss 21.786
09/29 08:44:57 AM | Train: [146/180] Step 100/1249 Loss 2.009 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.599, lat_loss 21.786
09/29 08:45:45 AM | Train: [146/180] Step 150/1249 Loss 1.953 Prec@(1,3) (92.6%, 99.8%), ce_loss 0.599, lat_loss 21.786
09/29 08:46:34 AM | Train: [146/180] Step 200/1249 Loss 1.855 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.599, lat_loss 21.786
09/29 08:47:17 AM | Train: [146/180] Step 250/1249 Loss 1.889 Prec@(1,3) (92.7%, 99.8%), ce_loss 0.599, lat_loss 21.786
09/29 08:48:08 AM | Train: [146/180] Step 300/1249 Loss 1.876 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.599, lat_loss 21.786
09/29 08:48:58 AM | Train: [146/180] Step 350/1249 Loss 1.837 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.599, lat_loss 21.786
09/29 08:49:46 AM | Train: [146/180] Step 400/1249 Loss 1.801 Prec@(1,3) (92.9%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:50:34 AM | Train: [146/180] Step 450/1249 Loss 1.777 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:51:23 AM | Train: [146/180] Step 500/1249 Loss 1.795 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:52:12 AM | Train: [146/180] Step 550/1249 Loss 1.788 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:52:54 AM | Train: [146/180] Step 600/1249 Loss 1.813 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:53:42 AM | Train: [146/180] Step 650/1249 Loss 1.802 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:54:31 AM | Train: [146/180] Step 700/1249 Loss 1.795 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:55:20 AM | Train: [146/180] Step 750/1249 Loss 1.789 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:56:08 AM | Train: [146/180] Step 800/1249 Loss 1.768 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:56:57 AM | Train: [146/180] Step 850/1249 Loss 1.767 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:57:46 AM | Train: [146/180] Step 900/1249 Loss 1.767 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:58:31 AM | Train: [146/180] Step 950/1249 Loss 1.757 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 08:59:21 AM | Train: [146/180] Step 1000/1249 Loss 1.762 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.598, lat_loss 21.785
09/29 09:00:11 AM | Train: [146/180] Step 1050/1249 Loss 1.751 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.597, lat_loss 21.785
09/29 09:01:02 AM | Train: [146/180] Step 1100/1249 Loss 1.738 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.597, lat_loss 21.785
09/29 09:01:50 AM | Train: [146/180] Step 1150/1249 Loss 1.737 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.597, lat_loss 21.785
09/29 09:02:40 AM | Train: [146/180] Step 1200/1249 Loss 1.738 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.597, lat_loss 21.785
09/29 09:03:29 AM | Train: [146/180] Step 1249/1249 Loss 1.739 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.597, lat_loss 21.785
09/29 09:03:29 AM | _w_step_train: [146/180] Final Prec@1 93.1550% Time 1213.75
09/29 09:03:29 AM | Start to train theta for epoch 145
09/29 09:04:07 AM | Train: [146/180] Step 050/312 Loss 3.937 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.597, lat_loss 21.785
09/29 09:04:50 AM | Train: [146/180] Step 100/312 Loss 3.900 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.597, lat_loss 21.785
09/29 09:05:33 AM | Train: [146/180] Step 150/312 Loss 3.877 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.597, lat_loss 21.785
09/29 09:06:16 AM | Train: [146/180] Step 200/312 Loss 3.887 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.597, lat_loss 21.785
09/29 09:06:59 AM | Train: [146/180] Step 250/312 Loss 4.042 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.597, lat_loss 21.785
09/29 09:07:42 AM | Train: [146/180] Step 300/312 Loss 4.044 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.597, lat_loss 21.785
09/29 09:07:52 AM | Train: [146/180] Step 312/312 Loss 4.064 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.597, lat_loss 21.785
09/29 09:07:52 AM | _theta_step_train: [146/180] Final Prec@1 86.3000% Time 262.98
09/29 09:08:02 AM | Valid: [146/180] Step 050/312 Loss 3.812 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.597, lat_loss 21.785
09/29 09:08:12 AM | Valid: [146/180] Step 100/312 Loss 4.137 Prec@(1,3) (85.9%, 99.3%), ce_loss 0.597, lat_loss 21.785
09/29 09:08:22 AM | Valid: [146/180] Step 150/312 Loss 4.049 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.597, lat_loss 21.785
09/29 09:08:31 AM | Valid: [146/180] Step 200/312 Loss 4.029 Prec@(1,3) (86.2%, 99.2%), ce_loss 0.597, lat_loss 21.785
09/29 09:08:41 AM | Valid: [146/180] Step 250/312 Loss 3.860 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.597, lat_loss 21.785
09/29 09:08:51 AM | Valid: [146/180] Step 300/312 Loss 4.083 Prec@(1,3) (85.8%, 99.2%), ce_loss 0.597, lat_loss 21.785
09/29 09:08:53 AM | Valid: [146/180] Step 312/312 Loss 4.112 Prec@(1,3) (85.7%, 99.2%), ce_loss 0.597, lat_loss 21.785
09/29 09:08:53 AM | val: [146/180] Final Prec@1 85.7200% Time 61.40
09/29 09:08:53 AM | Start to train weights for epoch 146
09/29 09:09:41 AM | Train: [147/180] Step 050/1249 Loss 1.531 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.597, lat_loss 21.785
09/29 09:10:28 AM | Train: [147/180] Step 100/1249 Loss 1.520 Prec@(1,3) (93.5%, 100.0%), ce_loss 0.597, lat_loss 21.785
09/29 09:11:19 AM | Train: [147/180] Step 150/1249 Loss 1.731 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.597, lat_loss 21.785
09/29 09:12:07 AM | Train: [147/180] Step 200/1249 Loss 1.844 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.597, lat_loss 21.785
09/29 09:12:56 AM | Train: [147/180] Step 250/1249 Loss 1.945 Prec@(1,3) (92.7%, 99.7%), ce_loss 0.597, lat_loss 21.785
09/29 09:13:45 AM | Train: [147/180] Step 300/1249 Loss 1.904 Prec@(1,3) (92.8%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:14:33 AM | Train: [147/180] Step 350/1249 Loss 1.853 Prec@(1,3) (93.0%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:15:18 AM | Train: [147/180] Step 400/1249 Loss 1.798 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:16:05 AM | Train: [147/180] Step 450/1249 Loss 1.751 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:16:54 AM | Train: [147/180] Step 500/1249 Loss 1.740 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:17:42 AM | Train: [147/180] Step 550/1249 Loss 1.746 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:18:31 AM | Train: [147/180] Step 600/1249 Loss 1.731 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:19:19 AM | Train: [147/180] Step 650/1249 Loss 1.737 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:20:08 AM | Train: [147/180] Step 700/1249 Loss 1.720 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.596, lat_loss 21.785
09/29 09:20:52 AM | Train: [147/180] Step 750/1249 Loss 1.716 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.596, lat_loss 21.785
09/29 09:21:40 AM | Train: [147/180] Step 800/1249 Loss 1.709 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.596, lat_loss 21.785
09/29 09:22:29 AM | Train: [147/180] Step 850/1249 Loss 1.689 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.596, lat_loss 21.785
09/29 09:23:19 AM | Train: [147/180] Step 900/1249 Loss 1.701 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.596, lat_loss 21.785
09/29 09:24:09 AM | Train: [147/180] Step 950/1249 Loss 1.710 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.595, lat_loss 21.785
09/29 09:24:59 AM | Train: [147/180] Step 1000/1249 Loss 1.695 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.595, lat_loss 21.785
09/29 09:25:48 AM | Train: [147/180] Step 1050/1249 Loss 1.705 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.595, lat_loss 21.785
09/29 09:26:30 AM | Train: [147/180] Step 1100/1249 Loss 1.713 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.595, lat_loss 21.785
09/29 09:27:19 AM | Train: [147/180] Step 1150/1249 Loss 1.704 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.595, lat_loss 21.785
09/29 09:28:08 AM | Train: [147/180] Step 1200/1249 Loss 1.718 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.595, lat_loss 21.785
09/29 09:28:56 AM | Train: [147/180] Step 1249/1249 Loss 1.709 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.595, lat_loss 21.785
09/29 09:28:56 AM | _w_step_train: [147/180] Final Prec@1 93.2975% Time 1202.73
09/29 09:28:56 AM | Start to train theta for epoch 146
09/29 09:29:41 AM | Train: [147/180] Step 050/312 Loss 3.742 Prec@(1,3) (87.5%, 98.8%), ce_loss 0.595, lat_loss 21.785
09/29 09:30:23 AM | Train: [147/180] Step 100/312 Loss 3.932 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.595, lat_loss 21.785
09/29 09:31:06 AM | Train: [147/180] Step 150/312 Loss 3.848 Prec@(1,3) (87.0%, 99.2%), ce_loss 0.595, lat_loss 21.785
09/29 09:31:48 AM | Train: [147/180] Step 200/312 Loss 3.835 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.595, lat_loss 21.785
09/29 09:32:25 AM | Train: [147/180] Step 250/312 Loss 3.828 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.595, lat_loss 21.785
09/29 09:33:08 AM | Train: [147/180] Step 300/312 Loss 3.996 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.595, lat_loss 21.785
09/29 09:33:18 AM | Train: [147/180] Step 312/312 Loss 3.949 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.595, lat_loss 21.785
09/29 09:33:18 AM | _theta_step_train: [147/180] Final Prec@1 86.3200% Time 262.45
09/29 09:33:29 AM | Valid: [147/180] Step 050/312 Loss 3.291 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.595, lat_loss 21.785
09/29 09:33:39 AM | Valid: [147/180] Step 100/312 Loss 3.905 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.595, lat_loss 21.785
09/29 09:33:48 AM | Valid: [147/180] Step 150/312 Loss 3.908 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.595, lat_loss 21.785
09/29 09:33:58 AM | Valid: [147/180] Step 200/312 Loss 3.809 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.595, lat_loss 21.785
09/29 09:34:08 AM | Valid: [147/180] Step 250/312 Loss 3.710 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.595, lat_loss 21.785
09/29 09:34:18 AM | Valid: [147/180] Step 300/312 Loss 3.708 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.595, lat_loss 21.785
09/29 09:34:20 AM | Valid: [147/180] Step 312/312 Loss 3.690 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.595, lat_loss 21.785
09/29 09:34:20 AM | val: [147/180] Final Prec@1 86.9700% Time 61.42
09/29 09:34:20 AM | Start to train weights for epoch 147
09/29 09:35:12 AM | Train: [148/180] Step 050/1249 Loss 1.282 Prec@(1,3) (94.9%, 99.9%), ce_loss 0.595, lat_loss 21.785
09/29 09:36:02 AM | Train: [148/180] Step 100/1249 Loss 1.516 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:36:50 AM | Train: [148/180] Step 150/1249 Loss 1.556 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:37:36 AM | Train: [148/180] Step 200/1249 Loss 1.497 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:38:21 AM | Train: [148/180] Step 250/1249 Loss 1.590 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:39:10 AM | Train: [148/180] Step 300/1249 Loss 1.598 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:39:59 AM | Train: [148/180] Step 350/1249 Loss 1.623 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:40:48 AM | Train: [148/180] Step 400/1249 Loss 1.648 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:41:37 AM | Train: [148/180] Step 450/1249 Loss 1.661 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:42:27 AM | Train: [148/180] Step 500/1249 Loss 1.655 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:43:13 AM | Train: [148/180] Step 550/1249 Loss 1.676 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:44:00 AM | Train: [148/180] Step 600/1249 Loss 1.684 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:44:50 AM | Train: [148/180] Step 650/1249 Loss 1.666 Prec@(1,3) (93.3%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:45:41 AM | Train: [148/180] Step 700/1249 Loss 1.653 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:46:31 AM | Train: [148/180] Step 750/1249 Loss 1.650 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.594, lat_loss 21.785
09/29 09:47:21 AM | Train: [148/180] Step 800/1249 Loss 1.647 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.593, lat_loss 21.785
09/29 09:48:12 AM | Train: [148/180] Step 850/1249 Loss 1.659 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.593, lat_loss 21.785
09/29 09:48:56 AM | Train: [148/180] Step 900/1249 Loss 1.682 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.593, lat_loss 21.785
09/29 09:49:43 AM | Train: [148/180] Step 950/1249 Loss 1.672 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.593, lat_loss 21.785
09/29 09:50:32 AM | Train: [148/180] Step 1000/1249 Loss 1.663 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.593, lat_loss 21.785
09/29 09:51:21 AM | Train: [148/180] Step 1050/1249 Loss 1.697 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.593, lat_loss 21.785
09/29 09:52:10 AM | Train: [148/180] Step 1100/1249 Loss 1.709 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.593, lat_loss 21.785
09/29 09:53:00 AM | Train: [148/180] Step 1150/1249 Loss 1.713 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.593, lat_loss 21.785
09/29 09:53:49 AM | Train: [148/180] Step 1200/1249 Loss 1.709 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.593, lat_loss 21.785
09/29 09:54:30 AM | Train: [148/180] Step 1249/1249 Loss 1.710 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.593, lat_loss 21.784
09/29 09:54:31 AM | _w_step_train: [148/180] Final Prec@1 93.3425% Time 1210.65
09/29 09:54:31 AM | Start to train theta for epoch 147
09/29 09:55:15 AM | Train: [148/180] Step 050/312 Loss 3.904 Prec@(1,3) (86.2%, 99.0%), ce_loss 0.593, lat_loss 21.784
09/29 09:55:58 AM | Train: [148/180] Step 100/312 Loss 3.807 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.593, lat_loss 21.784
09/29 09:56:40 AM | Train: [148/180] Step 150/312 Loss 3.841 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.593, lat_loss 21.784
09/29 09:57:23 AM | Train: [148/180] Step 200/312 Loss 3.763 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.593, lat_loss 21.784
09/29 09:58:05 AM | Train: [148/180] Step 250/312 Loss 3.810 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.593, lat_loss 21.784
09/29 09:58:49 AM | Train: [148/180] Step 300/312 Loss 3.821 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.593, lat_loss 21.784
09/29 09:58:59 AM | Train: [148/180] Step 312/312 Loss 3.827 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.593, lat_loss 21.784
09/29 09:58:59 AM | _theta_step_train: [148/180] Final Prec@1 86.3000% Time 268.85
09/29 09:59:10 AM | Valid: [148/180] Step 050/312 Loss 3.475 Prec@(1,3) (87.3%, 99.5%), ce_loss 0.593, lat_loss 21.784
09/29 09:59:19 AM | Valid: [148/180] Step 100/312 Loss 3.970 Prec@(1,3) (85.8%, 99.3%), ce_loss 0.593, lat_loss 21.784
09/29 09:59:29 AM | Valid: [148/180] Step 150/312 Loss 3.893 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.593, lat_loss 21.784
09/29 09:59:39 AM | Valid: [148/180] Step 200/312 Loss 3.753 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.592, lat_loss 21.784
09/29 09:59:48 AM | Valid: [148/180] Step 250/312 Loss 3.646 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.592, lat_loss 21.784
09/29 09:59:58 AM | Valid: [148/180] Step 300/312 Loss 3.607 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.592, lat_loss 21.784
09/29 10:00:00 AM | Valid: [148/180] Step 312/312 Loss 3.597 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.592, lat_loss 21.784
09/29 10:00:00 AM | val: [148/180] Final Prec@1 86.6000% Time 60.14
09/29 10:00:00 AM | Start to train weights for epoch 148
09/29 10:00:49 AM | Train: [149/180] Step 050/1249 Loss 1.631 Prec@(1,3) (93.6%, 100.0%), ce_loss 0.592, lat_loss 21.784
09/29 10:01:39 AM | Train: [149/180] Step 100/1249 Loss 1.640 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:02:28 AM | Train: [149/180] Step 150/1249 Loss 1.655 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:03:17 AM | Train: [149/180] Step 200/1249 Loss 1.716 Prec@(1,3) (93.2%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:04:06 AM | Train: [149/180] Step 250/1249 Loss 1.654 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:04:56 AM | Train: [149/180] Step 300/1249 Loss 1.615 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:05:43 AM | Train: [149/180] Step 350/1249 Loss 1.582 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:06:31 AM | Train: [149/180] Step 400/1249 Loss 1.610 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:07:20 AM | Train: [149/180] Step 450/1249 Loss 1.633 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:08:10 AM | Train: [149/180] Step 500/1249 Loss 1.621 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:09:00 AM | Train: [149/180] Step 550/1249 Loss 1.597 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:09:51 AM | Train: [149/180] Step 600/1249 Loss 1.596 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.592, lat_loss 21.784
09/29 10:10:41 AM | Train: [149/180] Step 650/1249 Loss 1.631 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:11:27 AM | Train: [149/180] Step 700/1249 Loss 1.631 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:12:15 AM | Train: [149/180] Step 750/1249 Loss 1.615 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:13:04 AM | Train: [149/180] Step 800/1249 Loss 1.619 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:13:52 AM | Train: [149/180] Step 850/1249 Loss 1.606 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:14:41 AM | Train: [149/180] Step 900/1249 Loss 1.620 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:15:30 AM | Train: [149/180] Step 950/1249 Loss 1.626 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:16:19 AM | Train: [149/180] Step 1000/1249 Loss 1.639 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:17:05 AM | Train: [149/180] Step 1050/1249 Loss 1.636 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:17:55 AM | Train: [149/180] Step 1100/1249 Loss 1.634 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:18:46 AM | Train: [149/180] Step 1150/1249 Loss 1.633 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:19:35 AM | Train: [149/180] Step 1200/1249 Loss 1.632 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:20:23 AM | Train: [149/180] Step 1249/1249 Loss 1.631 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.591, lat_loss 21.784
09/29 10:20:23 AM | _w_step_train: [149/180] Final Prec@1 93.6325% Time 1223.39
09/29 10:20:23 AM | Start to train theta for epoch 148
09/29 10:21:08 AM | Train: [149/180] Step 050/312 Loss 3.670 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.591, lat_loss 21.784
09/29 10:21:52 AM | Train: [149/180] Step 100/312 Loss 3.658 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.590, lat_loss 21.784
09/29 10:22:31 AM | Train: [149/180] Step 150/312 Loss 3.712 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.590, lat_loss 21.784
09/29 10:23:11 AM | Train: [149/180] Step 200/312 Loss 3.801 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.590, lat_loss 21.784
09/29 10:23:54 AM | Train: [149/180] Step 250/312 Loss 3.791 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.590, lat_loss 21.784
09/29 10:24:36 AM | Train: [149/180] Step 300/312 Loss 3.849 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.590, lat_loss 21.784
09/29 10:24:46 AM | Train: [149/180] Step 312/312 Loss 3.820 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.590, lat_loss 21.784
09/29 10:24:46 AM | _theta_step_train: [149/180] Final Prec@1 86.3700% Time 263.28
09/29 10:24:57 AM | Valid: [149/180] Step 050/312 Loss 3.475 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.590, lat_loss 21.784
09/29 10:25:07 AM | Valid: [149/180] Step 100/312 Loss 3.727 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.590, lat_loss 21.784
09/29 10:25:16 AM | Valid: [149/180] Step 150/312 Loss 3.820 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.590, lat_loss 21.784
09/29 10:25:26 AM | Valid: [149/180] Step 200/312 Loss 3.891 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.590, lat_loss 21.784
09/29 10:25:36 AM | Valid: [149/180] Step 250/312 Loss 3.752 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.590, lat_loss 21.784
09/29 10:25:46 AM | Valid: [149/180] Step 300/312 Loss 3.721 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.590, lat_loss 21.784
09/29 10:25:48 AM | Valid: [149/180] Step 312/312 Loss 3.724 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.590, lat_loss 21.784
09/29 10:25:48 AM | val: [149/180] Final Prec@1 86.7900% Time 61.92
09/29 10:25:48 AM | Start to train weights for epoch 149
09/29 10:26:41 AM | Train: [150/180] Step 050/1249 Loss 1.670 Prec@(1,3) (93.5%, 99.7%), ce_loss 0.590, lat_loss 21.784
09/29 10:27:31 AM | Train: [150/180] Step 100/1249 Loss 1.530 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.590, lat_loss 21.784
09/29 10:28:17 AM | Train: [150/180] Step 150/1249 Loss 1.507 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.590, lat_loss 21.784
09/29 10:29:03 AM | Train: [150/180] Step 200/1249 Loss 1.494 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.590, lat_loss 21.784
09/29 10:29:52 AM | Train: [150/180] Step 250/1249 Loss 1.508 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.590, lat_loss 21.784
09/29 10:30:41 AM | Train: [150/180] Step 300/1249 Loss 1.519 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.590, lat_loss 21.784
09/29 10:31:30 AM | Train: [150/180] Step 350/1249 Loss 1.519 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.590, lat_loss 21.784
09/29 10:32:19 AM | Train: [150/180] Step 400/1249 Loss 1.532 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.590, lat_loss 21.784
09/29 10:33:08 AM | Train: [150/180] Step 450/1249 Loss 1.537 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.590, lat_loss 21.784
09/29 10:33:54 AM | Train: [150/180] Step 500/1249 Loss 1.530 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.589, lat_loss 21.784
09/29 10:34:43 AM | Train: [150/180] Step 550/1249 Loss 1.555 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:35:32 AM | Train: [150/180] Step 600/1249 Loss 1.573 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:36:21 AM | Train: [150/180] Step 650/1249 Loss 1.594 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:37:10 AM | Train: [150/180] Step 700/1249 Loss 1.596 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:37:59 AM | Train: [150/180] Step 750/1249 Loss 1.584 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:38:48 AM | Train: [150/180] Step 800/1249 Loss 1.567 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:39:30 AM | Train: [150/180] Step 850/1249 Loss 1.548 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.589, lat_loss 21.784
09/29 10:40:20 AM | Train: [150/180] Step 900/1249 Loss 1.621 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:41:09 AM | Train: [150/180] Step 950/1249 Loss 1.621 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:41:58 AM | Train: [150/180] Step 1000/1249 Loss 1.637 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:42:46 AM | Train: [150/180] Step 1050/1249 Loss 1.634 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:43:34 AM | Train: [150/180] Step 1100/1249 Loss 1.629 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:44:23 AM | Train: [150/180] Step 1150/1249 Loss 1.631 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.589, lat_loss 21.784
09/29 10:45:05 AM | Train: [150/180] Step 1200/1249 Loss 1.622 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.588, lat_loss 21.784
09/29 10:45:53 AM | Train: [150/180] Step 1249/1249 Loss 1.633 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.588, lat_loss 21.784
09/29 10:45:53 AM | _w_step_train: [150/180] Final Prec@1 93.5875% Time 1204.52
09/29 10:45:53 AM | Start to train theta for epoch 149
09/29 10:46:37 AM | Train: [150/180] Step 050/312 Loss 3.747 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.588, lat_loss 21.784
09/29 10:47:19 AM | Train: [150/180] Step 100/312 Loss 3.920 Prec@(1,3) (86.2%, 99.4%), ce_loss 0.588, lat_loss 21.784
09/29 10:48:01 AM | Train: [150/180] Step 150/312 Loss 3.839 Prec@(1,3) (86.4%, 99.5%), ce_loss 0.588, lat_loss 21.784
09/29 10:48:45 AM | Train: [150/180] Step 200/312 Loss 3.868 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.588, lat_loss 21.784
09/29 10:49:28 AM | Train: [150/180] Step 250/312 Loss 3.831 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.588, lat_loss 21.784
09/29 10:50:10 AM | Train: [150/180] Step 300/312 Loss 3.805 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.588, lat_loss 21.784
09/29 10:50:20 AM | Train: [150/180] Step 312/312 Loss 3.824 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.588, lat_loss 21.784
09/29 10:50:20 AM | _theta_step_train: [150/180] Final Prec@1 86.6000% Time 267.38
09/29 10:50:29 AM | Valid: [150/180] Step 050/312 Loss 3.516 Prec@(1,3) (87.6%, 99.6%), ce_loss 0.588, lat_loss 21.784
09/29 10:50:37 AM | Valid: [150/180] Step 100/312 Loss 3.869 Prec@(1,3) (86.7%, 99.5%), ce_loss 0.588, lat_loss 21.784
09/29 10:50:46 AM | Valid: [150/180] Step 150/312 Loss 4.128 Prec@(1,3) (86.3%, 99.1%), ce_loss 0.588, lat_loss 21.784
09/29 10:50:55 AM | Valid: [150/180] Step 200/312 Loss 4.051 Prec@(1,3) (86.3%, 99.1%), ce_loss 0.588, lat_loss 21.784
09/29 10:51:05 AM | Valid: [150/180] Step 250/312 Loss 3.925 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.588, lat_loss 21.784
09/29 10:51:14 AM | Valid: [150/180] Step 300/312 Loss 3.865 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.588, lat_loss 21.784
09/29 10:51:17 AM | Valid: [150/180] Step 312/312 Loss 3.852 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.588, lat_loss 21.784
09/29 10:51:17 AM | val: [150/180] Final Prec@1 86.5500% Time 56.62
09/29 10:51:17 AM | Start to train weights for epoch 150
09/29 10:52:09 AM | Train: [151/180] Step 050/1249 Loss 1.597 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.588, lat_loss 21.784
09/29 10:52:59 AM | Train: [151/180] Step 100/1249 Loss 1.569 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.588, lat_loss 21.784
09/29 10:53:49 AM | Train: [151/180] Step 150/1249 Loss 1.615 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.588, lat_loss 21.784
09/29 10:54:39 AM | Train: [151/180] Step 200/1249 Loss 1.580 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.588, lat_loss 21.784
09/29 10:55:29 AM | Train: [151/180] Step 250/1249 Loss 1.540 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.588, lat_loss 21.784
09/29 10:56:13 AM | Train: [151/180] Step 300/1249 Loss 1.539 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.588, lat_loss 21.784
09/29 10:56:59 AM | Train: [151/180] Step 350/1249 Loss 1.525 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.588, lat_loss 21.784
09/29 10:57:48 AM | Train: [151/180] Step 400/1249 Loss 1.539 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.587, lat_loss 21.784
09/29 10:58:37 AM | Train: [151/180] Step 450/1249 Loss 1.548 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.587, lat_loss 21.784
09/29 10:59:25 AM | Train: [151/180] Step 500/1249 Loss 1.574 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.587, lat_loss 21.784
09/29 11:00:14 AM | Train: [151/180] Step 550/1249 Loss 1.546 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.587, lat_loss 21.784
09/29 11:01:03 AM | Train: [151/180] Step 600/1249 Loss 1.573 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.587, lat_loss 21.784
09/29 11:01:49 AM | Train: [151/180] Step 650/1249 Loss 1.565 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.587, lat_loss 21.784
09/29 11:02:39 AM | Train: [151/180] Step 700/1249 Loss 1.566 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.587, lat_loss 21.784
09/29 11:03:28 AM | Train: [151/180] Step 750/1249 Loss 1.606 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.587, lat_loss 21.784
09/29 11:04:16 AM | Train: [151/180] Step 800/1249 Loss 1.614 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.587, lat_loss 21.784
09/29 11:05:05 AM | Train: [151/180] Step 850/1249 Loss 1.601 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.587, lat_loss 21.784
09/29 11:05:54 AM | Train: [151/180] Step 900/1249 Loss 1.588 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.587, lat_loss 21.784
09/29 11:06:42 AM | Train: [151/180] Step 950/1249 Loss 1.593 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.587, lat_loss 21.784
09/29 11:07:25 AM | Train: [151/180] Step 1000/1249 Loss 1.584 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.587, lat_loss 21.784
09/29 11:08:11 AM | Train: [151/180] Step 1050/1249 Loss 1.579 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.586, lat_loss 21.784
09/29 11:09:00 AM | Train: [151/180] Step 1100/1249 Loss 1.591 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.586, lat_loss 21.784
09/29 11:09:48 AM | Train: [151/180] Step 1150/1249 Loss 1.584 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.586, lat_loss 21.784
09/29 11:10:37 AM | Train: [151/180] Step 1200/1249 Loss 1.581 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.586, lat_loss 21.784
09/29 11:11:24 AM | Train: [151/180] Step 1249/1249 Loss 1.583 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.586, lat_loss 21.784
09/29 11:11:24 AM | _w_step_train: [151/180] Final Prec@1 93.7950% Time 1207.39
09/29 11:11:24 AM | Start to train theta for epoch 150
09/29 11:12:09 AM | Train: [151/180] Step 050/312 Loss 3.820 Prec@(1,3) (85.7%, 99.6%), ce_loss 0.586, lat_loss 21.784
09/29 11:12:50 AM | Train: [151/180] Step 100/312 Loss 3.833 Prec@(1,3) (86.2%, 99.6%), ce_loss 0.586, lat_loss 21.783
09/29 11:13:28 AM | Train: [151/180] Step 150/312 Loss 3.756 Prec@(1,3) (86.4%, 99.6%), ce_loss 0.586, lat_loss 21.783
09/29 11:14:11 AM | Train: [151/180] Step 200/312 Loss 3.801 Prec@(1,3) (86.5%, 99.5%), ce_loss 0.586, lat_loss 21.783
09/29 11:14:54 AM | Train: [151/180] Step 250/312 Loss 3.859 Prec@(1,3) (86.6%, 99.5%), ce_loss 0.586, lat_loss 21.783
09/29 11:15:36 AM | Train: [151/180] Step 300/312 Loss 3.829 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.586, lat_loss 21.783
09/29 11:15:46 AM | Train: [151/180] Step 312/312 Loss 3.849 Prec@(1,3) (86.7%, 99.5%), ce_loss 0.586, lat_loss 21.783
09/29 11:15:47 AM | _theta_step_train: [151/180] Final Prec@1 86.7100% Time 262.31
09/29 11:15:57 AM | Valid: [151/180] Step 050/312 Loss 3.271 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.586, lat_loss 21.783
09/29 11:16:07 AM | Valid: [151/180] Step 100/312 Loss 3.685 Prec@(1,3) (87.3%, 99.2%), ce_loss 0.586, lat_loss 21.783
09/29 11:16:17 AM | Valid: [151/180] Step 150/312 Loss 3.769 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.586, lat_loss 21.783
09/29 11:16:26 AM | Valid: [151/180] Step 200/312 Loss 3.678 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.586, lat_loss 21.783
09/29 11:16:36 AM | Valid: [151/180] Step 250/312 Loss 3.607 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.586, lat_loss 21.783
09/29 11:16:46 AM | Valid: [151/180] Step 300/312 Loss 3.704 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.586, lat_loss 21.783
09/29 11:16:48 AM | Valid: [151/180] Step 312/312 Loss 3.749 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.586, lat_loss 21.783
09/29 11:16:48 AM | val: [151/180] Final Prec@1 86.5500% Time 61.59
09/29 11:16:48 AM | Start to train weights for epoch 151
09/29 11:17:39 AM | Train: [152/180] Step 050/1249 Loss 1.535 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.586, lat_loss 21.783
09/29 11:18:28 AM | Train: [152/180] Step 100/1249 Loss 1.726 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.586, lat_loss 21.783
09/29 11:19:11 AM | Train: [152/180] Step 150/1249 Loss 1.637 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.586, lat_loss 21.783
09/29 11:20:00 AM | Train: [152/180] Step 200/1249 Loss 1.559 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.586, lat_loss 21.783
09/29 11:20:48 AM | Train: [152/180] Step 250/1249 Loss 1.688 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:21:37 AM | Train: [152/180] Step 300/1249 Loss 1.646 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:22:26 AM | Train: [152/180] Step 350/1249 Loss 1.642 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:23:14 AM | Train: [152/180] Step 400/1249 Loss 1.667 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:24:02 AM | Train: [152/180] Step 450/1249 Loss 1.640 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:24:44 AM | Train: [152/180] Step 500/1249 Loss 1.637 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:25:33 AM | Train: [152/180] Step 550/1249 Loss 1.635 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:26:22 AM | Train: [152/180] Step 600/1249 Loss 1.690 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:27:10 AM | Train: [152/180] Step 650/1249 Loss 1.685 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:27:59 AM | Train: [152/180] Step 700/1249 Loss 1.741 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:28:47 AM | Train: [152/180] Step 750/1249 Loss 1.737 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:29:35 AM | Train: [152/180] Step 800/1249 Loss 1.712 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:30:17 AM | Train: [152/180] Step 850/1249 Loss 1.711 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:31:05 AM | Train: [152/180] Step 900/1249 Loss 1.708 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:31:54 AM | Train: [152/180] Step 950/1249 Loss 1.694 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.585, lat_loss 21.783
09/29 11:32:43 AM | Train: [152/180] Step 1000/1249 Loss 1.702 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.584, lat_loss 21.783
09/29 11:33:30 AM | Train: [152/180] Step 1050/1249 Loss 1.683 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.584, lat_loss 21.783
09/29 11:34:18 AM | Train: [152/180] Step 1100/1249 Loss 1.674 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.584, lat_loss 21.783
09/29 11:35:06 AM | Train: [152/180] Step 1150/1249 Loss 1.679 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.584, lat_loss 21.783
09/29 11:35:49 AM | Train: [152/180] Step 1200/1249 Loss 1.682 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.584, lat_loss 21.783
09/29 11:36:36 AM | Train: [152/180] Step 1249/1249 Loss 1.676 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.584, lat_loss 21.783
09/29 11:36:36 AM | _w_step_train: [152/180] Final Prec@1 93.4475% Time 1188.07
09/29 11:36:36 AM | Start to train theta for epoch 151
09/29 11:37:21 AM | Train: [152/180] Step 050/312 Loss 3.625 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.584, lat_loss 21.783
09/29 11:38:05 AM | Train: [152/180] Step 100/312 Loss 3.630 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.584, lat_loss 21.783
09/29 11:38:47 AM | Train: [152/180] Step 150/312 Loss 3.791 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.584, lat_loss 21.783
09/29 11:39:30 AM | Train: [152/180] Step 200/312 Loss 3.754 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.584, lat_loss 21.783
09/29 11:40:12 AM | Train: [152/180] Step 250/312 Loss 3.738 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.584, lat_loss 21.783
09/29 11:40:54 AM | Train: [152/180] Step 300/312 Loss 3.830 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.584, lat_loss 21.783
09/29 11:41:02 AM | Train: [152/180] Step 312/312 Loss 3.815 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.584, lat_loss 21.783
09/29 11:41:02 AM | _theta_step_train: [152/180] Final Prec@1 86.5400% Time 265.85
09/29 11:41:11 AM | Valid: [152/180] Step 050/312 Loss 3.582 Prec@(1,3) (87.3%, 99.6%), ce_loss 0.584, lat_loss 21.783
09/29 11:41:19 AM | Valid: [152/180] Step 100/312 Loss 4.354 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.584, lat_loss 21.783
09/29 11:41:29 AM | Valid: [152/180] Step 150/312 Loss 4.332 Prec@(1,3) (85.6%, 99.0%), ce_loss 0.584, lat_loss 21.783
09/29 11:41:38 AM | Valid: [152/180] Step 200/312 Loss 4.151 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.584, lat_loss 21.783
09/29 11:41:48 AM | Valid: [152/180] Step 250/312 Loss 4.256 Prec@(1,3) (86.1%, 98.9%), ce_loss 0.584, lat_loss 21.783
09/29 11:41:58 AM | Valid: [152/180] Step 300/312 Loss 4.161 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.584, lat_loss 21.783
09/29 11:42:00 AM | Valid: [152/180] Step 312/312 Loss 4.131 Prec@(1,3) (86.2%, 99.0%), ce_loss 0.584, lat_loss 21.783
09/29 11:42:00 AM | val: [152/180] Final Prec@1 86.1700% Time 58.12
09/29 11:42:00 AM | Start to train weights for epoch 152
09/29 11:42:52 AM | Train: [153/180] Step 050/1249 Loss 1.439 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.584, lat_loss 21.783
09/29 11:43:42 AM | Train: [153/180] Step 100/1249 Loss 1.438 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.584, lat_loss 21.783
09/29 11:44:31 AM | Train: [153/180] Step 150/1249 Loss 1.463 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.584, lat_loss 21.783
09/29 11:45:20 AM | Train: [153/180] Step 200/1249 Loss 1.486 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.584, lat_loss 21.783
09/29 11:46:09 AM | Train: [153/180] Step 250/1249 Loss 1.463 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:46:51 AM | Train: [153/180] Step 300/1249 Loss 1.536 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:47:40 AM | Train: [153/180] Step 350/1249 Loss 1.513 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:48:30 AM | Train: [153/180] Step 400/1249 Loss 1.496 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:49:20 AM | Train: [153/180] Step 450/1249 Loss 1.499 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:50:09 AM | Train: [153/180] Step 500/1249 Loss 1.521 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:50:58 AM | Train: [153/180] Step 550/1249 Loss 1.501 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:51:47 AM | Train: [153/180] Step 600/1249 Loss 1.529 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:52:31 AM | Train: [153/180] Step 650/1249 Loss 1.526 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:53:19 AM | Train: [153/180] Step 700/1249 Loss 1.520 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:54:08 AM | Train: [153/180] Step 750/1249 Loss 1.525 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:54:58 AM | Train: [153/180] Step 800/1249 Loss 1.572 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:55:47 AM | Train: [153/180] Step 850/1249 Loss 1.580 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:56:35 AM | Train: [153/180] Step 900/1249 Loss 1.595 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.583, lat_loss 21.783
09/29 11:57:24 AM | Train: [153/180] Step 950/1249 Loss 1.624 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.582, lat_loss 21.783
09/29 11:58:07 AM | Train: [153/180] Step 1000/1249 Loss 1.631 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.582, lat_loss 21.783
09/29 11:58:56 AM | Train: [153/180] Step 1050/1249 Loss 1.630 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.582, lat_loss 21.783
09/29 11:59:44 AM | Train: [153/180] Step 1100/1249 Loss 1.655 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.582, lat_loss 21.783
09/29 12:00:33 PM | Train: [153/180] Step 1150/1249 Loss 1.660 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.582, lat_loss 21.783
09/29 12:01:23 PM | Train: [153/180] Step 1200/1249 Loss 1.673 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.582, lat_loss 21.783
09/29 12:02:12 PM | Train: [153/180] Step 1249/1249 Loss 1.669 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.582, lat_loss 21.783
09/29 12:02:12 PM | _w_step_train: [153/180] Final Prec@1 93.4800% Time 1212.09
09/29 12:02:12 PM | Start to train theta for epoch 152
09/29 12:02:56 PM | Train: [153/180] Step 050/312 Loss 3.657 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.582, lat_loss 21.783
09/29 12:03:34 PM | Train: [153/180] Step 100/312 Loss 3.787 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.582, lat_loss 21.783
09/29 12:04:18 PM | Train: [153/180] Step 150/312 Loss 3.942 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.582, lat_loss 21.783
09/29 12:05:01 PM | Train: [153/180] Step 200/312 Loss 3.924 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.582, lat_loss 21.783
09/29 12:05:45 PM | Train: [153/180] Step 250/312 Loss 3.814 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.582, lat_loss 21.783
09/29 12:06:28 PM | Train: [153/180] Step 300/312 Loss 3.812 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.582, lat_loss 21.783
09/29 12:06:38 PM | Train: [153/180] Step 312/312 Loss 3.834 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.582, lat_loss 21.783
09/29 12:06:39 PM | _theta_step_train: [153/180] Final Prec@1 86.5400% Time 266.23
09/29 12:06:49 PM | Valid: [153/180] Step 050/312 Loss 3.643 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.582, lat_loss 21.783
09/29 12:06:59 PM | Valid: [153/180] Step 100/312 Loss 4.010 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.582, lat_loss 21.783
09/29 12:07:08 PM | Valid: [153/180] Step 150/312 Loss 3.900 Prec@(1,3) (86.3%, 99.0%), ce_loss 0.582, lat_loss 21.783
09/29 12:07:18 PM | Valid: [153/180] Step 200/312 Loss 3.972 Prec@(1,3) (86.5%, 99.0%), ce_loss 0.582, lat_loss 21.783
09/29 12:07:28 PM | Valid: [153/180] Step 250/312 Loss 3.817 Prec@(1,3) (86.8%, 99.1%), ce_loss 0.582, lat_loss 21.783
09/29 12:07:38 PM | Valid: [153/180] Step 300/312 Loss 3.741 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.582, lat_loss 21.783
09/29 12:07:40 PM | Valid: [153/180] Step 312/312 Loss 3.735 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.582, lat_loss 21.783
09/29 12:07:40 PM | val: [153/180] Final Prec@1 86.8300% Time 61.43
09/29 12:07:40 PM | Start to train weights for epoch 153
09/29 12:08:33 PM | Train: [154/180] Step 050/1249 Loss 1.683 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.582, lat_loss 21.783
09/29 12:09:17 PM | Train: [154/180] Step 100/1249 Loss 1.719 Prec@(1,3) (93.3%, 100.0%), ce_loss 0.582, lat_loss 21.783
09/29 12:10:08 PM | Train: [154/180] Step 150/1249 Loss 1.623 Prec@(1,3) (93.5%, 100.0%), ce_loss 0.582, lat_loss 21.783
09/29 12:10:57 PM | Train: [154/180] Step 200/1249 Loss 1.532 Prec@(1,3) (93.8%, 100.0%), ce_loss 0.581, lat_loss 21.783
09/29 12:11:47 PM | Train: [154/180] Step 250/1249 Loss 1.526 Prec@(1,3) (93.9%, 100.0%), ce_loss 0.581, lat_loss 21.783
09/29 12:12:38 PM | Train: [154/180] Step 300/1249 Loss 1.508 Prec@(1,3) (94.0%, 100.0%), ce_loss 0.581, lat_loss 21.783
09/29 12:13:26 PM | Train: [154/180] Step 350/1249 Loss 1.482 Prec@(1,3) (94.0%, 100.0%), ce_loss 0.581, lat_loss 21.783
09/29 12:14:15 PM | Train: [154/180] Step 400/1249 Loss 1.483 Prec@(1,3) (94.0%, 100.0%), ce_loss 0.581, lat_loss 21.783
09/29 12:14:58 PM | Train: [154/180] Step 450/1249 Loss 1.500 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:15:47 PM | Train: [154/180] Step 500/1249 Loss 1.525 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:16:36 PM | Train: [154/180] Step 550/1249 Loss 1.527 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:17:24 PM | Train: [154/180] Step 600/1249 Loss 1.527 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:18:14 PM | Train: [154/180] Step 650/1249 Loss 1.536 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:19:03 PM | Train: [154/180] Step 700/1249 Loss 1.549 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:19:52 PM | Train: [154/180] Step 750/1249 Loss 1.538 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:20:34 PM | Train: [154/180] Step 800/1249 Loss 1.543 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:21:23 PM | Train: [154/180] Step 850/1249 Loss 1.528 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.581, lat_loss 21.783
09/29 12:22:11 PM | Train: [154/180] Step 900/1249 Loss 1.575 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.580, lat_loss 21.783
09/29 12:23:00 PM | Train: [154/180] Step 950/1249 Loss 1.567 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.580, lat_loss 21.783
09/29 12:23:49 PM | Train: [154/180] Step 1000/1249 Loss 1.558 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.580, lat_loss 21.783
09/29 12:24:39 PM | Train: [154/180] Step 1050/1249 Loss 1.574 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.580, lat_loss 21.783
09/29 12:25:29 PM | Train: [154/180] Step 1100/1249 Loss 1.560 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.580, lat_loss 21.783
09/29 12:26:14 PM | Train: [154/180] Step 1150/1249 Loss 1.567 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.580, lat_loss 21.782
09/29 12:27:03 PM | Train: [154/180] Step 1200/1249 Loss 1.576 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.580, lat_loss 21.782
09/29 12:27:52 PM | Train: [154/180] Step 1249/1249 Loss 1.588 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.580, lat_loss 21.782
09/29 12:27:53 PM | _w_step_train: [154/180] Final Prec@1 93.8525% Time 1212.61
09/29 12:27:53 PM | Start to train theta for epoch 153
09/29 12:28:37 PM | Train: [154/180] Step 050/312 Loss 4.712 Prec@(1,3) (84.6%, 98.7%), ce_loss 0.580, lat_loss 21.782
09/29 12:29:20 PM | Train: [154/180] Step 100/312 Loss 3.928 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.580, lat_loss 21.782
09/29 12:30:02 PM | Train: [154/180] Step 150/312 Loss 4.005 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.580, lat_loss 21.782
09/29 12:30:44 PM | Train: [154/180] Step 200/312 Loss 3.897 Prec@(1,3) (86.2%, 99.3%), ce_loss 0.580, lat_loss 21.782
09/29 12:31:25 PM | Train: [154/180] Step 250/312 Loss 3.834 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.580, lat_loss 21.782
09/29 12:32:04 PM | Train: [154/180] Step 300/312 Loss 3.835 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.580, lat_loss 21.782
09/29 12:32:14 PM | Train: [154/180] Step 312/312 Loss 3.871 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.580, lat_loss 21.782
09/29 12:32:14 PM | _theta_step_train: [154/180] Final Prec@1 86.4400% Time 261.17
09/29 12:32:24 PM | Valid: [154/180] Step 050/312 Loss 4.178 Prec@(1,3) (85.2%, 99.0%), ce_loss 0.580, lat_loss 21.782
09/29 12:32:34 PM | Valid: [154/180] Step 100/312 Loss 4.664 Prec@(1,3) (84.7%, 98.4%), ce_loss 0.580, lat_loss 21.782
09/29 12:32:44 PM | Valid: [154/180] Step 150/312 Loss 4.335 Prec@(1,3) (85.7%, 98.7%), ce_loss 0.580, lat_loss 21.782
09/29 12:32:53 PM | Valid: [154/180] Step 200/312 Loss 4.126 Prec@(1,3) (86.2%, 98.8%), ce_loss 0.580, lat_loss 21.782
09/29 12:33:03 PM | Valid: [154/180] Step 250/312 Loss 4.039 Prec@(1,3) (86.3%, 98.9%), ce_loss 0.580, lat_loss 21.782
09/29 12:33:13 PM | Valid: [154/180] Step 300/312 Loss 3.969 Prec@(1,3) (86.5%, 99.0%), ce_loss 0.580, lat_loss 21.782
09/29 12:33:15 PM | Valid: [154/180] Step 312/312 Loss 3.953 Prec@(1,3) (86.4%, 99.0%), ce_loss 0.580, lat_loss 21.782
09/29 12:33:15 PM | val: [154/180] Final Prec@1 86.4400% Time 61.37
09/29 12:33:15 PM | Start to train weights for epoch 154
09/29 12:34:07 PM | Train: [155/180] Step 050/1249 Loss 1.675 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.580, lat_loss 21.782
09/29 12:34:58 PM | Train: [155/180] Step 100/1249 Loss 1.618 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.580, lat_loss 21.782
09/29 12:35:48 PM | Train: [155/180] Step 150/1249 Loss 1.629 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:36:37 PM | Train: [155/180] Step 200/1249 Loss 1.674 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:37:20 PM | Train: [155/180] Step 250/1249 Loss 1.692 Prec@(1,3) (93.4%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:38:09 PM | Train: [155/180] Step 300/1249 Loss 1.710 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:38:58 PM | Train: [155/180] Step 350/1249 Loss 1.669 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:39:46 PM | Train: [155/180] Step 400/1249 Loss 1.660 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:40:36 PM | Train: [155/180] Step 450/1249 Loss 1.673 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.579, lat_loss 21.782
09/29 12:41:25 PM | Train: [155/180] Step 500/1249 Loss 1.649 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:42:15 PM | Train: [155/180] Step 550/1249 Loss 1.632 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:43:00 PM | Train: [155/180] Step 600/1249 Loss 1.613 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:43:50 PM | Train: [155/180] Step 650/1249 Loss 1.612 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:44:40 PM | Train: [155/180] Step 700/1249 Loss 1.597 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:45:29 PM | Train: [155/180] Step 750/1249 Loss 1.584 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:46:18 PM | Train: [155/180] Step 800/1249 Loss 1.625 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.579, lat_loss 21.782
09/29 12:47:06 PM | Train: [155/180] Step 850/1249 Loss 1.607 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:47:55 PM | Train: [155/180] Step 900/1249 Loss 1.608 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:48:37 PM | Train: [155/180] Step 950/1249 Loss 1.634 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:49:25 PM | Train: [155/180] Step 1000/1249 Loss 1.629 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:50:14 PM | Train: [155/180] Step 1050/1249 Loss 1.626 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:51:02 PM | Train: [155/180] Step 1100/1249 Loss 1.625 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:51:50 PM | Train: [155/180] Step 1150/1249 Loss 1.624 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:52:39 PM | Train: [155/180] Step 1200/1249 Loss 1.616 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:53:28 PM | Train: [155/180] Step 1249/1249 Loss 1.608 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:53:28 PM | _w_step_train: [155/180] Final Prec@1 93.6775% Time 1212.53
09/29 12:53:28 PM | Start to train theta for epoch 154
09/29 12:54:06 PM | Train: [155/180] Step 050/312 Loss 3.660 Prec@(1,3) (87.7%, 99.4%), ce_loss 0.578, lat_loss 21.782
09/29 12:54:48 PM | Train: [155/180] Step 100/312 Loss 3.783 Prec@(1,3) (87.2%, 99.2%), ce_loss 0.578, lat_loss 21.782
09/29 12:55:32 PM | Train: [155/180] Step 150/312 Loss 3.782 Prec@(1,3) (87.4%, 99.2%), ce_loss 0.578, lat_loss 21.782
09/29 12:56:15 PM | Train: [155/180] Step 200/312 Loss 3.687 Prec@(1,3) (87.5%, 99.2%), ce_loss 0.578, lat_loss 21.782
09/29 12:56:58 PM | Train: [155/180] Step 250/312 Loss 3.656 Prec@(1,3) (87.4%, 99.3%), ce_loss 0.578, lat_loss 21.782
09/29 12:57:40 PM | Train: [155/180] Step 300/312 Loss 3.721 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.578, lat_loss 21.782
09/29 12:57:50 PM | Train: [155/180] Step 312/312 Loss 3.749 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.578, lat_loss 21.782
09/29 12:57:50 PM | _theta_step_train: [155/180] Final Prec@1 87.1800% Time 262.66
09/29 12:58:01 PM | Valid: [155/180] Step 050/312 Loss 3.878 Prec@(1,3) (86.2%, 98.8%), ce_loss 0.578, lat_loss 21.782
09/29 12:58:10 PM | Valid: [155/180] Step 100/312 Loss 4.020 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.578, lat_loss 21.782
09/29 12:58:20 PM | Valid: [155/180] Step 150/312 Loss 3.998 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.578, lat_loss 21.782
09/29 12:58:30 PM | Valid: [155/180] Step 200/312 Loss 4.055 Prec@(1,3) (86.3%, 99.0%), ce_loss 0.578, lat_loss 21.782
09/29 12:58:39 PM | Valid: [155/180] Step 250/312 Loss 3.903 Prec@(1,3) (86.7%, 99.1%), ce_loss 0.578, lat_loss 21.782
09/29 12:58:49 PM | Valid: [155/180] Step 300/312 Loss 3.989 Prec@(1,3) (86.4%, 98.9%), ce_loss 0.578, lat_loss 21.782
09/29 12:58:52 PM | Valid: [155/180] Step 312/312 Loss 4.060 Prec@(1,3) (86.0%, 98.8%), ce_loss 0.578, lat_loss 21.782
09/29 12:58:52 PM | val: [155/180] Final Prec@1 85.9600% Time 61.37
09/29 12:58:52 PM | Start to train weights for epoch 155
09/29 12:59:39 PM | Train: [156/180] Step 050/1249 Loss 1.610 Prec@(1,3) (93.0%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 01:00:27 PM | Train: [156/180] Step 100/1249 Loss 1.467 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.578, lat_loss 21.782
09/29 01:01:16 PM | Train: [156/180] Step 150/1249 Loss 1.436 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.577, lat_loss 21.782
09/29 01:02:05 PM | Train: [156/180] Step 200/1249 Loss 1.390 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.577, lat_loss 21.782
09/29 01:02:54 PM | Train: [156/180] Step 250/1249 Loss 1.433 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.577, lat_loss 21.782
09/29 01:03:43 PM | Train: [156/180] Step 300/1249 Loss 1.467 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.577, lat_loss 21.782
09/29 01:04:32 PM | Train: [156/180] Step 350/1249 Loss 1.490 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.577, lat_loss 21.782
09/29 01:05:15 PM | Train: [156/180] Step 400/1249 Loss 1.561 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:06:04 PM | Train: [156/180] Step 450/1249 Loss 1.583 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:06:53 PM | Train: [156/180] Step 500/1249 Loss 1.560 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:07:41 PM | Train: [156/180] Step 550/1249 Loss 1.554 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:08:30 PM | Train: [156/180] Step 600/1249 Loss 1.551 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:09:18 PM | Train: [156/180] Step 650/1249 Loss 1.551 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:10:07 PM | Train: [156/180] Step 700/1249 Loss 1.540 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:10:51 PM | Train: [156/180] Step 750/1249 Loss 1.543 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:11:41 PM | Train: [156/180] Step 800/1249 Loss 1.571 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.577, lat_loss 21.782
09/29 01:12:30 PM | Train: [156/180] Step 850/1249 Loss 1.559 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.576, lat_loss 21.782
09/29 01:13:19 PM | Train: [156/180] Step 900/1249 Loss 1.551 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.576, lat_loss 21.782
09/29 01:14:07 PM | Train: [156/180] Step 950/1249 Loss 1.538 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.576, lat_loss 21.782
09/29 01:14:56 PM | Train: [156/180] Step 1000/1249 Loss 1.538 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.576, lat_loss 21.782
09/29 01:15:45 PM | Train: [156/180] Step 1050/1249 Loss 1.554 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.576, lat_loss 21.782
09/29 01:16:30 PM | Train: [156/180] Step 1100/1249 Loss 1.565 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.576, lat_loss 21.782
09/29 01:17:18 PM | Train: [156/180] Step 1150/1249 Loss 1.579 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.576, lat_loss 21.782
09/29 01:18:07 PM | Train: [156/180] Step 1200/1249 Loss 1.604 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.576, lat_loss 21.782
09/29 01:18:56 PM | Train: [156/180] Step 1249/1249 Loss 1.605 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.576, lat_loss 21.782
09/29 01:18:56 PM | _w_step_train: [156/180] Final Prec@1 93.7700% Time 1204.21
09/29 01:18:56 PM | Start to train theta for epoch 155
09/29 01:19:41 PM | Train: [156/180] Step 050/312 Loss 5.031 Prec@(1,3) (84.4%, 98.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:20:24 PM | Train: [156/180] Step 100/312 Loss 4.473 Prec@(1,3) (85.2%, 98.7%), ce_loss 0.576, lat_loss 21.782
09/29 01:21:07 PM | Train: [156/180] Step 150/312 Loss 4.199 Prec@(1,3) (85.9%, 99.0%), ce_loss 0.576, lat_loss 21.782
09/29 01:21:49 PM | Train: [156/180] Step 200/312 Loss 4.044 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:22:26 PM | Train: [156/180] Step 250/312 Loss 4.010 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:23:09 PM | Train: [156/180] Step 300/312 Loss 3.972 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:23:19 PM | Train: [156/180] Step 312/312 Loss 3.946 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:23:19 PM | _theta_step_train: [156/180] Final Prec@1 86.4500% Time 262.79
09/29 01:23:29 PM | Valid: [156/180] Step 050/312 Loss 3.395 Prec@(1,3) (88.4%, 99.8%), ce_loss 0.576, lat_loss 21.782
09/29 01:23:39 PM | Valid: [156/180] Step 100/312 Loss 3.572 Prec@(1,3) (87.7%, 99.4%), ce_loss 0.576, lat_loss 21.782
09/29 01:23:49 PM | Valid: [156/180] Step 150/312 Loss 3.593 Prec@(1,3) (87.6%, 99.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:23:58 PM | Valid: [156/180] Step 200/312 Loss 3.675 Prec@(1,3) (87.3%, 99.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:24:08 PM | Valid: [156/180] Step 250/312 Loss 3.665 Prec@(1,3) (87.3%, 99.1%), ce_loss 0.576, lat_loss 21.782
09/29 01:24:18 PM | Valid: [156/180] Step 300/312 Loss 3.590 Prec@(1,3) (87.3%, 99.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:24:20 PM | Valid: [156/180] Step 312/312 Loss 3.586 Prec@(1,3) (87.3%, 99.2%), ce_loss 0.576, lat_loss 21.782
09/29 01:24:20 PM | val: [156/180] Final Prec@1 87.3100% Time 61.32
09/29 01:24:20 PM | Best top1 acc by now. Save model
09/29 01:24:20 PM | Start to train weights for epoch 156
09/29 01:25:12 PM | Train: [157/180] Step 050/1249 Loss 1.519 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.576, lat_loss 21.782
09/29 01:26:01 PM | Train: [157/180] Step 100/1249 Loss 1.565 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:26:49 PM | Train: [157/180] Step 150/1249 Loss 1.530 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:27:35 PM | Train: [157/180] Step 200/1249 Loss 1.518 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:28:20 PM | Train: [157/180] Step 250/1249 Loss 1.540 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:29:09 PM | Train: [157/180] Step 300/1249 Loss 1.545 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:29:58 PM | Train: [157/180] Step 350/1249 Loss 1.573 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:30:46 PM | Train: [157/180] Step 400/1249 Loss 1.544 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:31:35 PM | Train: [157/180] Step 450/1249 Loss 1.559 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:32:24 PM | Train: [157/180] Step 500/1249 Loss 1.553 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:33:11 PM | Train: [157/180] Step 550/1249 Loss 1.555 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:33:58 PM | Train: [157/180] Step 600/1249 Loss 1.554 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:34:47 PM | Train: [157/180] Step 650/1249 Loss 1.559 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:35:36 PM | Train: [157/180] Step 700/1249 Loss 1.551 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:36:25 PM | Train: [157/180] Step 750/1249 Loss 1.585 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:37:14 PM | Train: [157/180] Step 800/1249 Loss 1.573 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.575, lat_loss 21.782
09/29 01:38:03 PM | Train: [157/180] Step 850/1249 Loss 1.582 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.574, lat_loss 21.782
09/29 01:38:48 PM | Train: [157/180] Step 900/1249 Loss 1.577 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.574, lat_loss 21.782
09/29 01:39:36 PM | Train: [157/180] Step 950/1249 Loss 1.570 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.574, lat_loss 21.782
09/29 01:40:25 PM | Train: [157/180] Step 1000/1249 Loss 1.568 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.574, lat_loss 21.782
09/29 01:41:14 PM | Train: [157/180] Step 1050/1249 Loss 1.568 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.574, lat_loss 21.782
09/29 01:42:02 PM | Train: [157/180] Step 1100/1249 Loss 1.563 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.574, lat_loss 21.782
09/29 01:42:51 PM | Train: [157/180] Step 1150/1249 Loss 1.556 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.574, lat_loss 21.782
09/29 01:43:42 PM | Train: [157/180] Step 1200/1249 Loss 1.557 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.574, lat_loss 21.781
09/29 01:44:26 PM | Train: [157/180] Step 1249/1249 Loss 1.551 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.574, lat_loss 21.781
09/29 01:44:26 PM | _w_step_train: [157/180] Final Prec@1 93.9500% Time 1205.38
09/29 01:44:26 PM | Start to train theta for epoch 156
09/29 01:45:11 PM | Train: [157/180] Step 050/312 Loss 3.528 Prec@(1,3) (87.9%, 99.6%), ce_loss 0.574, lat_loss 21.781
09/29 01:45:54 PM | Train: [157/180] Step 100/312 Loss 3.730 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.574, lat_loss 21.781
09/29 01:46:38 PM | Train: [157/180] Step 150/312 Loss 3.707 Prec@(1,3) (87.5%, 99.4%), ce_loss 0.574, lat_loss 21.781
09/29 01:47:21 PM | Train: [157/180] Step 200/312 Loss 3.750 Prec@(1,3) (87.3%, 99.5%), ce_loss 0.574, lat_loss 21.781
09/29 01:48:03 PM | Train: [157/180] Step 250/312 Loss 3.790 Prec@(1,3) (87.1%, 99.5%), ce_loss 0.574, lat_loss 21.781
09/29 01:48:46 PM | Train: [157/180] Step 300/312 Loss 3.852 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.574, lat_loss 21.781
09/29 01:48:56 PM | Train: [157/180] Step 312/312 Loss 3.860 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.574, lat_loss 21.781
09/29 01:48:56 PM | _theta_step_train: [157/180] Final Prec@1 87.0000% Time 270.03
09/29 01:49:06 PM | Valid: [157/180] Step 050/312 Loss 3.502 Prec@(1,3) (87.1%, 99.5%), ce_loss 0.574, lat_loss 21.781
09/29 01:49:16 PM | Valid: [157/180] Step 100/312 Loss 3.765 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.574, lat_loss 21.781
09/29 01:49:26 PM | Valid: [157/180] Step 150/312 Loss 3.877 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.574, lat_loss 21.781
09/29 01:49:35 PM | Valid: [157/180] Step 200/312 Loss 3.794 Prec@(1,3) (86.7%, 99.1%), ce_loss 0.574, lat_loss 21.781
09/29 01:49:45 PM | Valid: [157/180] Step 250/312 Loss 3.779 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.574, lat_loss 21.781
09/29 01:49:54 PM | Valid: [157/180] Step 300/312 Loss 3.851 Prec@(1,3) (86.3%, 99.0%), ce_loss 0.574, lat_loss 21.781
09/29 01:49:55 PM | Valid: [157/180] Step 312/312 Loss 3.839 Prec@(1,3) (86.3%, 99.0%), ce_loss 0.574, lat_loss 21.781
09/29 01:49:56 PM | val: [157/180] Final Prec@1 86.2900% Time 59.77
09/29 01:49:56 PM | Start to train weights for epoch 157
09/29 01:50:43 PM | Train: [158/180] Step 050/1249 Loss 1.440 Prec@(1,3) (94.0%, 100.0%), ce_loss 0.574, lat_loss 21.781
09/29 01:51:32 PM | Train: [158/180] Step 100/1249 Loss 1.373 Prec@(1,3) (94.3%, 100.0%), ce_loss 0.573, lat_loss 21.781
09/29 01:52:20 PM | Train: [158/180] Step 150/1249 Loss 1.378 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.573, lat_loss 21.781
09/29 01:53:09 PM | Train: [158/180] Step 200/1249 Loss 1.721 Prec@(1,3) (93.6%, 99.7%), ce_loss 0.573, lat_loss 21.781
09/29 01:53:58 PM | Train: [158/180] Step 250/1249 Loss 1.709 Prec@(1,3) (93.5%, 99.7%), ce_loss 0.573, lat_loss 21.781
09/29 01:54:47 PM | Train: [158/180] Step 300/1249 Loss 1.642 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 01:55:33 PM | Train: [158/180] Step 350/1249 Loss 1.610 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 01:56:19 PM | Train: [158/180] Step 400/1249 Loss 1.628 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 01:57:08 PM | Train: [158/180] Step 450/1249 Loss 1.732 Prec@(1,3) (93.3%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 01:57:57 PM | Train: [158/180] Step 500/1249 Loss 1.718 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 01:58:45 PM | Train: [158/180] Step 550/1249 Loss 1.691 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 01:59:34 PM | Train: [158/180] Step 600/1249 Loss 1.675 Prec@(1,3) (93.5%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 02:00:22 PM | Train: [158/180] Step 650/1249 Loss 1.654 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 02:01:08 PM | Train: [158/180] Step 700/1249 Loss 1.662 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 02:01:56 PM | Train: [158/180] Step 750/1249 Loss 1.668 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 02:02:44 PM | Train: [158/180] Step 800/1249 Loss 1.664 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.573, lat_loss 21.781
09/29 02:03:33 PM | Train: [158/180] Step 850/1249 Loss 1.657 Prec@(1,3) (93.6%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:04:21 PM | Train: [158/180] Step 900/1249 Loss 1.644 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:05:10 PM | Train: [158/180] Step 950/1249 Loss 1.649 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:05:58 PM | Train: [158/180] Step 1000/1249 Loss 1.645 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:06:43 PM | Train: [158/180] Step 1050/1249 Loss 1.634 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:07:29 PM | Train: [158/180] Step 1100/1249 Loss 1.637 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:08:18 PM | Train: [158/180] Step 1150/1249 Loss 1.631 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:09:07 PM | Train: [158/180] Step 1200/1249 Loss 1.619 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:09:54 PM | Train: [158/180] Step 1249/1249 Loss 1.619 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:09:55 PM | _w_step_train: [158/180] Final Prec@1 93.6800% Time 1199.06
09/29 02:09:55 PM | Start to train theta for epoch 157
09/29 02:10:39 PM | Train: [158/180] Step 050/312 Loss 4.187 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.572, lat_loss 21.781
09/29 02:11:21 PM | Train: [158/180] Step 100/312 Loss 4.283 Prec@(1,3) (86.1%, 99.3%), ce_loss 0.572, lat_loss 21.781
09/29 02:12:04 PM | Train: [158/180] Step 150/312 Loss 4.415 Prec@(1,3) (85.8%, 98.9%), ce_loss 0.572, lat_loss 21.781
09/29 02:12:41 PM | Train: [158/180] Step 200/312 Loss 4.160 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.572, lat_loss 21.781
09/29 02:13:23 PM | Train: [158/180] Step 250/312 Loss 4.110 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.572, lat_loss 21.781
09/29 02:14:06 PM | Train: [158/180] Step 300/312 Loss 4.113 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.572, lat_loss 21.781
09/29 02:14:16 PM | Train: [158/180] Step 312/312 Loss 4.107 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.572, lat_loss 21.781
09/29 02:14:16 PM | _theta_step_train: [158/180] Final Prec@1 86.0800% Time 261.06
09/29 02:14:26 PM | Valid: [158/180] Step 050/312 Loss 4.001 Prec@(1,3) (85.7%, 99.1%), ce_loss 0.572, lat_loss 21.781
09/29 02:14:36 PM | Valid: [158/180] Step 100/312 Loss 3.980 Prec@(1,3) (86.0%, 99.3%), ce_loss 0.572, lat_loss 21.781
09/29 02:14:46 PM | Valid: [158/180] Step 150/312 Loss 3.890 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.572, lat_loss 21.781
09/29 02:14:55 PM | Valid: [158/180] Step 200/312 Loss 3.781 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.572, lat_loss 21.781
09/29 02:15:05 PM | Valid: [158/180] Step 250/312 Loss 3.755 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.572, lat_loss 21.781
09/29 02:15:15 PM | Valid: [158/180] Step 300/312 Loss 3.711 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.572, lat_loss 21.781
09/29 02:15:17 PM | Valid: [158/180] Step 312/312 Loss 3.707 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.572, lat_loss 21.781
09/29 02:15:17 PM | val: [158/180] Final Prec@1 86.7400% Time 61.29
09/29 02:15:17 PM | Start to train weights for epoch 158
09/29 02:16:09 PM | Train: [159/180] Step 050/1249 Loss 1.537 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.572, lat_loss 21.781
09/29 02:16:59 PM | Train: [159/180] Step 100/1249 Loss 1.575 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.572, lat_loss 21.781
09/29 02:17:49 PM | Train: [159/180] Step 150/1249 Loss 1.613 Prec@(1,3) (93.6%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:18:22 PM | Train: [159/180] Step 200/1249 Loss 1.593 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:18:49 PM | Train: [159/180] Step 250/1249 Loss 1.544 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:19:16 PM | Train: [159/180] Step 300/1249 Loss 1.511 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:19:44 PM | Train: [159/180] Step 350/1249 Loss 1.542 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:20:11 PM | Train: [159/180] Step 400/1249 Loss 1.536 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:20:38 PM | Train: [159/180] Step 450/1249 Loss 1.532 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:21:05 PM | Train: [159/180] Step 500/1249 Loss 1.509 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:21:32 PM | Train: [159/180] Step 550/1249 Loss 1.521 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.571, lat_loss 21.781
09/29 02:21:59 PM | Train: [159/180] Step 600/1249 Loss 1.538 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.571, lat_loss 21.781
09/29 02:22:26 PM | Train: [159/180] Step 650/1249 Loss 1.583 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.571, lat_loss 21.781
09/29 02:22:50 PM | Train: [159/180] Step 700/1249 Loss 1.635 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.571, lat_loss 21.781
09/29 02:23:12 PM | Train: [159/180] Step 750/1249 Loss 1.636 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.571, lat_loss 21.781
09/29 02:23:32 PM | Train: [159/180] Step 800/1249 Loss 1.611 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.571, lat_loss 21.781
09/29 02:23:53 PM | Train: [159/180] Step 850/1249 Loss 1.627 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.571, lat_loss 21.781
09/29 02:24:13 PM | Train: [159/180] Step 900/1249 Loss 1.634 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:24:33 PM | Train: [159/180] Step 950/1249 Loss 1.629 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:24:53 PM | Train: [159/180] Step 1000/1249 Loss 1.627 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:25:13 PM | Train: [159/180] Step 1050/1249 Loss 1.613 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:25:33 PM | Train: [159/180] Step 1100/1249 Loss 1.628 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:25:54 PM | Train: [159/180] Step 1150/1249 Loss 1.634 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:26:14 PM | Train: [159/180] Step 1200/1249 Loss 1.629 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:26:33 PM | Train: [159/180] Step 1249/1249 Loss 1.628 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:26:33 PM | _w_step_train: [159/180] Final Prec@1 93.7475% Time 676.37
09/29 02:26:33 PM | Start to train theta for epoch 158
09/29 02:26:55 PM | Train: [159/180] Step 050/312 Loss 3.573 Prec@(1,3) (87.9%, 99.3%), ce_loss 0.570, lat_loss 21.781
09/29 02:27:17 PM | Train: [159/180] Step 100/312 Loss 3.575 Prec@(1,3) (87.7%, 99.3%), ce_loss 0.570, lat_loss 21.781
09/29 02:27:38 PM | Train: [159/180] Step 150/312 Loss 3.645 Prec@(1,3) (87.5%, 99.3%), ce_loss 0.570, lat_loss 21.781
09/29 02:27:59 PM | Train: [159/180] Step 200/312 Loss 3.586 Prec@(1,3) (87.6%, 99.4%), ce_loss 0.570, lat_loss 21.781
09/29 02:28:17 PM | Train: [159/180] Step 250/312 Loss 3.660 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.570, lat_loss 21.781
09/29 02:28:34 PM | Train: [159/180] Step 300/312 Loss 3.687 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.570, lat_loss 21.781
09/29 02:28:37 PM | Train: [159/180] Step 312/312 Loss 3.721 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.570, lat_loss 21.781
09/29 02:28:38 PM | _theta_step_train: [159/180] Final Prec@1 87.3200% Time 124.13
09/29 02:28:43 PM | Valid: [159/180] Step 050/312 Loss 4.074 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.570, lat_loss 21.781
09/29 02:28:47 PM | Valid: [159/180] Step 100/312 Loss 4.061 Prec@(1,3) (86.3%, 99.1%), ce_loss 0.570, lat_loss 21.781
09/29 02:28:52 PM | Valid: [159/180] Step 150/312 Loss 4.019 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.570, lat_loss 21.781
09/29 02:28:57 PM | Valid: [159/180] Step 200/312 Loss 3.876 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.570, lat_loss 21.781
09/29 02:29:01 PM | Valid: [159/180] Step 250/312 Loss 3.829 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.570, lat_loss 21.781
09/29 02:29:06 PM | Valid: [159/180] Step 300/312 Loss 3.777 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.570, lat_loss 21.781
09/29 02:29:07 PM | Valid: [159/180] Step 312/312 Loss 3.781 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.570, lat_loss 21.781
09/29 02:29:07 PM | val: [159/180] Final Prec@1 86.7600% Time 29.64
09/29 02:29:07 PM | Start to train weights for epoch 159
09/29 02:29:33 PM | Train: [160/180] Step 050/1249 Loss 1.458 Prec@(1,3) (94.2%, 100.0%), ce_loss 0.570, lat_loss 21.781
09/29 02:29:57 PM | Train: [160/180] Step 100/1249 Loss 1.520 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.570, lat_loss 21.781
09/29 02:30:20 PM | Train: [160/180] Step 150/1249 Loss 1.573 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.570, lat_loss 21.781
09/29 02:30:44 PM | Train: [160/180] Step 200/1249 Loss 1.682 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:31:08 PM | Train: [160/180] Step 250/1249 Loss 1.600 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:31:32 PM | Train: [160/180] Step 300/1249 Loss 1.567 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:31:56 PM | Train: [160/180] Step 350/1249 Loss 1.549 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:32:20 PM | Train: [160/180] Step 400/1249 Loss 1.504 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:32:44 PM | Train: [160/180] Step 450/1249 Loss 1.533 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.569, lat_loss 21.781
09/29 02:33:08 PM | Train: [160/180] Step 500/1249 Loss 1.531 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:33:31 PM | Train: [160/180] Step 550/1249 Loss 1.562 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:33:55 PM | Train: [160/180] Step 600/1249 Loss 1.546 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:34:18 PM | Train: [160/180] Step 650/1249 Loss 1.570 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.569, lat_loss 21.781
09/29 02:34:41 PM | Train: [160/180] Step 700/1249 Loss 1.570 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.569, lat_loss 21.781
09/29 02:35:04 PM | Train: [160/180] Step 750/1249 Loss 1.573 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.569, lat_loss 21.781
09/29 02:35:28 PM | Train: [160/180] Step 800/1249 Loss 1.572 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.569, lat_loss 21.781
09/29 02:35:50 PM | Train: [160/180] Step 850/1249 Loss 1.564 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:36:12 PM | Train: [160/180] Step 900/1249 Loss 1.545 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.569, lat_loss 21.781
09/29 02:36:36 PM | Train: [160/180] Step 950/1249 Loss 1.563 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:37:00 PM | Train: [160/180] Step 1000/1249 Loss 1.565 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:37:24 PM | Train: [160/180] Step 1050/1249 Loss 1.550 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:37:48 PM | Train: [160/180] Step 1100/1249 Loss 1.537 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:38:11 PM | Train: [160/180] Step 1150/1249 Loss 1.531 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:38:36 PM | Train: [160/180] Step 1200/1249 Loss 1.521 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:39:00 PM | Train: [160/180] Step 1249/1249 Loss 1.515 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:39:00 PM | _w_step_train: [160/180] Final Prec@1 94.0475% Time 592.85
09/29 02:39:00 PM | Start to train theta for epoch 159
09/29 02:39:21 PM | Train: [160/180] Step 050/312 Loss 3.809 Prec@(1,3) (86.1%, 99.7%), ce_loss 0.568, lat_loss 21.781
09/29 02:39:42 PM | Train: [160/180] Step 100/312 Loss 3.842 Prec@(1,3) (86.4%, 99.7%), ce_loss 0.568, lat_loss 21.781
09/29 02:40:02 PM | Train: [160/180] Step 150/312 Loss 3.670 Prec@(1,3) (86.9%, 99.6%), ce_loss 0.568, lat_loss 21.781
09/29 02:40:23 PM | Train: [160/180] Step 200/312 Loss 3.647 Prec@(1,3) (87.4%, 99.5%), ce_loss 0.568, lat_loss 21.781
09/29 02:40:43 PM | Train: [160/180] Step 250/312 Loss 3.781 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:03 PM | Train: [160/180] Step 300/312 Loss 3.884 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:08 PM | Train: [160/180] Step 312/312 Loss 3.864 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:08 PM | _theta_step_train: [160/180] Final Prec@1 86.8000% Time 128.35
09/29 02:41:14 PM | Valid: [160/180] Step 050/312 Loss 3.383 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:18 PM | Valid: [160/180] Step 100/312 Loss 3.794 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:23 PM | Valid: [160/180] Step 150/312 Loss 3.679 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:28 PM | Valid: [160/180] Step 200/312 Loss 3.556 Prec@(1,3) (87.3%, 99.3%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:32 PM | Valid: [160/180] Step 250/312 Loss 3.538 Prec@(1,3) (87.4%, 99.3%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:37 PM | Valid: [160/180] Step 300/312 Loss 3.583 Prec@(1,3) (87.2%, 99.2%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:38 PM | Valid: [160/180] Step 312/312 Loss 3.569 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.568, lat_loss 21.781
09/29 02:41:38 PM | val: [160/180] Final Prec@1 87.1500% Time 29.51
09/29 02:41:38 PM | Start to train weights for epoch 160
09/29 02:42:02 PM | Train: [161/180] Step 050/1249 Loss 1.487 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:42:25 PM | Train: [161/180] Step 100/1249 Loss 1.393 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:42:48 PM | Train: [161/180] Step 150/1249 Loss 1.370 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.568, lat_loss 21.781
09/29 02:43:11 PM | Train: [161/180] Step 200/1249 Loss 1.361 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:43:35 PM | Train: [161/180] Step 250/1249 Loss 1.342 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:43:58 PM | Train: [161/180] Step 300/1249 Loss 1.370 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:44:22 PM | Train: [161/180] Step 350/1249 Loss 1.360 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:44:44 PM | Train: [161/180] Step 400/1249 Loss 1.378 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:45:07 PM | Train: [161/180] Step 450/1249 Loss 1.391 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:45:30 PM | Train: [161/180] Step 500/1249 Loss 1.414 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:45:53 PM | Train: [161/180] Step 550/1249 Loss 1.405 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:46:16 PM | Train: [161/180] Step 600/1249 Loss 1.395 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:46:38 PM | Train: [161/180] Step 650/1249 Loss 1.445 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:47:01 PM | Train: [161/180] Step 700/1249 Loss 1.468 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:47:24 PM | Train: [161/180] Step 750/1249 Loss 1.469 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:47:48 PM | Train: [161/180] Step 800/1249 Loss 1.461 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:48:11 PM | Train: [161/180] Step 850/1249 Loss 1.450 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:48:35 PM | Train: [161/180] Step 900/1249 Loss 1.468 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.567, lat_loss 21.781
09/29 02:48:59 PM | Train: [161/180] Step 950/1249 Loss 1.497 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.566, lat_loss 21.781
09/29 02:49:24 PM | Train: [161/180] Step 1000/1249 Loss 1.505 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.566, lat_loss 21.781
09/29 02:49:48 PM | Train: [161/180] Step 1050/1249 Loss 1.502 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.566, lat_loss 21.781
09/29 02:50:12 PM | Train: [161/180] Step 1100/1249 Loss 1.498 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.566, lat_loss 21.781
09/29 02:50:36 PM | Train: [161/180] Step 1150/1249 Loss 1.489 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.566, lat_loss 21.780
09/29 02:51:00 PM | Train: [161/180] Step 1200/1249 Loss 1.506 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.566, lat_loss 21.780
09/29 02:51:25 PM | Train: [161/180] Step 1249/1249 Loss 1.500 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.566, lat_loss 21.780
09/29 02:51:25 PM | _w_step_train: [161/180] Final Prec@1 94.2250% Time 586.79
09/29 02:51:25 PM | Start to train theta for epoch 160
09/29 02:51:47 PM | Train: [161/180] Step 050/312 Loss 4.476 Prec@(1,3) (85.5%, 99.4%), ce_loss 0.566, lat_loss 21.780
09/29 02:52:08 PM | Train: [161/180] Step 100/312 Loss 4.084 Prec@(1,3) (86.4%, 99.5%), ce_loss 0.566, lat_loss 21.780
09/29 02:52:29 PM | Train: [161/180] Step 150/312 Loss 3.940 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.566, lat_loss 21.780
09/29 02:52:49 PM | Train: [161/180] Step 200/312 Loss 3.984 Prec@(1,3) (86.6%, 99.3%), ce_loss 0.566, lat_loss 21.780
09/29 02:53:10 PM | Train: [161/180] Step 250/312 Loss 3.882 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.566, lat_loss 21.780
09/29 02:53:32 PM | Train: [161/180] Step 300/312 Loss 3.887 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.566, lat_loss 21.780
09/29 02:53:37 PM | Train: [161/180] Step 312/312 Loss 3.892 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.566, lat_loss 21.780
09/29 02:53:37 PM | _theta_step_train: [161/180] Final Prec@1 86.9000% Time 132.57
09/29 02:53:43 PM | Valid: [161/180] Step 050/312 Loss 3.377 Prec@(1,3) (88.1%, 99.5%), ce_loss 0.566, lat_loss 21.780
09/29 02:53:47 PM | Valid: [161/180] Step 100/312 Loss 4.067 Prec@(1,3) (86.8%, 99.1%), ce_loss 0.566, lat_loss 21.780
09/29 02:53:52 PM | Valid: [161/180] Step 150/312 Loss 3.999 Prec@(1,3) (86.8%, 99.0%), ce_loss 0.566, lat_loss 21.780
09/29 02:53:57 PM | Valid: [161/180] Step 200/312 Loss 3.921 Prec@(1,3) (87.0%, 99.0%), ce_loss 0.566, lat_loss 21.780
09/29 02:54:01 PM | Valid: [161/180] Step 250/312 Loss 3.797 Prec@(1,3) (87.4%, 99.1%), ce_loss 0.566, lat_loss 21.780
09/29 02:54:06 PM | Valid: [161/180] Step 300/312 Loss 3.765 Prec@(1,3) (87.2%, 99.1%), ce_loss 0.566, lat_loss 21.780
09/29 02:54:07 PM | Valid: [161/180] Step 312/312 Loss 3.754 Prec@(1,3) (87.2%, 99.2%), ce_loss 0.566, lat_loss 21.780
09/29 02:54:07 PM | val: [161/180] Final Prec@1 87.2100% Time 29.76
09/29 02:54:07 PM | Start to train weights for epoch 161
09/29 02:54:33 PM | Train: [162/180] Step 050/1249 Loss 1.837 Prec@(1,3) (93.3%, 99.6%), ce_loss 0.566, lat_loss 21.780
09/29 02:54:58 PM | Train: [162/180] Step 100/1249 Loss 1.815 Prec@(1,3) (93.2%, 99.7%), ce_loss 0.566, lat_loss 21.780
09/29 02:55:19 PM | Train: [162/180] Step 150/1249 Loss 1.660 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.566, lat_loss 21.780
09/29 02:55:41 PM | Train: [162/180] Step 200/1249 Loss 1.578 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.566, lat_loss 21.780
09/29 02:56:05 PM | Train: [162/180] Step 250/1249 Loss 1.581 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 02:56:30 PM | Train: [162/180] Step 300/1249 Loss 1.542 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.565, lat_loss 21.780
09/29 02:56:54 PM | Train: [162/180] Step 350/1249 Loss 1.480 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.565, lat_loss 21.780
09/29 02:57:19 PM | Train: [162/180] Step 400/1249 Loss 1.533 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 02:57:43 PM | Train: [162/180] Step 450/1249 Loss 1.502 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 02:58:07 PM | Train: [162/180] Step 500/1249 Loss 1.467 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.565, lat_loss 21.780
09/29 02:58:31 PM | Train: [162/180] Step 550/1249 Loss 1.528 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 02:58:56 PM | Train: [162/180] Step 600/1249 Loss 1.526 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 02:59:20 PM | Train: [162/180] Step 650/1249 Loss 1.508 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 02:59:45 PM | Train: [162/180] Step 700/1249 Loss 1.514 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 03:00:08 PM | Train: [162/180] Step 750/1249 Loss 1.525 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 03:00:33 PM | Train: [162/180] Step 800/1249 Loss 1.514 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 03:00:57 PM | Train: [162/180] Step 850/1249 Loss 1.501 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.565, lat_loss 21.780
09/29 03:01:22 PM | Train: [162/180] Step 900/1249 Loss 1.499 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.565, lat_loss 21.780
09/29 03:01:46 PM | Train: [162/180] Step 950/1249 Loss 1.489 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.565, lat_loss 21.780
09/29 03:02:11 PM | Train: [162/180] Step 1000/1249 Loss 1.480 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.564, lat_loss 21.780
09/29 03:02:36 PM | Train: [162/180] Step 1050/1249 Loss 1.478 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.564, lat_loss 21.780
09/29 03:03:01 PM | Train: [162/180] Step 1100/1249 Loss 1.475 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.564, lat_loss 21.780
09/29 03:03:26 PM | Train: [162/180] Step 1150/1249 Loss 1.470 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.564, lat_loss 21.780
09/29 03:03:50 PM | Train: [162/180] Step 1200/1249 Loss 1.471 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.564, lat_loss 21.780
09/29 03:04:15 PM | Train: [162/180] Step 1249/1249 Loss 1.472 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.564, lat_loss 21.780
09/29 03:04:15 PM | _w_step_train: [162/180] Final Prec@1 94.2375% Time 607.67
09/29 03:04:15 PM | Start to train theta for epoch 161
09/29 03:04:35 PM | Train: [162/180] Step 050/312 Loss 4.019 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.564, lat_loss 21.780
09/29 03:04:55 PM | Train: [162/180] Step 100/312 Loss 3.989 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.564, lat_loss 21.780
09/29 03:05:14 PM | Train: [162/180] Step 150/312 Loss 4.098 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.564, lat_loss 21.780
09/29 03:05:33 PM | Train: [162/180] Step 200/312 Loss 4.050 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.564, lat_loss 21.780
09/29 03:05:53 PM | Train: [162/180] Step 250/312 Loss 3.919 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:12 PM | Train: [162/180] Step 300/312 Loss 3.946 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:17 PM | Train: [162/180] Step 312/312 Loss 3.945 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:17 PM | _theta_step_train: [162/180] Final Prec@1 86.4100% Time 122.16
09/29 03:06:22 PM | Valid: [162/180] Step 050/312 Loss 3.500 Prec@(1,3) (87.1%, 99.6%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:27 PM | Valid: [162/180] Step 100/312 Loss 3.725 Prec@(1,3) (87.0%, 99.5%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:31 PM | Valid: [162/180] Step 150/312 Loss 3.702 Prec@(1,3) (87.2%, 99.3%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:36 PM | Valid: [162/180] Step 200/312 Loss 3.718 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:41 PM | Valid: [162/180] Step 250/312 Loss 3.698 Prec@(1,3) (87.1%, 99.2%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:45 PM | Valid: [162/180] Step 300/312 Loss 3.644 Prec@(1,3) (87.1%, 99.2%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:46 PM | Valid: [162/180] Step 312/312 Loss 3.632 Prec@(1,3) (87.1%, 99.2%), ce_loss 0.564, lat_loss 21.780
09/29 03:06:46 PM | val: [162/180] Final Prec@1 87.1100% Time 29.39
09/29 03:06:46 PM | Start to train weights for epoch 162
09/29 03:07:12 PM | Train: [163/180] Step 050/1249 Loss 1.337 Prec@(1,3) (94.9%, 100.0%), ce_loss 0.564, lat_loss 21.780
09/29 03:07:33 PM | Train: [163/180] Step 100/1249 Loss 1.409 Prec@(1,3) (94.4%, 100.0%), ce_loss 0.564, lat_loss 21.780
09/29 03:07:55 PM | Train: [163/180] Step 150/1249 Loss 1.368 Prec@(1,3) (94.6%, 100.0%), ce_loss 0.564, lat_loss 21.780
09/29 03:08:17 PM | Train: [163/180] Step 200/1249 Loss 1.673 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.564, lat_loss 21.780
09/29 03:08:38 PM | Train: [163/180] Step 250/1249 Loss 1.636 Prec@(1,3) (93.8%, 99.8%), ce_loss 0.564, lat_loss 21.780
09/29 03:09:02 PM | Train: [163/180] Step 300/1249 Loss 1.594 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:09:27 PM | Train: [163/180] Step 350/1249 Loss 1.618 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.563, lat_loss 21.780
09/29 03:09:51 PM | Train: [163/180] Step 400/1249 Loss 1.601 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:10:16 PM | Train: [163/180] Step 450/1249 Loss 1.587 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:10:41 PM | Train: [163/180] Step 500/1249 Loss 1.574 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:11:05 PM | Train: [163/180] Step 550/1249 Loss 1.569 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:11:30 PM | Train: [163/180] Step 600/1249 Loss 1.553 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:11:54 PM | Train: [163/180] Step 650/1249 Loss 1.524 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:12:19 PM | Train: [163/180] Step 700/1249 Loss 1.527 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:12:43 PM | Train: [163/180] Step 750/1249 Loss 1.503 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:13:08 PM | Train: [163/180] Step 800/1249 Loss 1.506 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:13:32 PM | Train: [163/180] Step 850/1249 Loss 1.521 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:13:57 PM | Train: [163/180] Step 900/1249 Loss 1.524 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:14:22 PM | Train: [163/180] Step 950/1249 Loss 1.514 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:14:45 PM | Train: [163/180] Step 1000/1249 Loss 1.520 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.563, lat_loss 21.780
09/29 03:15:07 PM | Train: [163/180] Step 1050/1249 Loss 1.531 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.562, lat_loss 21.780
09/29 03:15:29 PM | Train: [163/180] Step 1100/1249 Loss 1.517 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.562, lat_loss 21.780
09/29 03:15:51 PM | Train: [163/180] Step 1150/1249 Loss 1.507 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.562, lat_loss 21.780
09/29 03:16:13 PM | Train: [163/180] Step 1200/1249 Loss 1.509 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.562, lat_loss 21.780
09/29 03:16:37 PM | Train: [163/180] Step 1249/1249 Loss 1.516 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.562, lat_loss 21.780
09/29 03:16:38 PM | _w_step_train: [163/180] Final Prec@1 94.0325% Time 591.17
09/29 03:16:38 PM | Start to train theta for epoch 162
09/29 03:16:59 PM | Train: [163/180] Step 050/312 Loss 3.925 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.562, lat_loss 21.780
09/29 03:17:19 PM | Train: [163/180] Step 100/312 Loss 4.086 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.562, lat_loss 21.780
09/29 03:17:40 PM | Train: [163/180] Step 150/312 Loss 4.011 Prec@(1,3) (86.4%, 99.2%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:01 PM | Train: [163/180] Step 200/312 Loss 4.004 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:15 PM | Train: [163/180] Step 250/312 Loss 3.969 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:27 PM | Train: [163/180] Step 300/312 Loss 3.908 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:30 PM | Train: [163/180] Step 312/312 Loss 3.864 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:30 PM | _theta_step_train: [163/180] Final Prec@1 86.7600% Time 112.61
09/29 03:18:35 PM | Valid: [163/180] Step 050/312 Loss 3.398 Prec@(1,3) (88.2%, 99.7%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:40 PM | Valid: [163/180] Step 100/312 Loss 3.913 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:45 PM | Valid: [163/180] Step 150/312 Loss 3.841 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:49 PM | Valid: [163/180] Step 200/312 Loss 3.788 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:54 PM | Valid: [163/180] Step 250/312 Loss 3.730 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.562, lat_loss 21.780
09/29 03:18:58 PM | Valid: [163/180] Step 300/312 Loss 3.671 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.562, lat_loss 21.780
09/29 03:19:00 PM | Valid: [163/180] Step 312/312 Loss 3.673 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.562, lat_loss 21.780
09/29 03:19:00 PM | val: [163/180] Final Prec@1 87.1900% Time 29.64
09/29 03:19:00 PM | Start to train weights for epoch 163
09/29 03:19:26 PM | Train: [164/180] Step 050/1249 Loss 2.554 Prec@(1,3) (91.1%, 99.3%), ce_loss 0.562, lat_loss 21.780
09/29 03:19:51 PM | Train: [164/180] Step 100/1249 Loss 2.246 Prec@(1,3) (91.9%, 99.5%), ce_loss 0.562, lat_loss 21.780
09/29 03:20:16 PM | Train: [164/180] Step 150/1249 Loss 1.885 Prec@(1,3) (93.0%, 99.6%), ce_loss 0.562, lat_loss 21.780
09/29 03:20:42 PM | Train: [164/180] Step 200/1249 Loss 1.821 Prec@(1,3) (93.2%, 99.6%), ce_loss 0.562, lat_loss 21.780
09/29 03:21:07 PM | Train: [164/180] Step 250/1249 Loss 1.734 Prec@(1,3) (93.5%, 99.7%), ce_loss 0.562, lat_loss 21.780
09/29 03:21:33 PM | Train: [164/180] Step 300/1249 Loss 1.697 Prec@(1,3) (93.6%, 99.7%), ce_loss 0.562, lat_loss 21.780
09/29 03:21:58 PM | Train: [164/180] Step 350/1249 Loss 1.621 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.562, lat_loss 21.780
09/29 03:22:23 PM | Train: [164/180] Step 400/1249 Loss 1.631 Prec@(1,3) (93.9%, 99.7%), ce_loss 0.561, lat_loss 21.780
09/29 03:22:48 PM | Train: [164/180] Step 450/1249 Loss 1.597 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:23:13 PM | Train: [164/180] Step 500/1249 Loss 1.621 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:23:39 PM | Train: [164/180] Step 550/1249 Loss 1.592 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:24:04 PM | Train: [164/180] Step 600/1249 Loss 1.558 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:24:29 PM | Train: [164/180] Step 650/1249 Loss 1.540 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:24:54 PM | Train: [164/180] Step 700/1249 Loss 1.558 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:25:19 PM | Train: [164/180] Step 750/1249 Loss 1.576 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:25:44 PM | Train: [164/180] Step 800/1249 Loss 1.553 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:26:09 PM | Train: [164/180] Step 850/1249 Loss 1.542 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:26:34 PM | Train: [164/180] Step 900/1249 Loss 1.540 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:26:59 PM | Train: [164/180] Step 950/1249 Loss 1.539 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:27:25 PM | Train: [164/180] Step 1000/1249 Loss 1.573 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:27:50 PM | Train: [164/180] Step 1050/1249 Loss 1.614 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:28:16 PM | Train: [164/180] Step 1100/1249 Loss 1.629 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.561, lat_loss 21.780
09/29 03:28:41 PM | Train: [164/180] Step 1150/1249 Loss 1.656 Prec@(1,3) (93.8%, 99.7%), ce_loss 0.561, lat_loss 21.780
09/29 03:29:06 PM | Train: [164/180] Step 1200/1249 Loss 1.643 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.560, lat_loss 21.780
09/29 03:29:31 PM | Train: [164/180] Step 1249/1249 Loss 1.631 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.560, lat_loss 21.780
09/29 03:29:32 PM | _w_step_train: [164/180] Final Prec@1 93.9075% Time 631.78
09/29 03:29:32 PM | Start to train theta for epoch 163
09/29 03:29:53 PM | Train: [164/180] Step 050/312 Loss 3.638 Prec@(1,3) (87.6%, 99.6%), ce_loss 0.560, lat_loss 21.780
09/29 03:30:13 PM | Train: [164/180] Step 100/312 Loss 3.859 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.560, lat_loss 21.780
09/29 03:30:30 PM | Train: [164/180] Step 150/312 Loss 3.731 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.560, lat_loss 21.780
09/29 03:30:50 PM | Train: [164/180] Step 200/312 Loss 4.085 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.560, lat_loss 21.780
09/29 03:31:10 PM | Train: [164/180] Step 250/312 Loss 4.039 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.560, lat_loss 21.780
09/29 03:31:31 PM | Train: [164/180] Step 300/312 Loss 3.986 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.560, lat_loss 21.780
09/29 03:31:36 PM | Train: [164/180] Step 312/312 Loss 3.989 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.560, lat_loss 21.780
09/29 03:31:36 PM | _theta_step_train: [164/180] Final Prec@1 86.4200% Time 124.53
09/29 03:31:42 PM | Valid: [164/180] Step 050/312 Loss 3.164 Prec@(1,3) (89.2%, 99.8%), ce_loss 0.560, lat_loss 21.780
09/29 03:31:46 PM | Valid: [164/180] Step 100/312 Loss 3.968 Prec@(1,3) (86.9%, 99.1%), ce_loss 0.560, lat_loss 21.780
09/29 03:31:51 PM | Valid: [164/180] Step 150/312 Loss 4.182 Prec@(1,3) (86.2%, 98.7%), ce_loss 0.560, lat_loss 21.780
09/29 03:31:55 PM | Valid: [164/180] Step 200/312 Loss 4.038 Prec@(1,3) (86.5%, 98.8%), ce_loss 0.560, lat_loss 21.780
09/29 03:32:00 PM | Valid: [164/180] Step 250/312 Loss 3.886 Prec@(1,3) (86.7%, 98.9%), ce_loss 0.560, lat_loss 21.780
09/29 03:32:04 PM | Valid: [164/180] Step 300/312 Loss 3.825 Prec@(1,3) (86.9%, 99.0%), ce_loss 0.560, lat_loss 21.780
09/29 03:32:06 PM | Valid: [164/180] Step 312/312 Loss 3.797 Prec@(1,3) (86.9%, 99.0%), ce_loss 0.560, lat_loss 21.780
09/29 03:32:06 PM | val: [164/180] Final Prec@1 86.9200% Time 29.59
09/29 03:32:06 PM | Start to train weights for epoch 164
09/29 03:32:28 PM | Train: [165/180] Step 050/1249 Loss 1.448 Prec@(1,3) (95.2%, 99.7%), ce_loss 0.560, lat_loss 21.780
09/29 03:32:49 PM | Train: [165/180] Step 100/1249 Loss 1.456 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.560, lat_loss 21.780
09/29 03:33:10 PM | Train: [165/180] Step 150/1249 Loss 1.408 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.560, lat_loss 21.780
09/29 03:33:32 PM | Train: [165/180] Step 200/1249 Loss 1.438 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.560, lat_loss 21.780
09/29 03:33:56 PM | Train: [165/180] Step 250/1249 Loss 1.428 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.560, lat_loss 21.780
09/29 03:34:20 PM | Train: [165/180] Step 300/1249 Loss 1.417 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.560, lat_loss 21.780
09/29 03:34:43 PM | Train: [165/180] Step 350/1249 Loss 1.389 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.560, lat_loss 21.780
09/29 03:35:03 PM | Train: [165/180] Step 400/1249 Loss 1.353 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.560, lat_loss 21.780
09/29 03:35:26 PM | Train: [165/180] Step 450/1249 Loss 1.343 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.560, lat_loss 21.780
09/29 03:35:50 PM | Train: [165/180] Step 500/1249 Loss 1.345 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.559, lat_loss 21.780
09/29 03:36:15 PM | Train: [165/180] Step 550/1249 Loss 1.342 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.559, lat_loss 21.780
09/29 03:36:40 PM | Train: [165/180] Step 600/1249 Loss 1.402 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.559, lat_loss 21.780
09/29 03:37:04 PM | Train: [165/180] Step 650/1249 Loss 1.408 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.559, lat_loss 21.780
09/29 03:37:29 PM | Train: [165/180] Step 700/1249 Loss 1.387 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.559, lat_loss 21.780
09/29 03:37:54 PM | Train: [165/180] Step 750/1249 Loss 1.391 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.559, lat_loss 21.780
09/29 03:38:19 PM | Train: [165/180] Step 800/1249 Loss 1.379 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.559, lat_loss 21.780
09/29 03:38:44 PM | Train: [165/180] Step 850/1249 Loss 1.370 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.559, lat_loss 21.780
09/29 03:39:08 PM | Train: [165/180] Step 900/1249 Loss 1.380 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.559, lat_loss 21.780
09/29 03:39:33 PM | Train: [165/180] Step 950/1249 Loss 1.372 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.559, lat_loss 21.780
09/29 03:39:58 PM | Train: [165/180] Step 1000/1249 Loss 1.389 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.559, lat_loss 21.780
09/29 03:40:22 PM | Train: [165/180] Step 1050/1249 Loss 1.394 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.559, lat_loss 21.780
09/29 03:40:47 PM | Train: [165/180] Step 1100/1249 Loss 1.388 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.559, lat_loss 21.780
09/29 03:41:12 PM | Train: [165/180] Step 1150/1249 Loss 1.402 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.559, lat_loss 21.780
09/29 03:41:36 PM | Train: [165/180] Step 1200/1249 Loss 1.399 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.559, lat_loss 21.780
09/29 03:42:01 PM | Train: [165/180] Step 1249/1249 Loss 1.410 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.559, lat_loss 21.780
09/29 03:42:01 PM | _w_step_train: [165/180] Final Prec@1 94.4475% Time 595.18
09/29 03:42:01 PM | Start to train theta for epoch 164
09/29 03:42:22 PM | Train: [165/180] Step 050/312 Loss 3.741 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.559, lat_loss 21.780
09/29 03:42:43 PM | Train: [165/180] Step 100/312 Loss 3.935 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.558, lat_loss 21.780
09/29 03:43:03 PM | Train: [165/180] Step 150/312 Loss 3.948 Prec@(1,3) (86.5%, 99.3%), ce_loss 0.558, lat_loss 21.780
09/29 03:43:24 PM | Train: [165/180] Step 200/312 Loss 3.836 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.558, lat_loss 21.780
09/29 03:43:45 PM | Train: [165/180] Step 250/312 Loss 3.853 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:06 PM | Train: [165/180] Step 300/312 Loss 3.807 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:11 PM | Train: [165/180] Step 312/312 Loss 3.835 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:11 PM | _theta_step_train: [165/180] Final Prec@1 86.8500% Time 129.91
09/29 03:44:16 PM | Valid: [165/180] Step 050/312 Loss 3.386 Prec@(1,3) (87.6%, 99.7%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:21 PM | Valid: [165/180] Step 100/312 Loss 3.916 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:26 PM | Valid: [165/180] Step 150/312 Loss 3.835 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:30 PM | Valid: [165/180] Step 200/312 Loss 3.720 Prec@(1,3) (87.1%, 99.2%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:35 PM | Valid: [165/180] Step 250/312 Loss 3.622 Prec@(1,3) (87.4%, 99.3%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:39 PM | Valid: [165/180] Step 300/312 Loss 3.715 Prec@(1,3) (87.0%, 99.1%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:40 PM | Valid: [165/180] Step 312/312 Loss 3.704 Prec@(1,3) (87.0%, 99.2%), ce_loss 0.558, lat_loss 21.780
09/29 03:44:41 PM | val: [165/180] Final Prec@1 86.9700% Time 29.69
09/29 03:44:41 PM | Start to train weights for epoch 165
09/29 03:45:07 PM | Train: [166/180] Step 050/1249 Loss 1.695 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:45:32 PM | Train: [166/180] Step 100/1249 Loss 1.513 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:45:57 PM | Train: [166/180] Step 150/1249 Loss 1.381 Prec@(1,3) (94.9%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:46:21 PM | Train: [166/180] Step 200/1249 Loss 1.313 Prec@(1,3) (95.0%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:46:37 PM | Train: [166/180] Step 250/1249 Loss 1.340 Prec@(1,3) (94.9%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:46:53 PM | Train: [166/180] Step 300/1249 Loss 1.372 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:47:09 PM | Train: [166/180] Step 350/1249 Loss 1.390 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:47:25 PM | Train: [166/180] Step 400/1249 Loss 1.432 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:47:47 PM | Train: [166/180] Step 450/1249 Loss 1.404 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:48:08 PM | Train: [166/180] Step 500/1249 Loss 1.394 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:48:28 PM | Train: [166/180] Step 550/1249 Loss 1.397 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.558, lat_loss 21.780
09/29 03:48:50 PM | Train: [166/180] Step 600/1249 Loss 1.390 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:49:09 PM | Train: [166/180] Step 650/1249 Loss 1.397 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:49:32 PM | Train: [166/180] Step 700/1249 Loss 1.383 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:49:53 PM | Train: [166/180] Step 750/1249 Loss 1.400 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:50:16 PM | Train: [166/180] Step 800/1249 Loss 1.401 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:50:39 PM | Train: [166/180] Step 850/1249 Loss 1.411 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:51:02 PM | Train: [166/180] Step 900/1249 Loss 1.454 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:51:25 PM | Train: [166/180] Step 950/1249 Loss 1.451 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:51:50 PM | Train: [166/180] Step 1000/1249 Loss 1.443 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:52:15 PM | Train: [166/180] Step 1050/1249 Loss 1.432 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:52:39 PM | Train: [166/180] Step 1100/1249 Loss 1.462 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:53:03 PM | Train: [166/180] Step 1150/1249 Loss 1.455 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:53:28 PM | Train: [166/180] Step 1200/1249 Loss 1.477 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:53:53 PM | Train: [166/180] Step 1249/1249 Loss 1.472 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.557, lat_loss 21.780
09/29 03:53:53 PM | _w_step_train: [166/180] Final Prec@1 94.2750% Time 551.70
09/29 03:53:53 PM | Start to train theta for epoch 165
09/29 03:54:14 PM | Train: [166/180] Step 050/312 Loss 3.919 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.557, lat_loss 21.780
09/29 03:54:35 PM | Train: [166/180] Step 100/312 Loss 3.556 Prec@(1,3) (87.7%, 99.3%), ce_loss 0.557, lat_loss 21.780
09/29 03:54:56 PM | Train: [166/180] Step 150/312 Loss 3.565 Prec@(1,3) (87.7%, 99.4%), ce_loss 0.557, lat_loss 21.780
09/29 03:55:17 PM | Train: [166/180] Step 200/312 Loss 3.659 Prec@(1,3) (87.6%, 99.4%), ce_loss 0.557, lat_loss 21.780
09/29 03:55:36 PM | Train: [166/180] Step 250/312 Loss 3.665 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.557, lat_loss 21.780
09/29 03:55:57 PM | Train: [166/180] Step 300/312 Loss 3.705 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.557, lat_loss 21.780
09/29 03:56:01 PM | Train: [166/180] Step 312/312 Loss 3.722 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.557, lat_loss 21.780
09/29 03:56:02 PM | _theta_step_train: [166/180] Final Prec@1 87.1800% Time 128.91
09/29 03:56:07 PM | Valid: [166/180] Step 050/312 Loss 4.047 Prec@(1,3) (85.4%, 99.0%), ce_loss 0.557, lat_loss 21.780
09/29 03:56:11 PM | Valid: [166/180] Step 100/312 Loss 4.058 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.557, lat_loss 21.780
09/29 03:56:15 PM | Valid: [166/180] Step 150/312 Loss 4.366 Prec@(1,3) (85.8%, 98.6%), ce_loss 0.557, lat_loss 21.780
09/29 03:56:19 PM | Valid: [166/180] Step 200/312 Loss 4.195 Prec@(1,3) (86.2%, 98.9%), ce_loss 0.556, lat_loss 21.780
09/29 03:56:23 PM | Valid: [166/180] Step 250/312 Loss 4.065 Prec@(1,3) (86.3%, 98.9%), ce_loss 0.556, lat_loss 21.780
09/29 03:56:27 PM | Valid: [166/180] Step 300/312 Loss 4.016 Prec@(1,3) (86.5%, 99.0%), ce_loss 0.556, lat_loss 21.780
09/29 03:56:28 PM | Valid: [166/180] Step 312/312 Loss 3.994 Prec@(1,3) (86.4%, 99.0%), ce_loss 0.556, lat_loss 21.780
09/29 03:56:28 PM | val: [166/180] Final Prec@1 86.4400% Time 26.79
09/29 03:56:28 PM | Start to train weights for epoch 166
09/29 03:56:54 PM | Train: [167/180] Step 050/1249 Loss 1.780 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.556, lat_loss 21.780
09/29 03:57:19 PM | Train: [167/180] Step 100/1249 Loss 1.614 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.556, lat_loss 21.780
09/29 03:57:43 PM | Train: [167/180] Step 150/1249 Loss 1.516 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.556, lat_loss 21.780
09/29 03:58:08 PM | Train: [167/180] Step 200/1249 Loss 1.568 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.556, lat_loss 21.780
09/29 03:58:33 PM | Train: [167/180] Step 250/1249 Loss 1.514 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.556, lat_loss 21.780
09/29 03:58:58 PM | Train: [167/180] Step 300/1249 Loss 1.529 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.556, lat_loss 21.780
09/29 03:59:22 PM | Train: [167/180] Step 350/1249 Loss 1.488 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.556, lat_loss 21.780
09/29 03:59:47 PM | Train: [167/180] Step 400/1249 Loss 1.545 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.556, lat_loss 21.780
09/29 04:00:12 PM | Train: [167/180] Step 450/1249 Loss 1.530 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.556, lat_loss 21.780
09/29 04:00:37 PM | Train: [167/180] Step 500/1249 Loss 1.500 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.556, lat_loss 21.780
09/29 04:01:02 PM | Train: [167/180] Step 550/1249 Loss 1.496 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.556, lat_loss 21.780
09/29 04:01:27 PM | Train: [167/180] Step 600/1249 Loss 1.483 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.556, lat_loss 21.780
09/29 04:01:51 PM | Train: [167/180] Step 650/1249 Loss 1.499 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.556, lat_loss 21.780
09/29 04:02:16 PM | Train: [167/180] Step 700/1249 Loss 1.494 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.556, lat_loss 21.780
09/29 04:02:41 PM | Train: [167/180] Step 750/1249 Loss 1.499 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:03:05 PM | Train: [167/180] Step 800/1249 Loss 1.522 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:03:30 PM | Train: [167/180] Step 850/1249 Loss 1.540 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:03:55 PM | Train: [167/180] Step 900/1249 Loss 1.539 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:04:20 PM | Train: [167/180] Step 950/1249 Loss 1.555 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:04:45 PM | Train: [167/180] Step 1000/1249 Loss 1.557 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:05:09 PM | Train: [167/180] Step 1050/1249 Loss 1.541 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:05:34 PM | Train: [167/180] Step 1100/1249 Loss 1.523 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:05:59 PM | Train: [167/180] Step 1150/1249 Loss 1.521 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:06:24 PM | Train: [167/180] Step 1200/1249 Loss 1.509 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:06:48 PM | Train: [167/180] Step 1249/1249 Loss 1.501 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:06:48 PM | _w_step_train: [167/180] Final Prec@1 94.2300% Time 619.75
09/29 04:06:48 PM | Start to train theta for epoch 166
09/29 04:07:08 PM | Train: [167/180] Step 050/312 Loss 3.666 Prec@(1,3) (87.6%, 99.6%), ce_loss 0.555, lat_loss 21.780
09/29 04:07:28 PM | Train: [167/180] Step 100/312 Loss 3.911 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.555, lat_loss 21.780
09/29 04:07:49 PM | Train: [167/180] Step 150/312 Loss 3.903 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.555, lat_loss 21.780
09/29 04:08:07 PM | Train: [167/180] Step 200/312 Loss 3.959 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.555, lat_loss 21.780
09/29 04:08:27 PM | Train: [167/180] Step 250/312 Loss 3.894 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.555, lat_loss 21.780
09/29 04:08:47 PM | Train: [167/180] Step 300/312 Loss 3.876 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.555, lat_loss 21.780
09/29 04:08:51 PM | Train: [167/180] Step 312/312 Loss 3.887 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.555, lat_loss 21.780
09/29 04:08:52 PM | _theta_step_train: [167/180] Final Prec@1 86.8100% Time 123.39
09/29 04:08:57 PM | Valid: [167/180] Step 050/312 Loss 3.271 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.555, lat_loss 21.780
09/29 04:09:02 PM | Valid: [167/180] Step 100/312 Loss 3.765 Prec@(1,3) (87.5%, 99.2%), ce_loss 0.555, lat_loss 21.780
09/29 04:09:07 PM | Valid: [167/180] Step 150/312 Loss 3.955 Prec@(1,3) (86.9%, 99.1%), ce_loss 0.555, lat_loss 21.780
09/29 04:09:11 PM | Valid: [167/180] Step 200/312 Loss 3.983 Prec@(1,3) (86.7%, 98.8%), ce_loss 0.555, lat_loss 21.780
09/29 04:09:16 PM | Valid: [167/180] Step 250/312 Loss 3.917 Prec@(1,3) (86.7%, 98.9%), ce_loss 0.555, lat_loss 21.780
09/29 04:09:20 PM | Valid: [167/180] Step 300/312 Loss 3.871 Prec@(1,3) (86.7%, 98.9%), ce_loss 0.555, lat_loss 21.780
09/29 04:09:22 PM | Valid: [167/180] Step 312/312 Loss 3.868 Prec@(1,3) (86.6%, 99.0%), ce_loss 0.555, lat_loss 21.780
09/29 04:09:22 PM | val: [167/180] Final Prec@1 86.6000% Time 30.01
09/29 04:09:22 PM | Start to train weights for epoch 167
09/29 04:09:48 PM | Train: [168/180] Step 050/1249 Loss 1.117 Prec@(1,3) (95.4%, 100.0%), ce_loss 0.555, lat_loss 21.780
09/29 04:10:11 PM | Train: [168/180] Step 100/1249 Loss 1.185 Prec@(1,3) (95.1%, 100.0%), ce_loss 0.555, lat_loss 21.780
09/29 04:10:36 PM | Train: [168/180] Step 150/1249 Loss 1.252 Prec@(1,3) (94.7%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:10:58 PM | Train: [168/180] Step 200/1249 Loss 1.201 Prec@(1,3) (94.9%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:11:22 PM | Train: [168/180] Step 250/1249 Loss 1.215 Prec@(1,3) (94.9%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:11:44 PM | Train: [168/180] Step 300/1249 Loss 1.226 Prec@(1,3) (95.0%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:12:08 PM | Train: [168/180] Step 350/1249 Loss 1.210 Prec@(1,3) (94.9%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:12:33 PM | Train: [168/180] Step 400/1249 Loss 1.204 Prec@(1,3) (95.0%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:12:58 PM | Train: [168/180] Step 450/1249 Loss 1.234 Prec@(1,3) (95.0%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:13:23 PM | Train: [168/180] Step 500/1249 Loss 1.248 Prec@(1,3) (94.9%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:13:48 PM | Train: [168/180] Step 550/1249 Loss 1.264 Prec@(1,3) (94.9%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:14:13 PM | Train: [168/180] Step 600/1249 Loss 1.311 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.554, lat_loss 21.780
09/29 04:14:38 PM | Train: [168/180] Step 650/1249 Loss 1.310 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.554, lat_loss 21.780
09/29 04:15:03 PM | Train: [168/180] Step 700/1249 Loss 1.308 Prec@(1,3) (94.7%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:15:27 PM | Train: [168/180] Step 750/1249 Loss 1.321 Prec@(1,3) (94.7%, 100.0%), ce_loss 0.554, lat_loss 21.780
09/29 04:15:53 PM | Train: [168/180] Step 800/1249 Loss 1.347 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.554, lat_loss 21.780
09/29 04:16:18 PM | Train: [168/180] Step 850/1249 Loss 1.367 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.554, lat_loss 21.780
09/29 04:16:43 PM | Train: [168/180] Step 900/1249 Loss 1.378 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:17:01 PM | Train: [168/180] Step 950/1249 Loss 1.395 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:17:17 PM | Train: [168/180] Step 1000/1249 Loss 1.389 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:17:33 PM | Train: [168/180] Step 1050/1249 Loss 1.383 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:17:52 PM | Train: [168/180] Step 1100/1249 Loss 1.374 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:18:17 PM | Train: [168/180] Step 1150/1249 Loss 1.376 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:18:42 PM | Train: [168/180] Step 1200/1249 Loss 1.377 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:19:06 PM | Train: [168/180] Step 1249/1249 Loss 1.407 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:19:06 PM | _w_step_train: [168/180] Final Prec@1 94.4425% Time 584.49
09/29 04:19:06 PM | Start to train theta for epoch 167
09/29 04:19:27 PM | Train: [168/180] Step 050/312 Loss 4.104 Prec@(1,3) (85.8%, 99.4%), ce_loss 0.553, lat_loss 21.780
09/29 04:19:47 PM | Train: [168/180] Step 100/312 Loss 3.970 Prec@(1,3) (86.1%, 99.5%), ce_loss 0.553, lat_loss 21.780
09/29 04:20:06 PM | Train: [168/180] Step 150/312 Loss 3.946 Prec@(1,3) (86.5%, 99.5%), ce_loss 0.553, lat_loss 21.780
09/29 04:20:23 PM | Train: [168/180] Step 200/312 Loss 3.948 Prec@(1,3) (86.6%, 99.5%), ce_loss 0.553, lat_loss 21.780
09/29 04:20:40 PM | Train: [168/180] Step 250/312 Loss 4.026 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.553, lat_loss 21.780
09/29 04:20:59 PM | Train: [168/180] Step 300/312 Loss 4.032 Prec@(1,3) (86.3%, 99.4%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:04 PM | Train: [168/180] Step 312/312 Loss 4.009 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:04 PM | _theta_step_train: [168/180] Final Prec@1 86.3900% Time 118.27
09/29 04:21:10 PM | Valid: [168/180] Step 050/312 Loss 4.313 Prec@(1,3) (85.5%, 98.7%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:14 PM | Valid: [168/180] Step 100/312 Loss 4.366 Prec@(1,3) (85.6%, 98.8%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:19 PM | Valid: [168/180] Step 150/312 Loss 4.363 Prec@(1,3) (85.3%, 98.8%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:23 PM | Valid: [168/180] Step 200/312 Loss 4.287 Prec@(1,3) (85.6%, 98.8%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:28 PM | Valid: [168/180] Step 250/312 Loss 4.070 Prec@(1,3) (86.2%, 99.0%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:32 PM | Valid: [168/180] Step 300/312 Loss 4.150 Prec@(1,3) (86.0%, 98.8%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:34 PM | Valid: [168/180] Step 312/312 Loss 4.166 Prec@(1,3) (85.8%, 98.8%), ce_loss 0.553, lat_loss 21.780
09/29 04:21:34 PM | val: [168/180] Final Prec@1 85.8300% Time 29.30
09/29 04:21:34 PM | Start to train weights for epoch 168
09/29 04:22:00 PM | Train: [169/180] Step 050/1249 Loss 1.763 Prec@(1,3) (92.5%, 99.8%), ce_loss 0.553, lat_loss 21.780
09/29 04:22:23 PM | Train: [169/180] Step 100/1249 Loss 1.752 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.553, lat_loss 21.780
09/29 04:22:47 PM | Train: [169/180] Step 150/1249 Loss 1.630 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:23:11 PM | Train: [169/180] Step 200/1249 Loss 1.581 Prec@(1,3) (93.7%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:23:34 PM | Train: [169/180] Step 250/1249 Loss 1.517 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:23:58 PM | Train: [169/180] Step 300/1249 Loss 1.509 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.553, lat_loss 21.780
09/29 04:24:21 PM | Train: [169/180] Step 350/1249 Loss 1.448 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:24:45 PM | Train: [169/180] Step 400/1249 Loss 1.420 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:25:09 PM | Train: [169/180] Step 450/1249 Loss 1.457 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:25:32 PM | Train: [169/180] Step 500/1249 Loss 1.488 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:25:53 PM | Train: [169/180] Step 550/1249 Loss 1.469 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:26:13 PM | Train: [169/180] Step 600/1249 Loss 1.454 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:26:33 PM | Train: [169/180] Step 650/1249 Loss 1.448 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:26:54 PM | Train: [169/180] Step 700/1249 Loss 1.489 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.552, lat_loss 21.780
09/29 04:27:16 PM | Train: [169/180] Step 750/1249 Loss 1.485 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.552, lat_loss 21.780
09/29 04:27:40 PM | Train: [169/180] Step 800/1249 Loss 1.482 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.552, lat_loss 21.780
09/29 04:28:03 PM | Train: [169/180] Step 850/1249 Loss 1.474 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.552, lat_loss 21.780
09/29 04:28:26 PM | Train: [169/180] Step 900/1249 Loss 1.465 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:28:50 PM | Train: [169/180] Step 950/1249 Loss 1.446 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:29:14 PM | Train: [169/180] Step 1000/1249 Loss 1.431 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.552, lat_loss 21.780
09/29 04:29:38 PM | Train: [169/180] Step 1050/1249 Loss 1.458 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.552, lat_loss 21.780
09/29 04:30:02 PM | Train: [169/180] Step 1100/1249 Loss 1.478 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.552, lat_loss 21.780
09/29 04:30:27 PM | Train: [169/180] Step 1150/1249 Loss 1.470 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.551, lat_loss 21.780
09/29 04:30:51 PM | Train: [169/180] Step 1200/1249 Loss 1.461 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.551, lat_loss 21.780
09/29 04:31:16 PM | Train: [169/180] Step 1249/1249 Loss 1.462 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.551, lat_loss 21.780
09/29 04:31:16 PM | _w_step_train: [169/180] Final Prec@1 94.3850% Time 581.97
09/29 04:31:16 PM | Start to train theta for epoch 168
09/29 04:31:37 PM | Train: [169/180] Step 050/312 Loss 3.641 Prec@(1,3) (88.1%, 99.3%), ce_loss 0.551, lat_loss 21.780
09/29 04:31:58 PM | Train: [169/180] Step 100/312 Loss 3.786 Prec@(1,3) (87.4%, 99.3%), ce_loss 0.551, lat_loss 21.780
09/29 04:32:18 PM | Train: [169/180] Step 150/312 Loss 3.730 Prec@(1,3) (87.4%, 99.3%), ce_loss 0.551, lat_loss 21.780
09/29 04:32:37 PM | Train: [169/180] Step 200/312 Loss 3.817 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.551, lat_loss 21.780
09/29 04:32:56 PM | Train: [169/180] Step 250/312 Loss 3.780 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:16 PM | Train: [169/180] Step 300/312 Loss 3.793 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:21 PM | Train: [169/180] Step 312/312 Loss 3.803 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:21 PM | _theta_step_train: [169/180] Final Prec@1 87.1300% Time 125.79
09/29 04:33:27 PM | Valid: [169/180] Step 050/312 Loss 3.451 Prec@(1,3) (88.4%, 99.7%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:31 PM | Valid: [169/180] Step 100/312 Loss 3.902 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:36 PM | Valid: [169/180] Step 150/312 Loss 3.902 Prec@(1,3) (87.1%, 99.2%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:40 PM | Valid: [169/180] Step 200/312 Loss 3.827 Prec@(1,3) (87.3%, 99.2%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:45 PM | Valid: [169/180] Step 250/312 Loss 3.811 Prec@(1,3) (87.2%, 99.1%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:50 PM | Valid: [169/180] Step 300/312 Loss 3.789 Prec@(1,3) (87.0%, 99.2%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:51 PM | Valid: [169/180] Step 312/312 Loss 3.784 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.551, lat_loss 21.780
09/29 04:33:51 PM | val: [169/180] Final Prec@1 86.9400% Time 29.30
09/29 04:33:51 PM | Start to train weights for epoch 169
09/29 04:34:17 PM | Train: [170/180] Step 050/1249 Loss 1.672 Prec@(1,3) (93.5%, 99.9%), ce_loss 0.551, lat_loss 21.780
09/29 04:34:41 PM | Train: [170/180] Step 100/1249 Loss 1.803 Prec@(1,3) (93.1%, 99.8%), ce_loss 0.551, lat_loss 21.780
09/29 04:35:05 PM | Train: [170/180] Step 150/1249 Loss 1.674 Prec@(1,3) (93.2%, 99.8%), ce_loss 0.551, lat_loss 21.780
09/29 04:35:30 PM | Train: [170/180] Step 200/1249 Loss 1.708 Prec@(1,3) (93.4%, 99.8%), ce_loss 0.551, lat_loss 21.780
09/29 04:35:54 PM | Train: [170/180] Step 250/1249 Loss 1.613 Prec@(1,3) (93.7%, 99.8%), ce_loss 0.551, lat_loss 21.780
09/29 04:36:19 PM | Train: [170/180] Step 300/1249 Loss 1.602 Prec@(1,3) (93.8%, 99.9%), ce_loss 0.551, lat_loss 21.780
09/29 04:36:43 PM | Train: [170/180] Step 350/1249 Loss 1.561 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.551, lat_loss 21.780
09/29 04:37:08 PM | Train: [170/180] Step 400/1249 Loss 1.531 Prec@(1,3) (94.0%, 99.9%), ce_loss 0.551, lat_loss 21.780
09/29 04:37:30 PM | Train: [170/180] Step 450/1249 Loss 1.518 Prec@(1,3) (94.1%, 99.9%), ce_loss 0.551, lat_loss 21.780
09/29 04:37:54 PM | Train: [170/180] Step 500/1249 Loss 1.506 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:38:19 PM | Train: [170/180] Step 550/1249 Loss 1.461 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:38:43 PM | Train: [170/180] Step 600/1249 Loss 1.445 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:39:06 PM | Train: [170/180] Step 650/1249 Loss 1.440 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:39:29 PM | Train: [170/180] Step 700/1249 Loss 1.449 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:39:53 PM | Train: [170/180] Step 750/1249 Loss 1.430 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:40:17 PM | Train: [170/180] Step 800/1249 Loss 1.426 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:40:42 PM | Train: [170/180] Step 850/1249 Loss 1.415 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:41:07 PM | Train: [170/180] Step 900/1249 Loss 1.403 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:41:31 PM | Train: [170/180] Step 950/1249 Loss 1.407 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:41:55 PM | Train: [170/180] Step 1000/1249 Loss 1.414 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:42:16 PM | Train: [170/180] Step 1050/1249 Loss 1.407 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:42:37 PM | Train: [170/180] Step 1100/1249 Loss 1.408 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:42:57 PM | Train: [170/180] Step 1150/1249 Loss 1.406 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:43:19 PM | Train: [170/180] Step 1200/1249 Loss 1.411 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:43:43 PM | Train: [170/180] Step 1249/1249 Loss 1.409 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.550, lat_loss 21.780
09/29 04:43:43 PM | _w_step_train: [170/180] Final Prec@1 94.5150% Time 592.37
09/29 04:43:43 PM | Start to train theta for epoch 169
09/29 04:44:04 PM | Train: [170/180] Step 050/312 Loss 4.093 Prec@(1,3) (85.9%, 99.0%), ce_loss 0.550, lat_loss 21.780
09/29 04:44:24 PM | Train: [170/180] Step 100/312 Loss 4.037 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.550, lat_loss 21.780
09/29 04:44:44 PM | Train: [170/180] Step 150/312 Loss 3.849 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.549, lat_loss 21.780
09/29 04:45:04 PM | Train: [170/180] Step 200/312 Loss 3.898 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.549, lat_loss 21.780
09/29 04:45:24 PM | Train: [170/180] Step 250/312 Loss 3.845 Prec@(1,3) (86.6%, 99.4%), ce_loss 0.549, lat_loss 21.780
09/29 04:45:44 PM | Train: [170/180] Step 300/312 Loss 4.023 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.549, lat_loss 21.780
09/29 04:45:49 PM | Train: [170/180] Step 312/312 Loss 4.014 Prec@(1,3) (86.3%, 99.1%), ce_loss 0.549, lat_loss 21.780
09/29 04:45:49 PM | _theta_step_train: [170/180] Final Prec@1 86.2800% Time 126.17
09/29 04:45:55 PM | Valid: [170/180] Step 050/312 Loss 3.468 Prec@(1,3) (87.7%, 99.6%), ce_loss 0.549, lat_loss 21.780
09/29 04:45:59 PM | Valid: [170/180] Step 100/312 Loss 3.759 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.549, lat_loss 21.780
09/29 04:46:04 PM | Valid: [170/180] Step 150/312 Loss 3.795 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.549, lat_loss 21.780
09/29 04:46:09 PM | Valid: [170/180] Step 200/312 Loss 3.867 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.549, lat_loss 21.780
09/29 04:46:13 PM | Valid: [170/180] Step 250/312 Loss 3.808 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.549, lat_loss 21.780
09/29 04:46:18 PM | Valid: [170/180] Step 300/312 Loss 3.884 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.549, lat_loss 21.780
09/29 04:46:19 PM | Valid: [170/180] Step 312/312 Loss 3.877 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.549, lat_loss 21.780
09/29 04:46:19 PM | val: [170/180] Final Prec@1 86.4500% Time 29.73
09/29 04:46:19 PM | Start to train weights for epoch 170
09/29 04:46:45 PM | Train: [171/180] Step 050/1249 Loss 1.562 Prec@(1,3) (93.9%, 99.9%), ce_loss 0.549, lat_loss 21.780
09/29 04:47:09 PM | Train: [171/180] Step 100/1249 Loss 1.395 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.549, lat_loss 21.780
09/29 04:47:33 PM | Train: [171/180] Step 150/1249 Loss 1.478 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.549, lat_loss 21.780
09/29 04:47:58 PM | Train: [171/180] Step 200/1249 Loss 1.386 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.549, lat_loss 21.779
09/29 04:48:23 PM | Train: [171/180] Step 250/1249 Loss 1.385 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:48:47 PM | Train: [171/180] Step 300/1249 Loss 1.374 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:49:10 PM | Train: [171/180] Step 350/1249 Loss 1.365 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:49:35 PM | Train: [171/180] Step 400/1249 Loss 1.363 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:49:59 PM | Train: [171/180] Step 450/1249 Loss 1.399 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:50:23 PM | Train: [171/180] Step 500/1249 Loss 1.392 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:50:47 PM | Train: [171/180] Step 550/1249 Loss 1.350 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:51:10 PM | Train: [171/180] Step 600/1249 Loss 1.344 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:51:33 PM | Train: [171/180] Step 650/1249 Loss 1.339 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.549, lat_loss 21.779
09/29 04:51:57 PM | Train: [171/180] Step 700/1249 Loss 1.324 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:52:21 PM | Train: [171/180] Step 750/1249 Loss 1.334 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:52:45 PM | Train: [171/180] Step 800/1249 Loss 1.337 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:53:09 PM | Train: [171/180] Step 850/1249 Loss 1.343 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:53:33 PM | Train: [171/180] Step 900/1249 Loss 1.338 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:53:57 PM | Train: [171/180] Step 950/1249 Loss 1.327 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:54:20 PM | Train: [171/180] Step 1000/1249 Loss 1.380 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:54:45 PM | Train: [171/180] Step 1050/1249 Loss 1.387 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:55:09 PM | Train: [171/180] Step 1100/1249 Loss 1.402 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:55:33 PM | Train: [171/180] Step 1150/1249 Loss 1.394 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:55:57 PM | Train: [171/180] Step 1200/1249 Loss 1.389 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:56:21 PM | Train: [171/180] Step 1249/1249 Loss 1.382 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.548, lat_loss 21.779
09/29 04:56:21 PM | _w_step_train: [171/180] Final Prec@1 94.6325% Time 602.08
09/29 04:56:21 PM | Start to train theta for epoch 170
09/29 04:56:35 PM | Train: [171/180] Step 050/312 Loss 3.531 Prec@(1,3) (88.9%, 99.4%), ce_loss 0.548, lat_loss 21.779
09/29 04:56:47 PM | Train: [171/180] Step 100/312 Loss 3.971 Prec@(1,3) (87.6%, 99.4%), ce_loss 0.548, lat_loss 21.779
09/29 04:56:59 PM | Train: [171/180] Step 150/312 Loss 3.998 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.548, lat_loss 21.779
09/29 04:57:11 PM | Train: [171/180] Step 200/312 Loss 4.434 Prec@(1,3) (85.8%, 99.0%), ce_loss 0.548, lat_loss 21.779
09/29 04:57:23 PM | Train: [171/180] Step 250/312 Loss 4.282 Prec@(1,3) (86.0%, 99.1%), ce_loss 0.548, lat_loss 21.779
09/29 04:57:36 PM | Train: [171/180] Step 300/312 Loss 4.251 Prec@(1,3) (86.1%, 99.1%), ce_loss 0.548, lat_loss 21.779
09/29 04:57:39 PM | Train: [171/180] Step 312/312 Loss 4.179 Prec@(1,3) (86.3%, 99.1%), ce_loss 0.548, lat_loss 21.779
09/29 04:57:39 PM | _theta_step_train: [171/180] Final Prec@1 86.2800% Time 77.50
09/29 04:57:44 PM | Valid: [171/180] Step 050/312 Loss 3.679 Prec@(1,3) (87.0%, 99.6%), ce_loss 0.548, lat_loss 21.779
09/29 04:57:49 PM | Valid: [171/180] Step 100/312 Loss 3.766 Prec@(1,3) (87.3%, 99.4%), ce_loss 0.548, lat_loss 21.779
09/29 04:57:53 PM | Valid: [171/180] Step 150/312 Loss 3.812 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.548, lat_loss 21.779
09/29 04:57:58 PM | Valid: [171/180] Step 200/312 Loss 3.762 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.548, lat_loss 21.779
09/29 04:58:02 PM | Valid: [171/180] Step 250/312 Loss 3.662 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.548, lat_loss 21.779
09/29 04:58:07 PM | Valid: [171/180] Step 300/312 Loss 3.703 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.548, lat_loss 21.779
09/29 04:58:08 PM | Valid: [171/180] Step 312/312 Loss 3.698 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.548, lat_loss 21.779
09/29 04:58:08 PM | val: [171/180] Final Prec@1 86.9000% Time 29.56
09/29 04:58:08 PM | Start to train weights for epoch 171
09/29 04:58:34 PM | Train: [172/180] Step 050/1249 Loss 1.397 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.548, lat_loss 21.779
09/29 04:58:59 PM | Train: [172/180] Step 100/1249 Loss 1.236 Prec@(1,3) (95.3%, 99.9%), ce_loss 0.547, lat_loss 21.779
09/29 04:59:24 PM | Train: [172/180] Step 150/1249 Loss 1.262 Prec@(1,3) (95.1%, 99.9%), ce_loss 0.547, lat_loss 21.779
09/29 04:59:48 PM | Train: [172/180] Step 200/1249 Loss 1.293 Prec@(1,3) (95.0%, 99.9%), ce_loss 0.547, lat_loss 21.779
09/29 05:00:12 PM | Train: [172/180] Step 250/1249 Loss 1.492 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:00:37 PM | Train: [172/180] Step 300/1249 Loss 1.480 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:01:02 PM | Train: [172/180] Step 350/1249 Loss 1.480 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:01:26 PM | Train: [172/180] Step 400/1249 Loss 1.466 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:01:51 PM | Train: [172/180] Step 450/1249 Loss 1.449 Prec@(1,3) (94.7%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:02:15 PM | Train: [172/180] Step 500/1249 Loss 1.511 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:02:39 PM | Train: [172/180] Step 550/1249 Loss 1.479 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:03:03 PM | Train: [172/180] Step 600/1249 Loss 1.468 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:03:28 PM | Train: [172/180] Step 650/1249 Loss 1.488 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:03:53 PM | Train: [172/180] Step 700/1249 Loss 1.482 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:04:17 PM | Train: [172/180] Step 750/1249 Loss 1.467 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:04:41 PM | Train: [172/180] Step 800/1249 Loss 1.527 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:05:05 PM | Train: [172/180] Step 850/1249 Loss 1.511 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:05:30 PM | Train: [172/180] Step 900/1249 Loss 1.502 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.547, lat_loss 21.779
09/29 05:05:55 PM | Train: [172/180] Step 950/1249 Loss 1.491 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:06:20 PM | Train: [172/180] Step 1000/1249 Loss 1.491 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:06:44 PM | Train: [172/180] Step 1050/1249 Loss 1.484 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:07:09 PM | Train: [172/180] Step 1100/1249 Loss 1.478 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:07:34 PM | Train: [172/180] Step 1150/1249 Loss 1.461 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:07:59 PM | Train: [172/180] Step 1200/1249 Loss 1.464 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:08:23 PM | Train: [172/180] Step 1249/1249 Loss 1.457 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:08:23 PM | _w_step_train: [172/180] Final Prec@1 94.5650% Time 615.20
09/29 05:08:23 PM | Start to train theta for epoch 171
09/29 05:08:45 PM | Train: [172/180] Step 050/312 Loss 3.707 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.546, lat_loss 21.779
09/29 05:09:06 PM | Train: [172/180] Step 100/312 Loss 3.810 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.546, lat_loss 21.779
09/29 05:09:27 PM | Train: [172/180] Step 150/312 Loss 3.824 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.546, lat_loss 21.779
09/29 05:09:48 PM | Train: [172/180] Step 200/312 Loss 3.865 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.546, lat_loss 21.779
09/29 05:10:08 PM | Train: [172/180] Step 250/312 Loss 3.841 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.546, lat_loss 21.779
09/29 05:10:29 PM | Train: [172/180] Step 300/312 Loss 3.815 Prec@(1,3) (87.1%, 99.3%), ce_loss 0.546, lat_loss 21.779
09/29 05:10:34 PM | Train: [172/180] Step 312/312 Loss 3.864 Prec@(1,3) (87.0%, 99.3%), ce_loss 0.546, lat_loss 21.779
09/29 05:10:35 PM | _theta_step_train: [172/180] Final Prec@1 87.0100% Time 131.31
09/29 05:10:40 PM | Valid: [172/180] Step 050/312 Loss 3.359 Prec@(1,3) (87.3%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:10:45 PM | Valid: [172/180] Step 100/312 Loss 3.751 Prec@(1,3) (86.8%, 99.5%), ce_loss 0.546, lat_loss 21.779
09/29 05:10:49 PM | Valid: [172/180] Step 150/312 Loss 3.925 Prec@(1,3) (86.4%, 99.4%), ce_loss 0.546, lat_loss 21.779
09/29 05:10:54 PM | Valid: [172/180] Step 200/312 Loss 3.822 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.546, lat_loss 21.779
09/29 05:10:59 PM | Valid: [172/180] Step 250/312 Loss 3.760 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.546, lat_loss 21.779
09/29 05:11:03 PM | Valid: [172/180] Step 300/312 Loss 3.740 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.546, lat_loss 21.779
09/29 05:11:04 PM | Valid: [172/180] Step 312/312 Loss 3.735 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.546, lat_loss 21.779
09/29 05:11:04 PM | val: [172/180] Final Prec@1 86.9900% Time 29.70
09/29 05:11:04 PM | Start to train weights for epoch 172
09/29 05:11:30 PM | Train: [173/180] Step 050/1249 Loss 1.812 Prec@(1,3) (93.1%, 99.6%), ce_loss 0.546, lat_loss 21.779
09/29 05:11:54 PM | Train: [173/180] Step 100/1249 Loss 1.623 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:12:17 PM | Train: [173/180] Step 150/1249 Loss 1.438 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:12:41 PM | Train: [173/180] Step 200/1249 Loss 1.610 Prec@(1,3) (93.8%, 99.7%), ce_loss 0.546, lat_loss 21.779
09/29 05:13:04 PM | Train: [173/180] Step 250/1249 Loss 1.508 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:13:27 PM | Train: [173/180] Step 300/1249 Loss 1.538 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.546, lat_loss 21.779
09/29 05:13:50 PM | Train: [173/180] Step 350/1249 Loss 1.479 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:14:13 PM | Train: [173/180] Step 400/1249 Loss 1.509 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:14:36 PM | Train: [173/180] Step 450/1249 Loss 1.512 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:14:59 PM | Train: [173/180] Step 500/1249 Loss 1.491 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:15:22 PM | Train: [173/180] Step 550/1249 Loss 1.487 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:15:44 PM | Train: [173/180] Step 600/1249 Loss 1.489 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:16:06 PM | Train: [173/180] Step 650/1249 Loss 1.485 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:16:29 PM | Train: [173/180] Step 700/1249 Loss 1.519 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:16:51 PM | Train: [173/180] Step 750/1249 Loss 1.505 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:17:14 PM | Train: [173/180] Step 800/1249 Loss 1.510 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:17:35 PM | Train: [173/180] Step 850/1249 Loss 1.504 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:17:59 PM | Train: [173/180] Step 900/1249 Loss 1.480 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:18:23 PM | Train: [173/180] Step 950/1249 Loss 1.477 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:18:46 PM | Train: [173/180] Step 1000/1249 Loss 1.461 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:19:10 PM | Train: [173/180] Step 1050/1249 Loss 1.451 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:19:34 PM | Train: [173/180] Step 1100/1249 Loss 1.456 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:19:57 PM | Train: [173/180] Step 1150/1249 Loss 1.468 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.545, lat_loss 21.779
09/29 05:20:20 PM | Train: [173/180] Step 1200/1249 Loss 1.480 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.544, lat_loss 21.779
09/29 05:20:44 PM | Train: [173/180] Step 1249/1249 Loss 1.473 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.544, lat_loss 21.779
09/29 05:20:44 PM | _w_step_train: [173/180] Final Prec@1 94.3475% Time 579.96
09/29 05:20:44 PM | Start to train theta for epoch 172
09/29 05:21:06 PM | Train: [173/180] Step 050/312 Loss 3.615 Prec@(1,3) (87.7%, 99.1%), ce_loss 0.544, lat_loss 21.779
09/29 05:21:27 PM | Train: [173/180] Step 100/312 Loss 3.840 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.544, lat_loss 21.779
09/29 05:21:48 PM | Train: [173/180] Step 150/312 Loss 3.833 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.544, lat_loss 21.779
09/29 05:22:08 PM | Train: [173/180] Step 200/312 Loss 3.755 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.544, lat_loss 21.779
09/29 05:22:29 PM | Train: [173/180] Step 250/312 Loss 3.826 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.544, lat_loss 21.779
09/29 05:22:50 PM | Train: [173/180] Step 300/312 Loss 3.779 Prec@(1,3) (86.9%, 99.4%), ce_loss 0.544, lat_loss 21.779
09/29 05:22:55 PM | Train: [173/180] Step 312/312 Loss 3.805 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.544, lat_loss 21.779
09/29 05:22:55 PM | _theta_step_train: [173/180] Final Prec@1 86.7800% Time 130.75
09/29 05:23:00 PM | Valid: [173/180] Step 050/312 Loss 4.023 Prec@(1,3) (85.8%, 99.0%), ce_loss 0.544, lat_loss 21.779
09/29 05:23:05 PM | Valid: [173/180] Step 100/312 Loss 4.133 Prec@(1,3) (85.9%, 99.0%), ce_loss 0.544, lat_loss 21.779
09/29 05:23:10 PM | Valid: [173/180] Step 150/312 Loss 4.183 Prec@(1,3) (86.0%, 99.0%), ce_loss 0.544, lat_loss 21.779
09/29 05:23:14 PM | Valid: [173/180] Step 200/312 Loss 4.413 Prec@(1,3) (85.7%, 98.8%), ce_loss 0.544, lat_loss 21.779
09/29 05:23:19 PM | Valid: [173/180] Step 250/312 Loss 4.312 Prec@(1,3) (85.8%, 98.8%), ce_loss 0.544, lat_loss 21.779
09/29 05:23:23 PM | Valid: [173/180] Step 300/312 Loss 4.327 Prec@(1,3) (85.8%, 98.8%), ce_loss 0.544, lat_loss 21.779
09/29 05:23:24 PM | Valid: [173/180] Step 312/312 Loss 4.293 Prec@(1,3) (85.8%, 98.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:23:24 PM | val: [173/180] Final Prec@1 85.7800% Time 29.24
09/29 05:23:25 PM | Start to train weights for epoch 173
09/29 05:23:50 PM | Train: [174/180] Step 050/1249 Loss 1.261 Prec@(1,3) (95.0%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:24:15 PM | Train: [174/180] Step 100/1249 Loss 1.257 Prec@(1,3) (95.0%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:24:39 PM | Train: [174/180] Step 150/1249 Loss 1.297 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:25:03 PM | Train: [174/180] Step 200/1249 Loss 1.353 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:25:28 PM | Train: [174/180] Step 250/1249 Loss 1.336 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:25:51 PM | Train: [174/180] Step 300/1249 Loss 1.365 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:26:15 PM | Train: [174/180] Step 350/1249 Loss 1.354 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:26:38 PM | Train: [174/180] Step 400/1249 Loss 1.373 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:27:03 PM | Train: [174/180] Step 450/1249 Loss 1.448 Prec@(1,3) (94.3%, 99.9%), ce_loss 0.544, lat_loss 21.779
09/29 05:27:26 PM | Train: [174/180] Step 500/1249 Loss 1.477 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.544, lat_loss 21.779
09/29 05:27:51 PM | Train: [174/180] Step 550/1249 Loss 1.492 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.544, lat_loss 21.779
09/29 05:28:14 PM | Train: [174/180] Step 600/1249 Loss 1.530 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.544, lat_loss 21.779
09/29 05:28:38 PM | Train: [174/180] Step 650/1249 Loss 1.536 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:29:03 PM | Train: [174/180] Step 700/1249 Loss 1.507 Prec@(1,3) (94.2%, 99.9%), ce_loss 0.543, lat_loss 21.779
09/29 05:29:27 PM | Train: [174/180] Step 750/1249 Loss 1.539 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:29:51 PM | Train: [174/180] Step 800/1249 Loss 1.532 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:30:15 PM | Train: [174/180] Step 850/1249 Loss 1.519 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:30:39 PM | Train: [174/180] Step 900/1249 Loss 1.520 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:30:59 PM | Train: [174/180] Step 950/1249 Loss 1.518 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:31:21 PM | Train: [174/180] Step 1000/1249 Loss 1.499 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:31:42 PM | Train: [174/180] Step 1050/1249 Loss 1.505 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:32:03 PM | Train: [174/180] Step 1100/1249 Loss 1.506 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:32:25 PM | Train: [174/180] Step 1150/1249 Loss 1.525 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:32:47 PM | Train: [174/180] Step 1200/1249 Loss 1.560 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:33:11 PM | Train: [174/180] Step 1249/1249 Loss 1.562 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.543, lat_loss 21.779
09/29 05:33:11 PM | _w_step_train: [174/180] Final Prec@1 94.0150% Time 586.06
09/29 05:33:11 PM | Start to train theta for epoch 173
09/29 05:33:31 PM | Train: [174/180] Step 050/312 Loss 3.409 Prec@(1,3) (88.5%, 99.6%), ce_loss 0.543, lat_loss 21.779
09/29 05:33:52 PM | Train: [174/180] Step 100/312 Loss 3.488 Prec@(1,3) (87.9%, 99.5%), ce_loss 0.543, lat_loss 21.779
09/29 05:34:13 PM | Train: [174/180] Step 150/312 Loss 3.611 Prec@(1,3) (87.2%, 99.5%), ce_loss 0.543, lat_loss 21.779
09/29 05:34:34 PM | Train: [174/180] Step 200/312 Loss 3.816 Prec@(1,3) (86.7%, 99.4%), ce_loss 0.543, lat_loss 21.779
09/29 05:34:55 PM | Train: [174/180] Step 250/312 Loss 3.750 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:16 PM | Train: [174/180] Step 300/312 Loss 3.712 Prec@(1,3) (87.1%, 99.4%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:21 PM | Train: [174/180] Step 312/312 Loss 3.705 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:21 PM | _theta_step_train: [174/180] Final Prec@1 87.1500% Time 129.65
09/29 05:35:26 PM | Valid: [174/180] Step 050/312 Loss 3.413 Prec@(1,3) (87.8%, 99.7%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:31 PM | Valid: [174/180] Step 100/312 Loss 3.783 Prec@(1,3) (87.3%, 99.2%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:35 PM | Valid: [174/180] Step 150/312 Loss 3.985 Prec@(1,3) (86.5%, 99.0%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:40 PM | Valid: [174/180] Step 200/312 Loss 3.989 Prec@(1,3) (86.4%, 99.0%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:44 PM | Valid: [174/180] Step 250/312 Loss 3.909 Prec@(1,3) (86.5%, 99.0%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:49 PM | Valid: [174/180] Step 300/312 Loss 3.858 Prec@(1,3) (86.6%, 99.1%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:50 PM | Valid: [174/180] Step 312/312 Loss 3.867 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.543, lat_loss 21.779
09/29 05:35:50 PM | val: [174/180] Final Prec@1 86.5000% Time 29.53
09/29 05:35:50 PM | Start to train weights for epoch 174
09/29 05:36:15 PM | Train: [175/180] Step 050/1249 Loss 1.424 Prec@(1,3) (95.4%, 99.6%), ce_loss 0.543, lat_loss 21.779
09/29 05:36:38 PM | Train: [175/180] Step 100/1249 Loss 1.404 Prec@(1,3) (95.1%, 99.8%), ce_loss 0.542, lat_loss 21.779
09/29 05:37:00 PM | Train: [175/180] Step 150/1249 Loss 1.331 Prec@(1,3) (95.1%, 99.8%), ce_loss 0.542, lat_loss 21.779
09/29 05:37:24 PM | Train: [175/180] Step 200/1249 Loss 1.261 Prec@(1,3) (95.3%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:37:47 PM | Train: [175/180] Step 250/1249 Loss 1.348 Prec@(1,3) (95.1%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:38:10 PM | Train: [175/180] Step 300/1249 Loss 1.377 Prec@(1,3) (95.0%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:38:31 PM | Train: [175/180] Step 350/1249 Loss 1.468 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.542, lat_loss 21.779
09/29 05:38:53 PM | Train: [175/180] Step 400/1249 Loss 1.436 Prec@(1,3) (94.7%, 99.8%), ce_loss 0.542, lat_loss 21.779
09/29 05:39:15 PM | Train: [175/180] Step 450/1249 Loss 1.435 Prec@(1,3) (94.7%, 99.8%), ce_loss 0.542, lat_loss 21.779
09/29 05:39:37 PM | Train: [175/180] Step 500/1249 Loss 1.427 Prec@(1,3) (94.7%, 99.8%), ce_loss 0.542, lat_loss 21.779
09/29 05:40:00 PM | Train: [175/180] Step 550/1249 Loss 1.415 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:40:23 PM | Train: [175/180] Step 600/1249 Loss 1.407 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:40:47 PM | Train: [175/180] Step 650/1249 Loss 1.401 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:41:09 PM | Train: [175/180] Step 700/1249 Loss 1.397 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:41:32 PM | Train: [175/180] Step 750/1249 Loss 1.420 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:41:54 PM | Train: [175/180] Step 800/1249 Loss 1.405 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:42:14 PM | Train: [175/180] Step 850/1249 Loss 1.392 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:42:35 PM | Train: [175/180] Step 900/1249 Loss 1.401 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.542, lat_loss 21.779
09/29 05:42:55 PM | Train: [175/180] Step 950/1249 Loss 1.412 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:43:14 PM | Train: [175/180] Step 1000/1249 Loss 1.408 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:43:34 PM | Train: [175/180] Step 1050/1249 Loss 1.400 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:43:54 PM | Train: [175/180] Step 1100/1249 Loss 1.398 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:44:15 PM | Train: [175/180] Step 1150/1249 Loss 1.396 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:44:36 PM | Train: [175/180] Step 1200/1249 Loss 1.390 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:45:00 PM | Train: [175/180] Step 1249/1249 Loss 1.380 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:45:01 PM | _w_step_train: [175/180] Final Prec@1 94.6075% Time 550.37
09/29 05:45:01 PM | Start to train theta for epoch 174
09/29 05:45:22 PM | Train: [175/180] Step 050/312 Loss 3.893 Prec@(1,3) (87.1%, 99.5%), ce_loss 0.541, lat_loss 21.779
09/29 05:45:40 PM | Train: [175/180] Step 100/312 Loss 3.971 Prec@(1,3) (86.5%, 99.4%), ce_loss 0.541, lat_loss 21.779
09/29 05:46:00 PM | Train: [175/180] Step 150/312 Loss 4.099 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.541, lat_loss 21.779
09/29 05:46:20 PM | Train: [175/180] Step 200/312 Loss 4.052 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.541, lat_loss 21.779
09/29 05:46:38 PM | Train: [175/180] Step 250/312 Loss 4.257 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.541, lat_loss 21.779
09/29 05:46:59 PM | Train: [175/180] Step 300/312 Loss 4.193 Prec@(1,3) (85.7%, 99.2%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:04 PM | Train: [175/180] Step 312/312 Loss 4.149 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:04 PM | _theta_step_train: [175/180] Final Prec@1 85.8500% Time 123.27
09/29 05:47:09 PM | Valid: [175/180] Step 050/312 Loss 3.805 Prec@(1,3) (86.7%, 99.1%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:14 PM | Valid: [175/180] Step 100/312 Loss 3.814 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:18 PM | Valid: [175/180] Step 150/312 Loss 3.986 Prec@(1,3) (86.3%, 99.1%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:23 PM | Valid: [175/180] Step 200/312 Loss 3.838 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:28 PM | Valid: [175/180] Step 250/312 Loss 3.797 Prec@(1,3) (86.8%, 99.1%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:32 PM | Valid: [175/180] Step 300/312 Loss 3.769 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:33 PM | Valid: [175/180] Step 312/312 Loss 3.819 Prec@(1,3) (86.6%, 99.1%), ce_loss 0.541, lat_loss 21.779
09/29 05:47:34 PM | val: [175/180] Final Prec@1 86.6300% Time 29.80
09/29 05:47:34 PM | Start to train weights for epoch 175
09/29 05:48:00 PM | Train: [176/180] Step 050/1249 Loss 1.384 Prec@(1,3) (95.0%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:48:25 PM | Train: [176/180] Step 100/1249 Loss 1.366 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:48:50 PM | Train: [176/180] Step 150/1249 Loss 1.371 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:49:15 PM | Train: [176/180] Step 200/1249 Loss 1.320 Prec@(1,3) (94.9%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:49:40 PM | Train: [176/180] Step 250/1249 Loss 1.351 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.541, lat_loss 21.779
09/29 05:50:05 PM | Train: [176/180] Step 300/1249 Loss 1.501 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.541, lat_loss 21.779
09/29 05:50:30 PM | Train: [176/180] Step 350/1249 Loss 1.457 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.541, lat_loss 21.779
09/29 05:50:56 PM | Train: [176/180] Step 400/1249 Loss 1.427 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:51:21 PM | Train: [176/180] Step 450/1249 Loss 1.405 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:51:46 PM | Train: [176/180] Step 500/1249 Loss 1.409 Prec@(1,3) (94.7%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:52:11 PM | Train: [176/180] Step 550/1249 Loss 1.377 Prec@(1,3) (94.8%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:52:36 PM | Train: [176/180] Step 600/1249 Loss 1.361 Prec@(1,3) (94.8%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:53:01 PM | Train: [176/180] Step 650/1249 Loss 1.359 Prec@(1,3) (94.8%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:53:26 PM | Train: [176/180] Step 700/1249 Loss 1.342 Prec@(1,3) (94.9%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:53:51 PM | Train: [176/180] Step 750/1249 Loss 1.352 Prec@(1,3) (94.8%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:54:13 PM | Train: [176/180] Step 800/1249 Loss 1.342 Prec@(1,3) (94.9%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:54:37 PM | Train: [176/180] Step 850/1249 Loss 1.368 Prec@(1,3) (94.8%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:54:59 PM | Train: [176/180] Step 900/1249 Loss 1.398 Prec@(1,3) (94.7%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:55:23 PM | Train: [176/180] Step 950/1249 Loss 1.391 Prec@(1,3) (94.7%, 99.8%), ce_loss 0.540, lat_loss 21.779
09/29 05:55:47 PM | Train: [176/180] Step 1000/1249 Loss 1.470 Prec@(1,3) (94.6%, 99.7%), ce_loss 0.540, lat_loss 21.779
09/29 05:56:11 PM | Train: [176/180] Step 1050/1249 Loss 1.513 Prec@(1,3) (94.5%, 99.7%), ce_loss 0.540, lat_loss 21.779
09/29 05:56:35 PM | Train: [176/180] Step 1100/1249 Loss 1.507 Prec@(1,3) (94.5%, 99.7%), ce_loss 0.540, lat_loss 21.779
09/29 05:57:00 PM | Train: [176/180] Step 1150/1249 Loss 1.505 Prec@(1,3) (94.5%, 99.7%), ce_loss 0.540, lat_loss 21.779
09/29 05:57:25 PM | Train: [176/180] Step 1200/1249 Loss 1.494 Prec@(1,3) (94.5%, 99.7%), ce_loss 0.540, lat_loss 21.779
09/29 05:57:49 PM | Train: [176/180] Step 1249/1249 Loss 1.492 Prec@(1,3) (94.5%, 99.7%), ce_loss 0.540, lat_loss 21.779
09/29 05:57:49 PM | _w_step_train: [176/180] Final Prec@1 94.4900% Time 615.47
09/29 05:57:49 PM | Start to train theta for epoch 175
09/29 05:58:10 PM | Train: [176/180] Step 050/312 Loss 3.485 Prec@(1,3) (87.7%, 99.8%), ce_loss 0.539, lat_loss 21.779
09/29 05:58:30 PM | Train: [176/180] Step 100/312 Loss 3.563 Prec@(1,3) (87.2%, 99.5%), ce_loss 0.539, lat_loss 21.779
09/29 05:58:51 PM | Train: [176/180] Step 150/312 Loss 3.654 Prec@(1,3) (87.4%, 99.4%), ce_loss 0.539, lat_loss 21.779
09/29 05:59:11 PM | Train: [176/180] Step 200/312 Loss 3.650 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.539, lat_loss 21.779
09/29 05:59:31 PM | Train: [176/180] Step 250/312 Loss 3.668 Prec@(1,3) (87.2%, 99.4%), ce_loss 0.539, lat_loss 21.779
09/29 05:59:51 PM | Train: [176/180] Step 300/312 Loss 3.816 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.539, lat_loss 21.779
09/29 05:59:56 PM | Train: [176/180] Step 312/312 Loss 3.821 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.539, lat_loss 21.779
09/29 05:59:57 PM | _theta_step_train: [176/180] Final Prec@1 86.8000% Time 127.57
09/29 06:00:02 PM | Valid: [176/180] Step 050/312 Loss 3.748 Prec@(1,3) (86.3%, 99.2%), ce_loss 0.539, lat_loss 21.779
09/29 06:00:07 PM | Valid: [176/180] Step 100/312 Loss 4.047 Prec@(1,3) (86.1%, 99.2%), ce_loss 0.539, lat_loss 21.779
09/29 06:00:11 PM | Valid: [176/180] Step 150/312 Loss 3.964 Prec@(1,3) (86.1%, 99.0%), ce_loss 0.539, lat_loss 21.779
09/29 06:00:16 PM | Valid: [176/180] Step 200/312 Loss 4.209 Prec@(1,3) (85.5%, 98.6%), ce_loss 0.539, lat_loss 21.779
09/29 06:00:20 PM | Valid: [176/180] Step 250/312 Loss 3.982 Prec@(1,3) (86.1%, 98.8%), ce_loss 0.539, lat_loss 21.779
09/29 06:00:25 PM | Valid: [176/180] Step 300/312 Loss 3.921 Prec@(1,3) (86.2%, 99.0%), ce_loss 0.539, lat_loss 21.779
09/29 06:00:26 PM | Valid: [176/180] Step 312/312 Loss 3.937 Prec@(1,3) (86.0%, 98.9%), ce_loss 0.539, lat_loss 21.779
09/29 06:00:26 PM | val: [176/180] Final Prec@1 85.9900% Time 29.27
09/29 06:00:26 PM | Start to train weights for epoch 176
09/29 06:00:43 PM | Train: [177/180] Step 050/1249 Loss 1.259 Prec@(1,3) (95.3%, 100.0%), ce_loss 0.539, lat_loss 21.780
09/29 06:00:59 PM | Train: [177/180] Step 100/1249 Loss 1.498 Prec@(1,3) (94.2%, 100.0%), ce_loss 0.539, lat_loss 21.780
09/29 06:01:15 PM | Train: [177/180] Step 150/1249 Loss 1.375 Prec@(1,3) (94.6%, 100.0%), ce_loss 0.539, lat_loss 21.780
09/29 06:01:32 PM | Train: [177/180] Step 200/1249 Loss 1.491 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.539, lat_loss 21.780
09/29 06:01:48 PM | Train: [177/180] Step 250/1249 Loss 1.439 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.539, lat_loss 21.780
09/29 06:02:04 PM | Train: [177/180] Step 300/1249 Loss 1.451 Prec@(1,3) (94.5%, 99.9%), ce_loss 0.539, lat_loss 21.780
09/29 06:02:20 PM | Train: [177/180] Step 350/1249 Loss 1.408 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.539, lat_loss 21.780
09/29 06:02:36 PM | Train: [177/180] Step 400/1249 Loss 1.585 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.539, lat_loss 21.780
09/29 06:02:52 PM | Train: [177/180] Step 450/1249 Loss 1.534 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.539, lat_loss 21.780
09/29 06:03:08 PM | Train: [177/180] Step 500/1249 Loss 1.502 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.539, lat_loss 21.780
09/29 06:03:24 PM | Train: [177/180] Step 550/1249 Loss 1.509 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.539, lat_loss 21.780
09/29 06:03:40 PM | Train: [177/180] Step 600/1249 Loss 1.549 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.539, lat_loss 21.780
09/29 06:03:56 PM | Train: [177/180] Step 650/1249 Loss 1.532 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.539, lat_loss 21.780
09/29 06:04:12 PM | Train: [177/180] Step 700/1249 Loss 1.571 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.539, lat_loss 21.780
09/29 06:04:28 PM | Train: [177/180] Step 750/1249 Loss 1.550 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:04:44 PM | Train: [177/180] Step 800/1249 Loss 1.594 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:05:00 PM | Train: [177/180] Step 850/1249 Loss 1.608 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:05:16 PM | Train: [177/180] Step 900/1249 Loss 1.624 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:05:31 PM | Train: [177/180] Step 950/1249 Loss 1.630 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:05:48 PM | Train: [177/180] Step 1000/1249 Loss 1.620 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:06:04 PM | Train: [177/180] Step 1050/1249 Loss 1.608 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:06:20 PM | Train: [177/180] Step 1100/1249 Loss 1.616 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:06:35 PM | Train: [177/180] Step 1150/1249 Loss 1.614 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:06:52 PM | Train: [177/180] Step 1200/1249 Loss 1.603 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:07:08 PM | Train: [177/180] Step 1249/1249 Loss 1.589 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:07:08 PM | _w_step_train: [177/180] Final Prec@1 94.1250% Time 401.83
09/29 06:07:08 PM | Start to train theta for epoch 176
09/29 06:07:28 PM | Train: [177/180] Step 050/312 Loss 4.797 Prec@(1,3) (85.2%, 98.7%), ce_loss 0.538, lat_loss 21.780
09/29 06:07:46 PM | Train: [177/180] Step 100/312 Loss 4.105 Prec@(1,3) (86.6%, 99.1%), ce_loss 0.538, lat_loss 21.780
09/29 06:08:04 PM | Train: [177/180] Step 150/312 Loss 3.939 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.538, lat_loss 21.780
09/29 06:08:24 PM | Train: [177/180] Step 200/312 Loss 4.008 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.538, lat_loss 21.780
09/29 06:08:44 PM | Train: [177/180] Step 250/312 Loss 4.140 Prec@(1,3) (85.9%, 99.2%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:05 PM | Train: [177/180] Step 300/312 Loss 4.269 Prec@(1,3) (85.5%, 99.2%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:10 PM | Train: [177/180] Step 312/312 Loss 4.265 Prec@(1,3) (85.6%, 99.2%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:10 PM | _theta_step_train: [177/180] Final Prec@1 85.6100% Time 122.51
09/29 06:09:16 PM | Valid: [177/180] Step 050/312 Loss 3.644 Prec@(1,3) (87.3%, 99.8%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:20 PM | Valid: [177/180] Step 100/312 Loss 4.158 Prec@(1,3) (86.3%, 99.3%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:25 PM | Valid: [177/180] Step 150/312 Loss 4.053 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:29 PM | Valid: [177/180] Step 200/312 Loss 3.905 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:34 PM | Valid: [177/180] Step 250/312 Loss 3.785 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:39 PM | Valid: [177/180] Step 300/312 Loss 4.033 Prec@(1,3) (86.3%, 99.0%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:40 PM | Valid: [177/180] Step 312/312 Loss 4.015 Prec@(1,3) (86.3%, 99.0%), ce_loss 0.538, lat_loss 21.780
09/29 06:09:40 PM | val: [177/180] Final Prec@1 86.2600% Time 29.32
09/29 06:09:40 PM | Start to train weights for epoch 177
09/29 06:10:03 PM | Train: [178/180] Step 050/1249 Loss 1.166 Prec@(1,3) (95.4%, 99.9%), ce_loss 0.538, lat_loss 21.780
09/29 06:10:27 PM | Train: [178/180] Step 100/1249 Loss 1.233 Prec@(1,3) (95.3%, 99.9%), ce_loss 0.538, lat_loss 21.780
09/29 06:10:50 PM | Train: [178/180] Step 150/1249 Loss 1.317 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.538, lat_loss 21.780
09/29 06:11:13 PM | Train: [178/180] Step 200/1249 Loss 1.285 Prec@(1,3) (94.9%, 99.9%), ce_loss 0.538, lat_loss 21.780
09/29 06:11:36 PM | Train: [178/180] Step 250/1249 Loss 1.312 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.538, lat_loss 21.780
09/29 06:11:58 PM | Train: [178/180] Step 300/1249 Loss 1.405 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:12:21 PM | Train: [178/180] Step 350/1249 Loss 1.419 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.537, lat_loss 21.780
09/29 06:12:44 PM | Train: [178/180] Step 400/1249 Loss 1.403 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.537, lat_loss 21.780
09/29 06:13:07 PM | Train: [178/180] Step 450/1249 Loss 1.387 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.537, lat_loss 21.780
09/29 06:13:29 PM | Train: [178/180] Step 500/1249 Loss 1.374 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.537, lat_loss 21.780
09/29 06:13:52 PM | Train: [178/180] Step 550/1249 Loss 1.392 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.537, lat_loss 21.780
09/29 06:14:13 PM | Train: [178/180] Step 600/1249 Loss 1.464 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:14:36 PM | Train: [178/180] Step 650/1249 Loss 1.449 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:15:01 PM | Train: [178/180] Step 700/1249 Loss 1.450 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:15:26 PM | Train: [178/180] Step 750/1249 Loss 1.441 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:15:51 PM | Train: [178/180] Step 800/1249 Loss 1.452 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:16:16 PM | Train: [178/180] Step 850/1249 Loss 1.468 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:16:41 PM | Train: [178/180] Step 900/1249 Loss 1.458 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:17:06 PM | Train: [178/180] Step 950/1249 Loss 1.449 Prec@(1,3) (94.4%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:17:31 PM | Train: [178/180] Step 1000/1249 Loss 1.442 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:17:56 PM | Train: [178/180] Step 1050/1249 Loss 1.441 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:18:21 PM | Train: [178/180] Step 1100/1249 Loss 1.450 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:18:46 PM | Train: [178/180] Step 1150/1249 Loss 1.439 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.537, lat_loss 21.780
09/29 06:19:11 PM | Train: [178/180] Step 1200/1249 Loss 1.449 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.536, lat_loss 21.780
09/29 06:19:36 PM | Train: [178/180] Step 1249/1249 Loss 1.447 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.536, lat_loss 21.780
09/29 06:19:36 PM | _w_step_train: [178/180] Final Prec@1 94.4925% Time 596.09
09/29 06:19:36 PM | Start to train theta for epoch 177
09/29 06:19:57 PM | Train: [178/180] Step 050/312 Loss 3.500 Prec@(1,3) (88.8%, 99.5%), ce_loss 0.536, lat_loss 21.780
09/29 06:20:16 PM | Train: [178/180] Step 100/312 Loss 4.012 Prec@(1,3) (87.1%, 99.1%), ce_loss 0.536, lat_loss 21.780
09/29 06:20:37 PM | Train: [178/180] Step 150/312 Loss 4.044 Prec@(1,3) (87.1%, 99.1%), ce_loss 0.536, lat_loss 21.780
09/29 06:20:56 PM | Train: [178/180] Step 200/312 Loss 4.044 Prec@(1,3) (86.9%, 99.1%), ce_loss 0.536, lat_loss 21.780
09/29 06:21:16 PM | Train: [178/180] Step 250/312 Loss 3.961 Prec@(1,3) (87.0%, 99.2%), ce_loss 0.536, lat_loss 21.780
09/29 06:21:36 PM | Train: [178/180] Step 300/312 Loss 3.981 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.536, lat_loss 21.780
09/29 06:21:41 PM | Train: [178/180] Step 312/312 Loss 3.985 Prec@(1,3) (86.8%, 99.2%), ce_loss 0.536, lat_loss 21.780
09/29 06:21:41 PM | _theta_step_train: [178/180] Final Prec@1 86.7600% Time 125.16
09/29 06:21:46 PM | Valid: [178/180] Step 050/312 Loss 3.581 Prec@(1,3) (86.9%, 99.5%), ce_loss 0.536, lat_loss 21.780
09/29 06:21:51 PM | Valid: [178/180] Step 100/312 Loss 3.866 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.536, lat_loss 21.780
09/29 06:21:55 PM | Valid: [178/180] Step 150/312 Loss 3.918 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.536, lat_loss 21.780
09/29 06:22:00 PM | Valid: [178/180] Step 200/312 Loss 3.863 Prec@(1,3) (86.6%, 99.2%), ce_loss 0.536, lat_loss 21.780
09/29 06:22:04 PM | Valid: [178/180] Step 250/312 Loss 3.792 Prec@(1,3) (86.7%, 99.2%), ce_loss 0.536, lat_loss 21.780
09/29 06:22:09 PM | Valid: [178/180] Step 300/312 Loss 3.724 Prec@(1,3) (86.8%, 99.3%), ce_loss 0.536, lat_loss 21.780
09/29 06:22:10 PM | Valid: [178/180] Step 312/312 Loss 3.742 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.536, lat_loss 21.780
09/29 06:22:10 PM | val: [178/180] Final Prec@1 86.6500% Time 29.17
09/29 06:22:10 PM | Start to train weights for epoch 178
09/29 06:22:36 PM | Train: [179/180] Step 050/1249 Loss 1.236 Prec@(1,3) (95.4%, 99.9%), ce_loss 0.536, lat_loss 21.780
09/29 06:23:01 PM | Train: [179/180] Step 100/1249 Loss 1.468 Prec@(1,3) (94.6%, 99.8%), ce_loss 0.536, lat_loss 21.780
09/29 06:23:25 PM | Train: [179/180] Step 150/1249 Loss 1.349 Prec@(1,3) (95.1%, 99.9%), ce_loss 0.536, lat_loss 21.780
09/29 06:23:50 PM | Train: [179/180] Step 200/1249 Loss 1.392 Prec@(1,3) (95.1%, 99.9%), ce_loss 0.536, lat_loss 21.780
09/29 06:24:15 PM | Train: [179/180] Step 250/1249 Loss 1.373 Prec@(1,3) (95.0%, 99.9%), ce_loss 0.536, lat_loss 21.780
09/29 06:24:40 PM | Train: [179/180] Step 300/1249 Loss 1.408 Prec@(1,3) (94.8%, 99.8%), ce_loss 0.536, lat_loss 21.780
09/29 06:25:05 PM | Train: [179/180] Step 350/1249 Loss 1.392 Prec@(1,3) (94.8%, 99.8%), ce_loss 0.536, lat_loss 21.780
09/29 06:25:30 PM | Train: [179/180] Step 400/1249 Loss 1.397 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.536, lat_loss 21.780
09/29 06:25:54 PM | Train: [179/180] Step 450/1249 Loss 1.458 Prec@(1,3) (94.5%, 99.8%), ce_loss 0.536, lat_loss 21.780
09/29 06:26:19 PM | Train: [179/180] Step 500/1249 Loss 1.449 Prec@(1,3) (94.4%, 99.9%), ce_loss 0.536, lat_loss 21.780
09/29 06:26:44 PM | Train: [179/180] Step 550/1249 Loss 1.419 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.536, lat_loss 21.780
09/29 06:27:08 PM | Train: [179/180] Step 600/1249 Loss 1.401 Prec@(1,3) (94.6%, 99.9%), ce_loss 0.536, lat_loss 21.780
09/29 06:27:33 PM | Train: [179/180] Step 650/1249 Loss 1.387 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:27:58 PM | Train: [179/180] Step 700/1249 Loss 1.376 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:28:23 PM | Train: [179/180] Step 750/1249 Loss 1.359 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:28:47 PM | Train: [179/180] Step 800/1249 Loss 1.376 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:29:02 PM | Train: [179/180] Step 850/1249 Loss 1.385 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:29:18 PM | Train: [179/180] Step 900/1249 Loss 1.385 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:29:34 PM | Train: [179/180] Step 950/1249 Loss 1.373 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:29:50 PM | Train: [179/180] Step 1000/1249 Loss 1.357 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:30:06 PM | Train: [179/180] Step 1050/1249 Loss 1.355 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:30:22 PM | Train: [179/180] Step 1100/1249 Loss 1.352 Prec@(1,3) (94.8%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:30:38 PM | Train: [179/180] Step 1150/1249 Loss 1.369 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:30:54 PM | Train: [179/180] Step 1200/1249 Loss 1.388 Prec@(1,3) (94.7%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:31:09 PM | Train: [179/180] Step 1249/1249 Loss 1.409 Prec@(1,3) (94.7%, 99.8%), ce_loss 0.535, lat_loss 21.780
09/29 06:31:09 PM | _w_step_train: [179/180] Final Prec@1 94.6550% Time 539.31
09/29 06:31:09 PM | Start to train theta for epoch 178
09/29 06:31:32 PM | Train: [179/180] Step 050/312 Loss 4.331 Prec@(1,3) (85.6%, 99.1%), ce_loss 0.535, lat_loss 21.780
09/29 06:31:53 PM | Train: [179/180] Step 100/312 Loss 4.279 Prec@(1,3) (85.4%, 99.0%), ce_loss 0.535, lat_loss 21.780
09/29 06:32:14 PM | Train: [179/180] Step 150/312 Loss 4.033 Prec@(1,3) (86.5%, 99.2%), ce_loss 0.535, lat_loss 21.780
09/29 06:32:35 PM | Train: [179/180] Step 200/312 Loss 3.893 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.535, lat_loss 21.780
09/29 06:32:55 PM | Train: [179/180] Step 250/312 Loss 3.848 Prec@(1,3) (86.9%, 99.2%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:16 PM | Train: [179/180] Step 300/312 Loss 3.799 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:21 PM | Train: [179/180] Step 312/312 Loss 3.842 Prec@(1,3) (86.9%, 99.3%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:21 PM | _theta_step_train: [179/180] Final Prec@1 86.8800% Time 131.85
09/29 06:33:27 PM | Valid: [179/180] Step 050/312 Loss 3.939 Prec@(1,3) (86.0%, 99.4%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:31 PM | Valid: [179/180] Step 100/312 Loss 4.159 Prec@(1,3) (85.8%, 99.1%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:36 PM | Valid: [179/180] Step 150/312 Loss 4.192 Prec@(1,3) (85.5%, 98.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:41 PM | Valid: [179/180] Step 200/312 Loss 3.972 Prec@(1,3) (86.2%, 99.1%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:45 PM | Valid: [179/180] Step 250/312 Loss 3.916 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:50 PM | Valid: [179/180] Step 300/312 Loss 3.960 Prec@(1,3) (86.4%, 99.0%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:51 PM | Valid: [179/180] Step 312/312 Loss 3.942 Prec@(1,3) (86.4%, 99.1%), ce_loss 0.535, lat_loss 21.780
09/29 06:33:51 PM | val: [179/180] Final Prec@1 86.4300% Time 29.75
09/29 06:33:51 PM | Start to train weights for epoch 179
09/29 06:34:17 PM | Train: [180/180] Step 050/1249 Loss 1.164 Prec@(1,3) (95.4%, 99.9%), ce_loss 0.535, lat_loss 21.780
09/29 06:34:42 PM | Train: [180/180] Step 100/1249 Loss 1.337 Prec@(1,3) (94.7%, 100.0%), ce_loss 0.535, lat_loss 21.780
09/29 06:35:03 PM | Train: [180/180] Step 150/1249 Loss 1.830 Prec@(1,3) (93.4%, 99.6%), ce_loss 0.534, lat_loss 21.780
09/29 06:35:23 PM | Train: [180/180] Step 200/1249 Loss 1.754 Prec@(1,3) (93.6%, 99.7%), ce_loss 0.534, lat_loss 21.780
09/29 06:35:45 PM | Train: [180/180] Step 250/1249 Loss 1.704 Prec@(1,3) (93.7%, 99.7%), ce_loss 0.534, lat_loss 21.780
09/29 06:36:07 PM | Train: [180/180] Step 300/1249 Loss 1.621 Prec@(1,3) (93.9%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:36:29 PM | Train: [180/180] Step 350/1249 Loss 1.588 Prec@(1,3) (94.0%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:36:53 PM | Train: [180/180] Step 400/1249 Loss 1.558 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:37:15 PM | Train: [180/180] Step 450/1249 Loss 1.539 Prec@(1,3) (94.1%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:37:39 PM | Train: [180/180] Step 500/1249 Loss 1.488 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:38:04 PM | Train: [180/180] Step 550/1249 Loss 1.547 Prec@(1,3) (94.2%, 99.7%), ce_loss 0.534, lat_loss 21.780
09/29 06:38:26 PM | Train: [180/180] Step 600/1249 Loss 1.522 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:38:50 PM | Train: [180/180] Step 650/1249 Loss 1.501 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:39:15 PM | Train: [180/180] Step 700/1249 Loss 1.492 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:39:40 PM | Train: [180/180] Step 750/1249 Loss 1.511 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:40:05 PM | Train: [180/180] Step 800/1249 Loss 1.530 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:40:30 PM | Train: [180/180] Step 850/1249 Loss 1.511 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:40:51 PM | Train: [180/180] Step 900/1249 Loss 1.519 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:41:15 PM | Train: [180/180] Step 950/1249 Loss 1.503 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:41:40 PM | Train: [180/180] Step 1000/1249 Loss 1.497 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.534, lat_loss 21.780
09/29 06:42:04 PM | Train: [180/180] Step 1050/1249 Loss 1.495 Prec@(1,3) (94.3%, 99.8%), ce_loss 0.533, lat_loss 21.780
09/29 06:42:28 PM | Train: [180/180] Step 1100/1249 Loss 1.504 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.533, lat_loss 21.780
09/29 06:42:52 PM | Train: [180/180] Step 1150/1249 Loss 1.520 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.533, lat_loss 21.780
09/29 06:43:16 PM | Train: [180/180] Step 1200/1249 Loss 1.522 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.533, lat_loss 21.780
09/29 06:43:41 PM | Train: [180/180] Step 1249/1249 Loss 1.521 Prec@(1,3) (94.2%, 99.8%), ce_loss 0.533, lat_loss 21.780
09/29 06:43:41 PM | _w_step_train: [180/180] Final Prec@1 94.1950% Time 589.81
09/29 06:43:41 PM | Start to train theta for epoch 179
09/29 06:44:02 PM | Train: [180/180] Step 050/312 Loss 4.334 Prec@(1,3) (85.7%, 99.0%), ce_loss 0.533, lat_loss 21.780
09/29 06:44:24 PM | Train: [180/180] Step 100/312 Loss 4.161 Prec@(1,3) (86.5%, 99.1%), ce_loss 0.533, lat_loss 21.780
09/29 06:44:45 PM | Train: [180/180] Step 150/312 Loss 4.093 Prec@(1,3) (86.4%, 99.3%), ce_loss 0.533, lat_loss 21.780
09/29 06:45:06 PM | Train: [180/180] Step 200/312 Loss 3.971 Prec@(1,3) (86.8%, 99.4%), ce_loss 0.533, lat_loss 21.780
09/29 06:45:27 PM | Train: [180/180] Step 250/312 Loss 3.869 Prec@(1,3) (87.0%, 99.4%), ce_loss 0.533, lat_loss 21.780
09/29 06:45:48 PM | Train: [180/180] Step 300/312 Loss 3.988 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.533, lat_loss 21.780
09/29 06:45:53 PM | Train: [180/180] Step 312/312 Loss 3.992 Prec@(1,3) (86.7%, 99.3%), ce_loss 0.533, lat_loss 21.780
09/29 06:45:53 PM | _theta_step_train: [180/180] Final Prec@1 86.6800% Time 131.91
09/29 06:45:58 PM | Valid: [180/180] Step 050/312 Loss 5.392 Prec@(1,3) (83.8%, 97.1%), ce_loss 0.533, lat_loss 21.780
09/29 06:46:03 PM | Valid: [180/180] Step 100/312 Loss 4.799 Prec@(1,3) (85.0%, 97.7%), ce_loss 0.533, lat_loss 21.780
09/29 06:46:07 PM | Valid: [180/180] Step 150/312 Loss 4.533 Prec@(1,3) (85.5%, 98.0%), ce_loss 0.533, lat_loss 21.780
09/29 06:46:12 PM | Valid: [180/180] Step 200/312 Loss 4.347 Prec@(1,3) (85.8%, 98.3%), ce_loss 0.533, lat_loss 21.780
09/29 06:46:17 PM | Valid: [180/180] Step 250/312 Loss 4.223 Prec@(1,3) (85.9%, 98.3%), ce_loss 0.533, lat_loss 21.780
09/29 06:46:21 PM | Valid: [180/180] Step 300/312 Loss 4.241 Prec@(1,3) (85.9%, 98.5%), ce_loss 0.533, lat_loss 21.780
09/29 06:46:22 PM | Valid: [180/180] Step 312/312 Loss 4.298 Prec@(1,3) (85.7%, 98.5%), ce_loss 0.533, lat_loss 21.780
09/29 06:46:22 PM | val: [180/180] Final Prec@1 85.7000% Time 29.39
